# -*- coding: utf-8 -*-
"""
GraphRAG Service - Ø³Ø±ÙˆÛŒØ³ Ø§ØµÙ„ÛŒ GraphRAG
Ø§ÛŒÙ† Ø³Ø±ÙˆÛŒØ³ ØªÙ…Ø§Ù… Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ GraphRAG Ø±Ø§ ÙØ±Ø§Ù‡Ù… Ù…ÛŒâ€ŒÚ©Ù†Ø¯
"""

import pandas as pd
import networkx as nx
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from collections import deque
import pickle
import os
import json
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from enum import Enum

class RetrievalMethod(Enum):
    """Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ"""
    BFS = "BFS"
    DFS = "DFS"
    SHORTEST_PATH = "Shortest Path"
    NEIGHBORS = "Neighbors"
    HYBRID = "Hybrid"
    MULTI_METHOD = "Multi-Method"
    ENSEMBLE = "Ensemble"
    ADAPTIVE = "Adaptive"
    INTELLIGENT = "Intelligent Semantic Search"
    NO_RETRIEVAL = "Ø¨Ø¯ÙˆÙ† Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ (ÙÙ‚Ø· Ù…Ø¯Ù„)"

class GenerationModel(Enum):
    """Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ†"""
    SIMPLE = "Simple Template"
    GPT_SIMULATION = "GPT Simulation"
    CUSTOM = "Custom Model"
    HUGGINGFACE = "HuggingFace Models"
    OPENAI_GPT = "OpenAI GPT"
    ANTHROPIC_CLAUDE = "Anthropic Claude"
    GOOGLE_GEMINI = "Google Gemini"

@dataclass
class GraphNode:
    """Ù†Ù…Ø§ÛŒØ´ ÛŒÚ© Ù†ÙˆØ¯ Ú¯Ø±Ø§Ù"""
    id: str
    name: str
    kind: str
    depth: int = 0
    score: float = 1.0

@dataclass
class GraphEdge:
    """Ù†Ù…Ø§ÛŒØ´ ÛŒÚ© ÛŒØ§Ù„ Ú¯Ø±Ø§Ù"""
    source: str
    target: str
    relation: str
    weight: float = 1.0

@dataclass
class RetrievalResult:
    """Ù†ØªÛŒØ¬Ù‡ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ"""
    nodes: List[GraphNode]
    edges: List[GraphEdge]
    paths: List[List[str]]
    context_text: str
    method: str
    query: str

@dataclass
class GenerationResult:
    """Ù†ØªÛŒØ¬Ù‡ ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ†"""
    answer: str
    model: str
    context_used: str
    confidence: float

class GraphRAGService:
    """Ø³Ø±ÙˆÛŒØ³ Ø§ØµÙ„ÛŒ GraphRAG"""
    
    def __init__(self, graph_data_path: str = None):
        self.G = None
        self.nlp = None
        self.graph_data_path = graph_data_path
        self.initialize()
    
    def initialize(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³Ø±ÙˆÛŒØ³"""
        print("ğŸ”§ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ GraphRAG Service...")
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ spaCy
        try:
            self.nlp = spacy.load("en_core_web_sm")
            print("âœ… Ù…Ø¯Ù„ spaCy Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
        except:
            print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ spaCy")
            return
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÛŒØ§ Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø±Ø§Ù
        if self.graph_data_path and os.path.exists(self.graph_data_path):
            self.load_graph_from_file()
        else:
            self.create_sample_graph()
    
    def create_sample_graph(self):
        """Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø±Ø§Ù Ù†Ù…ÙˆÙ†Ù‡"""
        print("ğŸ”§ Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø±Ø§Ù Ù†Ù…ÙˆÙ†Ù‡...")
        
        self.G = nx.Graph()
        
        # Ù†ÙˆØ¯Ù‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡
        sample_nodes = [
            ('Gene::HMGB3', 'HMGB3', 'Gene'),
            ('Gene::PCNA', 'PCNA', 'Gene'),
            ('Gene::TP53', 'TP53', 'Gene'),
            ('Gene::BRCA1', 'BRCA1', 'Gene'),
            ('Disease::Diabetes', 'Type 2 Diabetes', 'Disease'),
            ('Disease::Cancer', 'Cancer', 'Disease'),
            ('Disease::HeartDisease', 'Heart Disease', 'Disease'),
            ('Drug::Metformin', 'Metformin', 'Drug'),
            ('Drug::Aspirin', 'Aspirin', 'Drug'),
            ('Drug::Insulin', 'Insulin', 'Drug'),
            ('Biological Process::GO:0008150', 'Metabolic Process', 'Biological Process'),
            ('Biological Process::GO:0006915', 'Apoptosis', 'Biological Process'),
            ('Biological Process::GO:0007049', 'Cell Cycle', 'Biological Process'),
            ('Anatomy::Heart', 'Heart', 'Anatomy'),
            ('Anatomy::Lung', 'Lung', 'Anatomy'),
            ('Anatomy::Brain', 'Brain', 'Anatomy'),
            ('Anatomy::Liver', 'Liver', 'Anatomy')
        ]
        
        for node_id, name, kind in sample_nodes:
            self.G.add_node(node_id, name=name, kind=kind)
        
        # ÛŒØ§Ù„â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡
        sample_edges = [
            ('Gene::HMGB3', 'Gene::PCNA', 'interacts_with'),
            ('Gene::PCNA', 'Disease::Diabetes', 'associates'),
            ('Gene::TP53', 'Disease::Cancer', 'causes'),
            ('Gene::BRCA1', 'Disease::Cancer', 'causes'),
            ('Drug::Metformin', 'Disease::Diabetes', 'treats'),
            ('Drug::Aspirin', 'Disease::HeartDisease', 'prevents'),
            ('Drug::Insulin', 'Disease::Diabetes', 'treats'),
            ('Gene::HMGB3', 'Biological Process::GO:0008150', 'participates_in'),
            ('Gene::TP53', 'Biological Process::GO:0006915', 'regulates'),
            ('Gene::BRCA1', 'Biological Process::GO:0007049', 'regulates'),
            ('Anatomy::Heart', 'Anatomy::Lung', 'adjacent_to'),
            ('Anatomy::Brain', 'Anatomy::Heart', 'controls'),
            ('Gene::HMGB3', 'Anatomy::Heart', 'expressed_in'),
            ('Gene::TP53', 'Anatomy::Brain', 'expressed_in'),
            ('Gene::BRCA1', 'Anatomy::Liver', 'expressed_in'),
            ('Disease::Diabetes', 'Anatomy::Heart', 'affects'),
            ('Disease::Cancer', 'Anatomy::Brain', 'affects')
        ]
        
        for source, target, relation in sample_edges:
            self.G.add_edge(source, target, metaedge=relation)
        
        print(f"âœ… Ú¯Ø±Ø§Ù Ù†Ù…ÙˆÙ†Ù‡ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯: {self.G.number_of_nodes()} Ù†ÙˆØ¯ØŒ {self.G.number_of_edges()} ÛŒØ§Ù„")
    
    def load_graph_from_file(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú¯Ø±Ø§Ù Ø§Ø² ÙØ§ÛŒÙ„"""
        try:
            with open(self.graph_data_path, 'rb') as f:
                self.G = pickle.load(f)
            print(f"âœ… Ú¯Ø±Ø§Ù Ø§Ø² ÙØ§ÛŒÙ„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯: {self.G.number_of_nodes()} Ù†ÙˆØ¯ØŒ {self.G.number_of_edges()} ÛŒØ§Ù„")
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú¯Ø±Ø§Ù: {e}")
            self.create_sample_graph()
    
    def extract_keywords(self, text: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø² Ù…ØªÙ†"""
        doc = self.nlp(text)
        keywords = set()
        
        # Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù…â€ŒØ¯Ø§Ø±
        for ent in doc.ents:
            if ent.label_ not in {"DATE", "TIME", "PERCENT", "MONEY", "QUANTITY", "ORDINAL", "CARDINAL"}:
                keywords.add(ent.text.lower())
        
        # Ø§Ø³Ù…â€ŒÙ‡Ø§ Ùˆ Ø§Ø³Ù… Ø®Ø§Øµâ€ŒÙ‡Ø§
        for token in doc:
            if (token.pos_ in {"NOUN", "PROPN"} and 
                token.text.lower() not in STOP_WORDS and 
                token.is_alpha and len(token.text) > 2):
                keywords.add(token.text.lower())
        
        return sorted(keywords)
    
    def analyze_question_intent(self, query: str) -> Dict[str, Any]:
        """ØªØ­Ù„ÛŒÙ„ Ø¹Ù…ÛŒÙ‚ Ø³ÙˆØ§Ù„ Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‚ØµØ¯ Ú©Ø§Ø±Ø¨Ø±"""
        query_lower = query.lower()
        
        # ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ Ø³ÙˆØ§Ù„
        question_type = self._analyze_question_type(query_lower)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
        main_entities = []
        entity_types = []
        
        # Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø§Ù†ÙˆØ§Ø¹ Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§
        entity_keywords = {
            'Gene': ['Ú˜Ù†', 'gene', 'protein', 'Ù¾Ø±ÙˆØªØ¦ÛŒÙ†', 'dna', 'rna', 'mrna'],
            'Disease': ['Ø¨ÛŒÙ…Ø§Ø±ÛŒ', 'disease', 'disorder', 'syndrome', 'cancer', 'Ø³Ø±Ø·Ø§Ù†', 'diabetes', 'Ø¯ÛŒØ§Ø¨Øª'],
            'Drug': ['Ø¯Ø§Ø±Ùˆ', 'drug', 'medicine', 'medication', 'treatment', 'Ø¯Ø±Ù…Ø§Ù†'],
            'Anatomy': ['Ù‚Ù„Ø¨', 'heart', 'brain', 'Ù…ØºØ²', 'liver', 'Ú©Ø¨Ø¯', 'lung', 'Ø±ÛŒÙ‡', 'kidney', 'Ú©Ù„ÛŒÙ‡'],
            'Biological_Process': ['process', 'ÙØ±Ø¢ÛŒÙ†Ø¯', 'pathway', 'Ù…Ø³ÛŒØ±', 'metabolism', 'Ù…ØªØ§Ø¨ÙˆÙ„ÛŒØ³Ù…'],
            'Compound': ['compound', 'ØªØ±Ú©ÛŒØ¨', 'chemical', 'Ø´ÛŒÙ…ÛŒØ§ÛŒÛŒ', 'molecule', 'Ù…ÙˆÙ„Ú©ÙˆÙ„']
        }
        
        # ØªØ´Ø®ÛŒØµ Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
        for entity_type, keywords in entity_keywords.items():
            for keyword in keywords:
                if keyword in query_lower:
                    main_entities.append(keyword)
                    entity_types.append(entity_type)
                    break
        
        # ØªØ´Ø®ÛŒØµ Ø±ÙˆØ§Ø¨Ø·
        relationships = []
        relationship_keywords = {
            'interacts_with': ['ØªØ¹Ø§Ù…Ù„', 'interact', 'interaction', 'ØªØ¹Ø§Ù…Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯'],
            'associates': ['Ù…Ø±ØªØ¨Ø·', 'associate', 'association', 'Ø§Ø±ØªØ¨Ø§Ø·'],
            'treats': ['Ø¯Ø±Ù…Ø§Ù†', 'treat', 'treatment', 'cure', 'Ø´ÙØ§'],
            'causes': ['Ø³Ø¨Ø¨', 'cause', 'causation', 'Ø¹Ù„Øª'],
            'expressed_in': ['Ø¨ÛŒØ§Ù†', 'express', 'expression', 'Ø¨ÛŒØ§Ù† Ù…ÛŒâ€ŒØ´ÙˆØ¯'],
            'regulates': ['ØªÙ†Ø¸ÛŒÙ…', 'regulate', 'regulation', 'Ú©Ù†ØªØ±Ù„']
        }
        
        for rel_type, keywords in relationship_keywords.items():
            for keyword in keywords:
                if keyword in query_lower:
                    relationships.append(rel_type)
                    break
        
        # ØªØ´Ø®ÛŒØµ Ø³ÙˆØ§Ù„â€ŒÙ‡Ø§ÛŒ Ø®Ø§Øµ
        question_patterns = {
            'what_genes': ['Ú†Ù‡ Ú˜Ù†', 'what gene', 'which gene'],
            'what_diseases': ['Ú†Ù‡ Ø¨ÛŒÙ…Ø§Ø±ÛŒ', 'what disease', 'which disease'],
            'what_drugs': ['Ú†Ù‡ Ø¯Ø§Ø±Ùˆ', 'what drug', 'which drug'],
            'how_treat': ['Ú†Ú¯ÙˆÙ†Ù‡ Ø¯Ø±Ù…Ø§Ù†', 'how treat', 'how cure'],
            'what_causes': ['Ú†Ù‡ Ø³Ø¨Ø¨', 'what cause', 'what causes'],
            'where_expressed': ['Ú©Ø¬Ø§ Ø¨ÛŒØ§Ù†', 'where express', 'where expressed']
        }
        
        detected_patterns = []
        for pattern_name, patterns in question_patterns.items():
            for pattern in patterns:
                if pattern in query_lower:
                    detected_patterns.append(pattern_name)
                    break
        
        return {
            'question_type': question_type,
            'main_entities': main_entities,
            'entity_types': entity_types,
            'relationships': relationships,
            'patterns': detected_patterns,
            'keywords': self.extract_keywords(query)
        }
    
    def intelligent_semantic_search(self, query: str, max_depth: int = 3) -> List[Tuple[str, int, float, str]]:
        """Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ­Ù„ÛŒÙ„ Ø³ÙˆØ§Ù„"""
        if not self.G:
            return []
        
        # ØªØ­Ù„ÛŒÙ„ Ø³ÙˆØ§Ù„
        intent = self.analyze_question_intent(query)
        print(f"ØªØ­Ù„ÛŒÙ„ Ø³ÙˆØ§Ù„: {intent}")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
        keywords = intent['keywords']
        main_entities = intent['main_entities']
        
        # ØªØ·Ø¨ÛŒÙ‚ Ø¨Ø§ Ù†ÙˆØ¯Ù‡Ø§ÛŒ Ú¯Ø±Ø§Ù
        matched_nodes = self.match_tokens_to_nodes(keywords + main_entities)
        
        if not matched_nodes:
            print("Ù‡ÛŒÚ† Ù†ÙˆØ¯ÛŒ ØªØ·Ø¨ÛŒÙ‚ Ù†ÛŒØ§ÙØª")
            return []
        
        print(f"Ù†ÙˆØ¯Ù‡Ø§ÛŒ ØªØ·Ø¨ÛŒÙ‚ ÛŒØ§ÙØªÙ‡: {matched_nodes}")
        
        # Ø¬Ø³ØªØ¬ÙˆÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ø³ÙˆØ§Ù„
        results = []
        
        if intent['question_type'] == 'anatomy_expression':
            # Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¨ÛŒØ§Ù† Ú˜Ù† Ø¯Ø± Ø¢Ù†Ø§ØªÙˆÙ…ÛŒ
            results = self._search_anatomy_expression(matched_nodes, intent, max_depth)
        elif intent['question_type'] == 'disease_info':
            # Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§
            results = self._search_disease_related(matched_nodes, intent, max_depth)
        elif intent['question_type'] == 'drug_treatment':
            # Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¯Ø§Ø±ÙˆÙ‡Ø§ - Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ Ø¨ÛŒÙ…Ø§Ø±ÛŒ Ø¯Ø± matched_nodes ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
            if any(self.G.nodes[node_id]['kind'] == 'Disease' for node_id in matched_nodes.values()):
                # Ø§Ú¯Ø± Ø¨ÛŒÙ…Ø§Ø±ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŒ Ø¯Ø§Ø±ÙˆÙ‡Ø§ÛŒ Ø¯Ø±Ù…Ø§Ù†ÛŒ Ø¢Ù† Ø±Ø§ Ø¬Ø³ØªØ¬Ùˆ Ú©Ù†
                results = self._search_disease_related(matched_nodes, intent, max_depth)
            else:
                # Ø§Ú¯Ø± Ø¯Ø§Ø±Ùˆ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŒ Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø±Ù…Ø§Ù† Ø´Ø¯Ù‡ ØªÙˆØ³Ø· Ø¢Ù† Ø±Ø§ Ø¬Ø³ØªØ¬Ùˆ Ú©Ù†
                results = self._search_drug_related(matched_nodes, intent, max_depth)
        elif intent['question_type'] == 'gene_function':
            # Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ú˜Ù†â€ŒÙ‡Ø§
            results = self._search_gene_function(matched_nodes, intent, max_depth)
        else:
            # Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¹Ù…ÙˆÙ…ÛŒ
            results = self._search_general(matched_nodes, intent, max_depth)
        
        return results
    
    def _search_anatomy_expression(self, matched_nodes: Dict[str, str], intent: Dict, max_depth: int) -> List[Tuple[str, int, float, str]]:
        """Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨ÛŒØ§Ù† Ú˜Ù† Ø¯Ø± Ø¢Ù†Ø§ØªÙˆÙ…ÛŒ"""
        results = []
        
        for token, node_id in matched_nodes.items():
            if self.G.nodes[node_id]['kind'] == 'Anatomy':
                # ÛŒØ§ÙØªÙ† Ú˜Ù†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø± Ø§ÛŒÙ† Ø¢Ù†Ø§ØªÙˆÙ…ÛŒ Ø¨ÛŒØ§Ù† Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
                for neighbor in self.G.neighbors(node_id):
                    if self.G.nodes[neighbor]['kind'] == 'Gene':
                        # Ø¨Ø±Ø±Ø³ÛŒ Ø±Ø§Ø¨Ø·Ù‡ Ø¨ÛŒØ§Ù†
                        edge_data = self.G.get_edge_data(node_id, neighbor)
                        if edge_data and 'expressed_in' in edge_data.get('metaedge', ''):
                            results.append((neighbor, 1, 5.0, f"Ø¨ÛŒØ§Ù† Ø¯Ø± {self.G.nodes[node_id]['name']}"))
                
                # Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¹Ù…ÛŒÙ‚â€ŒØªØ±
                for depth in range(2, max_depth + 1):
                    for path in nx.single_source_shortest_path(self.G, node_id, cutoff=depth).values():
                        if len(path) == depth + 1:
                            target_node = path[-1]
                            if self.G.nodes[target_node]['kind'] == 'Gene':
                                score = 5.0 / depth
                                results.append((target_node, depth, score, f"Ù…Ø³ÛŒØ± {depth} Ø³Ø·Ø­ÛŒ"))
        
        return results
    
    def _search_disease_related(self, matched_nodes: Dict[str, str], intent: Dict, max_depth: int) -> List[Tuple[str, int, float, str]]:
        """Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§"""
        results = []
        
        for token, node_id in matched_nodes.items():
            if self.G.nodes[node_id]['kind'] == 'Disease':
                # ÛŒØ§ÙØªÙ† Ú˜Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø¨ÛŒÙ…Ø§Ø±ÛŒ
                for neighbor in self.G.neighbors(node_id):
                    if self.G.nodes[neighbor]['kind'] == 'Gene':
                        edge_data = self.G.get_edge_data(node_id, neighbor)
                        if edge_data:
                            results.append((neighbor, 1, 5.0, f"Ù…Ø±ØªØ¨Ø· Ø¨Ø§ {self.G.nodes[node_id]['name']}"))
                
                # ÛŒØ§ÙØªÙ† Ø¯Ø§Ø±ÙˆÙ‡Ø§ÛŒ Ø¯Ø±Ù…Ø§Ù†ÛŒ
                for neighbor in self.G.neighbors(node_id):
                    if self.G.nodes[neighbor]['kind'] == 'Drug':
                        edge_data = self.G.get_edge_data(node_id, neighbor)
                        if edge_data and 'treats' in edge_data.get('metaedge', ''):
                            results.append((neighbor, 1, 4.5, f"Ø¯Ø±Ù…Ø§Ù† {self.G.nodes[node_id]['name']}"))
                
                # Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¹Ù…ÛŒÙ‚â€ŒØªØ±
                for depth in range(2, max_depth + 1):
                    for path in nx.single_source_shortest_path(self.G, node_id, cutoff=depth).values():
                        if len(path) == depth + 1:
                            target_node = path[-1]
                            score = 4.0 / depth
                            results.append((target_node, depth, score, f"Ù…Ø³ÛŒØ± {depth} Ø³Ø·Ø­ÛŒ"))
        
        return results
    
    def _search_drug_related(self, matched_nodes: Dict[str, str], intent: Dict, max_depth: int) -> List[Tuple[str, int, float, str]]:
        """Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø¯Ø§Ø±ÙˆÙ‡Ø§"""
        results = []
        
        for token, node_id in matched_nodes.items():
            if self.G.nodes[node_id]['kind'] == 'Drug':
                # ÛŒØ§ÙØªÙ† Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø§ÛŒÙ† Ø¯Ø§Ø±Ùˆ Ø¯Ø±Ù…Ø§Ù† Ù…ÛŒâ€ŒÚ©Ù†Ø¯
                for neighbor in self.G.neighbors(node_id):
                    if self.G.nodes[neighbor]['kind'] == 'Disease':
                        edge_data = self.G.get_edge_data(node_id, neighbor)
                        if edge_data and 'treats' in edge_data.get('metaedge', ''):
                            results.append((neighbor, 1, 5.0, f"Ø¯Ø±Ù…Ø§Ù† Ø´Ø¯Ù‡ ØªÙˆØ³Ø· {self.G.nodes[node_id]['name']}"))
                
                # ÛŒØ§ÙØªÙ† Ú˜Ù†â€ŒÙ‡Ø§ÛŒ Ù‡Ø¯Ù
                for neighbor in self.G.neighbors(node_id):
                    if self.G.nodes[neighbor]['kind'] == 'Gene':
                        edge_data = self.G.get_edge_data(node_id, neighbor)
                        if edge_data:
                            results.append((neighbor, 1, 4.5, f"Ù‡Ø¯Ù {self.G.nodes[node_id]['name']}"))
        
        return results
    
    def _search_gene_function(self, matched_nodes: Dict[str, str], intent: Dict, max_depth: int) -> List[Tuple[str, int, float, str]]:
        """Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ú˜Ù†â€ŒÙ‡Ø§"""
        results = []
        
        for token, node_id in matched_nodes.items():
            if self.G.nodes[node_id]['kind'] == 'Gene':
                # ÛŒØ§ÙØªÙ† ÙØ±Ø¢ÛŒÙ†Ø¯Ù‡Ø§ÛŒ Ø²ÛŒØ³ØªÛŒ Ù…Ø±ØªØ¨Ø·
                for neighbor in self.G.neighbors(node_id):
                    if self.G.nodes[neighbor]['kind'] == 'Biological_Process':
                        edge_data = self.G.get_edge_data(node_id, neighbor)
                        if edge_data:
                            results.append((neighbor, 1, 4.5, f"ÙØ±Ø¢ÛŒÙ†Ø¯ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ {self.G.nodes[node_id]['name']}"))
                
                # ÛŒØ§ÙØªÙ† Ú˜Ù†â€ŒÙ‡Ø§ÛŒ ØªØ¹Ø§Ù…Ù„ÛŒ
                for neighbor in self.G.neighbors(node_id):
                    if self.G.nodes[neighbor]['kind'] == 'Gene':
                        edge_data = self.G.get_edge_data(node_id, neighbor)
                        if edge_data and 'interacts_with' in edge_data.get('metaedge', ''):
                            results.append((neighbor, 1, 4.0, f"ØªØ¹Ø§Ù…Ù„ Ø¨Ø§ {self.G.nodes[node_id]['name']}"))
        
        return results
    
    def _search_general(self, matched_nodes: Dict[str, str], intent: Dict, max_depth: int) -> List[Tuple[str, int, float, str]]:
        """Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¹Ù…ÙˆÙ…ÛŒ"""
        results = []
        
        for token, node_id in matched_nodes.items():
            # Ø¬Ø³ØªØ¬ÙˆÛŒ Ù‡Ù…Ø³Ø§ÛŒÙ‡â€ŒÙ‡Ø§
            for neighbor in self.G.neighbors(node_id):
                score = 4.0
                results.append((neighbor, 1, score, f"Ù‡Ù…Ø³Ø§ÛŒÙ‡ {self.G.nodes[node_id]['name']}"))
            
            # Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¹Ù…ÛŒÙ‚â€ŒØªØ±
            for depth in range(2, max_depth + 1):
                for path in nx.single_source_shortest_path(self.G, node_id, cutoff=depth).values():
                    if len(path) == depth + 1:
                        target_node = path[-1]
                        score = 3.0 / depth
                        results.append((target_node, depth, score, f"Ù…Ø³ÛŒØ± {depth} Ø³Ø·Ø­ÛŒ"))
        
        return results
    
    def match_tokens_to_nodes(self, tokens: List[str]) -> Dict[str, str]:
        """ØªØ·Ø¨ÛŒÙ‚ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ Ø¨Ø§ Ù†ÙˆØ¯Ù‡Ø§ÛŒ Ú¯Ø±Ø§Ù"""
        matched = {}
        for token in tokens:
            token_lower = token.lower()
            for node_id, attrs in self.G.nodes(data=True):
                if token_lower in attrs['name'].lower():
                    matched[token] = node_id
                    break
        return matched
    
    def bfs_search(self, start_node: str, max_depth: int = 2) -> List[Tuple[str, int]]:
        """Ø¬Ø³ØªØ¬ÙˆÛŒ Ø³Ø·Ø­ Ø§ÙˆÙ„"""
        visited = set()
        queue = deque([(start_node, 0)])
        result = []
        
        while queue:
            node, depth = queue.popleft()
            if node in visited or depth > max_depth:
                continue
            visited.add(node)
            result.append((node, depth))
            for neighbor in self.G.neighbors(node):
                if neighbor not in visited:
                    queue.append((neighbor, depth + 1))
        
        return result
    
    def dfs_search(self, start_node: str, max_depth: int = 2) -> List[Tuple[str, int]]:
        """Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¹Ù…ÛŒÙ‚ Ø§ÙˆÙ„"""
        visited = set()
        result = []
        
        def dfs(node, depth):
            if depth > max_depth or node in visited:
                return
            visited.add(node)
            result.append((node, depth))
            for neighbor in self.G.neighbors(node):
                if neighbor not in visited:
                    dfs(neighbor, depth + 1)
        
        dfs(start_node, 0)
        return result
    
    def get_shortest_paths(self, source: str, target: str, max_paths: int = 3) -> List[List[str]]:
        """ÛŒØ§ÙØªÙ† Ú©ÙˆØªØ§Ù‡â€ŒØªØ±ÛŒÙ† Ù…Ø³ÛŒØ±Ù‡Ø§"""
        try:
            paths = list(nx.all_shortest_paths(self.G, source=source, target=target))
            return paths[:max_paths]
        except nx.NetworkXNoPath:
            return []
    
    def get_neighbors_by_type(self, node_id: str, kind_filter: str = None) -> List[Tuple[str, str]]:
        """Ø¯Ø±ÛŒØ§ÙØª Ù‡Ù…Ø³Ø§ÛŒÙ‡â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹"""
        neighbors = []
        for neighbor in self.G.neighbors(node_id):
            kind = self.G.nodes[neighbor].get('kind')
            if kind_filter is None or kind == kind_filter:
                neighbors.append((neighbor, self.G.nodes[neighbor]['name']))
        return neighbors
    
    def hybrid_search(self, nodes: List[str], max_depth: int = 2) -> List[Tuple[str, int]]:
        """Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØ±Ú©ÛŒØ¨ÛŒ"""
        all_results = []
        for node in nodes:
            bfs_result = self.bfs_search(node, max_depth)
            all_results.extend(bfs_result)
        
        # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§ Ùˆ Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¹Ù…Ù‚
        unique_results = {}
        for node, depth in all_results:
            if node not in unique_results or depth < unique_results[node]:
                unique_results[node] = depth
        
        return sorted(unique_results.items(), key=lambda x: x[1])
    
    def multi_method_search(self, nodes: List[str], max_depth: int = 2) -> List[Tuple[str, int, str]]:
        """Ø¬Ø³ØªØ¬ÙˆÛŒ Ú†Ù†Ø¯ Ø±ÙˆØ´ÛŒ - ØªØ±Ú©ÛŒØ¨ BFSØŒ DFSØŒ Ùˆ Ù‡Ù…Ø³Ø§ÛŒÙ‡â€ŒÙ‡Ø§"""
        all_results = []
        
        for node in nodes:
            # BFS
            bfs_result = self.bfs_search(node, max_depth)
            for n, depth in bfs_result:
                all_results.append((n, depth, 'BFS'))
            
            # DFS
            dfs_result = self.dfs_search(node, max_depth)
            for n, depth in dfs_result:
                all_results.append((n, depth, 'DFS'))
            
            # Ù‡Ù…Ø³Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ…
            neighbors = self.get_neighbors_by_type(node)
            for nid, name in neighbors:
                all_results.append((nid, 1, 'Neighbors'))
        
        # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§ Ùˆ Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ
        unique_results = {}
        for node, depth, method in all_results:
            if node not in unique_results:
                unique_results[node] = {'depth': depth, 'methods': [method], 'score': 1.0}
            else:
                unique_results[node]['methods'].append(method)
                unique_results[node]['score'] += 0.5  # Ø§Ù…ØªÛŒØ§Ø² Ø¨ÛŒØ´ØªØ± Ø¨Ø±Ø§ÛŒ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
                if depth < unique_results[node]['depth']:
                    unique_results[node]['depth'] = depth
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù…ØªÛŒØ§Ø² Ùˆ Ø¹Ù…Ù‚
        sorted_results = []
        for node, info in unique_results.items():
            sorted_results.append((node, info['depth'], ', '.join(info['methods'])))
        
        return sorted(sorted_results, key=lambda x: (x[1], -len(x[2].split(', '))))
    
    def ensemble_search(self, nodes: List[str], max_depth: int = 2) -> List[Tuple[str, int, float]]:
        """Ø¬Ø³ØªØ¬ÙˆÛŒ Ú¯Ø±ÙˆÙ‡ÛŒ - ØªØ±Ú©ÛŒØ¨ Ù‡Ù…Ù‡ Ø±ÙˆØ´â€ŒÙ‡Ø§ Ø¨Ø§ ÙˆØ²Ù†â€ŒØ¯Ù‡ÛŒ"""
        method_weights = {
            'BFS': 1.0,
            'DFS': 0.8,
            'SHORTEST_PATH': 1.2,
            'NEIGHBORS': 0.9
        }
        
        all_results = {}
        
        for node in nodes:
            # BFS
            bfs_result = self.bfs_search(node, max_depth)
            for n, depth in bfs_result:
                if n not in all_results:
                    all_results[n] = {'score': 0, 'depth': depth, 'count': 0}
                all_results[n]['score'] += method_weights['BFS'] / (depth + 1)
                all_results[n]['count'] += 1
            
            # DFS
            dfs_result = self.dfs_search(node, max_depth)
            for n, depth in dfs_result:
                if n not in all_results:
                    all_results[n] = {'score': 0, 'depth': depth, 'count': 0}
                all_results[n]['score'] += method_weights['DFS'] / (depth + 1)
                all_results[n]['count'] += 1
            
            # Ù‡Ù…Ø³Ø§ÛŒÙ‡â€ŒÙ‡Ø§
            neighbors = self.get_neighbors_by_type(node)
            for nid, name in neighbors:
                if nid not in all_results:
                    all_results[nid] = {'score': 0, 'depth': 1, 'count': 0}
                all_results[nid]['score'] += method_weights['NEIGHBORS']
                all_results[nid]['count'] += 1
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù…ØªÛŒØ§Ø²
        sorted_results = []
        for node, info in all_results.items():
            final_score = info['score'] * (1 + 0.1 * info['count'])  # Ù¾Ø§Ø¯Ø§Ø´ Ø¨Ø±Ø§ÛŒ ØªÚ©Ø±Ø§Ø±
            sorted_results.append((node, info['depth'], final_score))
        
        return sorted(sorted_results, key=lambda x: x[2], reverse=True)
    
    def adaptive_search(self, nodes: List[str], max_depth: int = 2) -> List[Tuple[str, int, str]]:
        """Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØ·Ø¨ÛŒÙ‚ÛŒ - Ø§Ù†ØªØ®Ø§Ø¨ Ø±ÙˆØ´ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ù†ÙˆØ¯"""
        all_results = []
        
        for node in nodes:
            node_kind = self.G.nodes[node]['kind']
            
            # Ø§Ù†ØªØ®Ø§Ø¨ Ø±ÙˆØ´ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ù†ÙˆØ¯
            if node_kind in ['Gene', 'Disease']:
                # Ø¨Ø±Ø§ÛŒ Ú˜Ù†â€ŒÙ‡Ø§ Ùˆ Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ Ø§Ø² BFS Ùˆ Ù‡Ù…Ø³Ø§ÛŒÙ‡â€ŒÙ‡Ø§
                bfs_result = self.bfs_search(node, max_depth)
                for n, depth in bfs_result:
                    all_results.append((n, depth, 'BFS'))
                
                neighbors = self.get_neighbors_by_type(node)
                for nid, name in neighbors:
                    all_results.append((nid, 1, 'Neighbors'))
            
            elif node_kind in ['Drug', 'Compound']:
                # Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø±ÙˆÙ‡Ø§ Ø§Ø² DFS Ùˆ Ú©ÙˆØªØ§Ù‡â€ŒØªØ±ÛŒÙ† Ù…Ø³ÛŒØ±
                dfs_result = self.dfs_search(node, max_depth)
                for n, depth in dfs_result:
                    all_results.append((n, depth, 'DFS'))
            
            elif node_kind in ['Anatomy', 'Biological Process']:
                # Ø¨Ø±Ø§ÛŒ Ø¢Ù†Ø§ØªÙˆÙ…ÛŒ Ùˆ ÙØ±Ø¢ÛŒÙ†Ø¯Ù‡Ø§ÛŒ Ø²ÛŒØ³ØªÛŒ Ø§Ø² Ù‡Ù…Ù‡ Ø±ÙˆØ´â€ŒÙ‡Ø§
                bfs_result = self.bfs_search(node, max_depth)
                for n, depth in bfs_result:
                    all_results.append((n, depth, 'BFS'))
                
                dfs_result = self.dfs_search(node, max_depth)
                for n, depth in dfs_result:
                    all_results.append((n, depth, 'DFS'))
            
            else:
                # Ø¨Ø±Ø§ÛŒ Ø¨Ù‚ÛŒÙ‡ Ø§Ø² Ø±ÙˆØ´ ØªØ±Ú©ÛŒØ¨ÛŒ
                hybrid_result = self.hybrid_search([node], max_depth)
                for n, depth in hybrid_result:
                    all_results.append((n, depth, 'Hybrid'))
        
        # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§
        unique_results = {}
        for node, depth, method in all_results:
            if node not in unique_results:
                unique_results[node] = (depth, method)
            elif depth < unique_results[node][0]:
                unique_results[node] = (depth, method)
        
        return [(node, depth, method) for node, (depth, method) in unique_results.items()]
    
    def retrieve_information(self, query: str, method: RetrievalMethod, 
                           max_depth: int = 2, max_nodes: int = 10) -> RetrievalResult:
        """Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø² Ú¯Ø±Ø§Ù"""
        print(f"ğŸ” Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ø§ Ø±ÙˆØ´ {method.value if hasattr(method, 'value') else method}...")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
        keywords = self.extract_keywords(query)
        print(f"Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ: {keywords}")
        
        # ØªØ·Ø¨ÛŒÙ‚ Ø¨Ø§ Ù†ÙˆØ¯Ù‡Ø§ÛŒ Ú¯Ø±Ø§Ù
        matches = self.match_tokens_to_nodes(keywords)
        print(f"ØªØ·Ø¨ÛŒÙ‚â€ŒÙ‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡: {matches}")
        
        nodes = []
        edges = []
        paths = []
        
        if method == RetrievalMethod.BFS:
            # BFS Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†ÙˆØ¯ ØªØ·Ø¨ÛŒÙ‚ ÛŒØ§ÙØªÙ‡
            for token, node_id in matches.items():
                bfs_result = self.bfs_search(node_id, max_depth)
                for node, depth in bfs_result[:max_nodes]:
                    nodes.append(GraphNode(
                        id=node,
                        name=self.G.nodes[node]['name'],
                        kind=self.G.nodes[node]['kind'],
                        depth=depth
                    ))
        
        elif method == RetrievalMethod.DFS:
            # DFS Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†ÙˆØ¯ ØªØ·Ø¨ÛŒÙ‚ ÛŒØ§ÙØªÙ‡
            for token, node_id in matches.items():
                dfs_result = self.dfs_search(node_id, max_depth)
                for node, depth in dfs_result[:max_nodes]:
                    nodes.append(GraphNode(
                        id=node,
                        name=self.G.nodes[node]['name'],
                        kind=self.G.nodes[node]['kind'],
                        depth=depth
                    ))
        
        elif method == RetrievalMethod.SHORTEST_PATH:
            # Ú©ÙˆØªØ§Ù‡â€ŒØªØ±ÛŒÙ† Ù…Ø³ÛŒØ± Ø¨ÛŒÙ† Ù†ÙˆØ¯Ù‡Ø§
            if len(matches) >= 2:
                node_ids = list(matches.values())
                for i in range(len(node_ids)):
                    for j in range(i+1, len(node_ids)):
                        paths.extend(self.get_shortest_paths(node_ids[i], node_ids[j]))
                        
                        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù†ÙˆØ¯Ù‡Ø§ÛŒ Ù…Ø³ÛŒØ±
                        for path in paths:
                            for k, node in enumerate(path):
                                nodes.append(GraphNode(
                                    id=node,
                                    name=self.G.nodes[node]['name'],
                                    kind=self.G.nodes[node]['kind'],
                                    depth=k
                                ))
        
        elif method == RetrievalMethod.NEIGHBORS:
            # Ù‡Ù…Ø³Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ…
            for token, node_id in matches.items():
                neighbors = self.get_neighbors_by_type(node_id)
                for nid, name in neighbors[:max_nodes]:
                    nodes.append(GraphNode(
                        id=nid,
                        name=name,
                        kind=self.G.nodes[nid]['kind'],
                        depth=1
                    ))
        
        elif method == RetrievalMethod.HYBRID:
            # ØªØ±Ú©ÛŒØ¨ÛŒ Ø§Ø² Ø±ÙˆØ´â€ŒÙ‡Ø§
            if len(matches) >= 2:
                node_ids = list(matches.values())
                hybrid_result = self.hybrid_search(node_ids, max_depth)
                for node, depth in hybrid_result[:max_nodes]:
                    nodes.append(GraphNode(
                        id=node,
                        name=self.G.nodes[node]['name'],
                        kind=self.G.nodes[node]['kind'],
                        depth=depth
                    ))
        
        elif method == RetrievalMethod.MULTI_METHOD:
            # Ø¬Ø³ØªØ¬ÙˆÛŒ Ú†Ù†Ø¯ Ø±ÙˆØ´ÛŒ
            node_ids = list(matches.values())
            multi_result = self.multi_method_search(node_ids, max_depth)
            for node, depth, methods in multi_result[:max_nodes]:
                nodes.append(GraphNode(
                    id=node,
                    name=self.G.nodes[node]['name'],
                    kind=self.G.nodes[node]['kind'],
                    depth=depth,
                    score=len(methods.split(', '))  # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ Ø±ÙˆØ´â€ŒÙ‡Ø§
                ))
        
        elif method == RetrievalMethod.ENSEMBLE:
            # Ø¬Ø³ØªØ¬ÙˆÛŒ Ú¯Ø±ÙˆÙ‡ÛŒ
            node_ids = list(matches.values())
            ensemble_result = self.ensemble_search(node_ids, max_depth)
            for node, depth, score in ensemble_result[:max_nodes]:
                nodes.append(GraphNode(
                    id=node,
                    name=self.G.nodes[node]['name'],
                    kind=self.G.nodes[node]['kind'],
                    depth=depth,
                    score=score
                ))
        
        elif method == RetrievalMethod.ADAPTIVE:
            # Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØ·Ø¨ÛŒÙ‚ÛŒ
            node_ids = list(matches.values())
            adaptive_result = self.adaptive_search(node_ids, max_depth)
            for node, depth, method in adaptive_result[:max_nodes]:
                nodes.append(GraphNode(
                    id=node,
                    name=self.G.nodes[node]['name'],
                    kind=self.G.nodes[node]['kind'],
                    depth=depth
                ))
        
        elif method == RetrievalMethod.INTELLIGENT:
            # Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯
            intelligent_result = self.intelligent_semantic_search(query, max_depth)
            
            # ØªØ¨Ø¯ÛŒÙ„ Ù†ØªØ§ÛŒØ¬ Ø¨Ù‡ GraphNode
            for node_id, depth, score, reason in intelligent_result[:max_nodes]:
                nodes.append(GraphNode(
                    id=node_id,
                    name=self.G.nodes[node_id]['name'],
                    kind=self.G.nodes[node_id]['kind'],
                    depth=depth,
                    score=score
                ))
            
            # ÛŒØ§ÙØªÙ† Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø§Ø±ØªØ¨Ø§Ø·ÛŒ Ø¨ÛŒÙ† Ù†ÙˆØ¯Ù‡Ø§
            if len(nodes) >= 2:
                node_ids = [node.id for node in nodes]
                for i in range(len(node_ids)):
                    for j in range(i+1, len(node_ids)):
                        paths.extend(self.get_shortest_paths(node_ids[i], node_ids[j]))
            
            # ÛŒØ§ÙØªÙ† ÛŒØ§Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·
            for node in nodes:
                for neighbor in self.G.neighbors(node.id):
                    if any(n.id == neighbor for n in nodes):
                        edge_data = self.G.get_edge_data(node.id, neighbor)
                        if edge_data:
                            edges.append(GraphEdge(
                                source=node.id,
                                target=neighbor,
                                relation=edge_data.get('metaedge', 'related'),
                                weight=edge_data.get('weight', 1.0)
                            ))
        
        elif method == RetrievalMethod.NO_RETRIEVAL:
            # Ø¨Ø¯ÙˆÙ† Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ - ÙÙ‚Ø· Ù…Ø¯Ù„
            print("ğŸ” Ø¨Ø¯ÙˆÙ† Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø² Ú¯Ø±Ø§Ù - ÙÙ‚Ø· Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„")
            # Ø§ÛŒØ¬Ø§Ø¯ ÛŒÚ© Ù†ÙˆØ¯ Ø®Ø§Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ø­ÙØ¸ Ø³Ø§Ø®ØªØ§Ø±
            nodes.append(GraphNode(
                id="no_retrieval",
                name="Ø¨Ø¯ÙˆÙ† Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ",
                kind="System",
                depth=0
            ))
        
        # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§
        unique_nodes = {}
        for node in nodes:
            if node.id not in unique_nodes:
                unique_nodes[node.id] = node
        
        nodes = list(unique_nodes.values())
        
        # Ø§ÛŒØ¬Ø§Ø¯ ÛŒØ§Ù„â€ŒÙ‡Ø§ Ø¨ÛŒÙ† Ù†ÙˆØ¯Ù‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·
        for i, node1 in enumerate(nodes):
            for j, node2 in enumerate(nodes[i+1:], i+1):
                if self.G.has_edge(node1.id, node2.id):
                    edge_data = self.G.get_edge_data(node1.id, node2.id)
                    edges.append(GraphEdge(
                        source=node1.id,
                        target=node2.id,
                        relation=edge_data['metaedge']
                    ))
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡
        context_text = self.create_context_text(nodes, edges, paths)
        
        return RetrievalResult(
            nodes=nodes,
            edges=edges,
            paths=paths,
            context_text=context_text,
            method=method.value if hasattr(method, 'value') else str(method),
            query=query
        )
    
    def create_context_text(self, nodes: List[GraphNode], edges: List[GraphEdge], 
                           paths: List[List[str]]) -> str:
        """Ø§ÛŒØ¬Ø§Ø¯ Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ Ø§Ø² Ù†ØªØ§ÛŒØ¬ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ"""
        context_parts = []
        
        # Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù†ÙˆØ¯Ù‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹
        nodes_by_type = {}
        for node in nodes:
            if node.kind not in nodes_by_type:
                nodes_by_type[node.kind] = []
            nodes_by_type[node.kind].append(node)
        
        # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù†ÙˆØ¯Ù‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡
        context_parts.append("ğŸ“Š ENTITIES FOUND IN KNOWLEDGE GRAPH:")
        context_parts.append("=" * 50)
        
        for kind, kind_nodes in nodes_by_type.items():
            context_parts.append(f"\nğŸ”¹ {kind.upper()} ({len(kind_nodes)} entities):")
            for node in kind_nodes:
                score_info = f" [Score: {node.score:.2f}]" if hasattr(node, 'score') and node.score != 1.0 else ""
                depth_info = f" [Depth: {node.depth}]" if node.depth > 0 else ""
                context_parts.append(f"  â€¢ {node.name}{score_info}{depth_info}")
        
        # Ø±ÙˆØ§Ø¨Ø· Ø¨Ø§ Ø¬Ø²Ø¦ÛŒØ§Øª Ø¨ÛŒØ´ØªØ±
        if edges:
            context_parts.append(f"\nğŸ”— RELATIONSHIPS ({len(edges)} connections):")
            context_parts.append("=" * 50)
            
            # Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø±ÙˆØ§Ø¨Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹
            relations_by_type = {}
            for edge in edges:
                if edge.relation not in relations_by_type:
                    relations_by_type[edge.relation] = []
                relations_by_type[edge.relation].append(edge)
            
            for relation, relation_edges in relations_by_type.items():
                context_parts.append(f"\nğŸ“Œ {relation.upper()} ({len(relation_edges)} connections):")
                for edge in relation_edges[:10]:  # Ø­Ø¯Ø§Ú©Ø«Ø± 10 Ø±Ø§Ø¨Ø·Ù‡ Ø§Ø² Ù‡Ø± Ù†ÙˆØ¹
                    source_name = next(n.name for n in nodes if n.id == edge.source)
                    target_name = next(n.name for n in nodes if n.id == edge.target)
                    source_kind = next(n.kind for n in nodes if n.id == edge.source)
                    target_kind = next(n.kind for n in nodes if n.id == edge.target)
                    context_parts.append(f"  â€¢ {source_name} ({source_kind}) â†’ {target_name} ({target_kind})")
        
        # Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ù…Ù‡Ù…
        if paths:
            context_parts.append(f"\nğŸ›¤ï¸ IMPORTANT PATHS ({len(paths)} paths):")
            context_parts.append("=" * 50)
            for i, path in enumerate(paths[:5], 1):  # Ø­Ø¯Ø§Ú©Ø«Ø± 5 Ù…Ø³ÛŒØ±
                path_names = [self.G.nodes[node]['name'] for node in path]
                path_kinds = [self.G.nodes[node]['kind'] for node in path]
                context_parts.append(f"\nPath {i} ({len(path)} steps):")
                for j, (name, kind) in enumerate(zip(path_names, path_kinds)):
                    context_parts.append(f"  {j+1}. {name} ({kind})")
        
        # Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ
        context_parts.append(f"\nğŸ“ˆ SUMMARY:")
        context_parts.append("=" * 50)
        context_parts.append(f"â€¢ Total entities: {len(nodes)}")
        context_parts.append(f"â€¢ Total relationships: {len(edges)}")
        context_parts.append(f"â€¢ Entity types: {len(nodes_by_type)}")
        context_parts.append(f"â€¢ Relationship types: {len(set(e.relation for e in edges))}")
        if paths:
            context_parts.append(f"â€¢ Important paths: {len(paths)}")
        
        # Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ ØªÙØ³ÛŒØ±
        context_parts.append(f"\nğŸ’¡ INTERPRETATION GUIDE:")
        context_parts.append("=" * 50)
        context_parts.append("â€¢ Genes often participate in biological processes")
        context_parts.append("â€¢ Drugs can treat diseases and interact with genes")
        context_parts.append("â€¢ Diseases are associated with specific genes and symptoms")
        context_parts.append("â€¢ Anatomy expresses genes and can be affected by diseases")
        context_parts.append("â€¢ Compounds can interact with genes and biological processes")
        
        return "\n".join(context_parts)
    
    def generate_answer(self, retrieval_result: RetrievalResult, 
                       model: GenerationModel) -> GenerationResult:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ØªØ§ÛŒØ¬ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ"""
        print(f"ğŸ¤– ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ Ù…Ø¯Ù„ {model.value}...")
        
        if model == GenerationModel.SIMPLE:
            answer = self.simple_template_generation(retrieval_result)
            confidence = 0.7
        elif model == GenerationModel.GPT_SIMULATION:
            answer = self.gpt_simulation_generation(retrieval_result)
            confidence = 0.85
        elif model == GenerationModel.CUSTOM:
            answer = self.custom_generation(retrieval_result)
            confidence = 0.9
        elif model == GenerationModel.HUGGINGFACE:
            answer = self.huggingface_generation(retrieval_result)
            confidence = 0.92
        elif model == GenerationModel.OPENAI_GPT:
            answer = self.openai_gpt_generation(retrieval_result)
            confidence = 0.95
        elif model == GenerationModel.ANTHROPIC_CLAUDE:
            answer = self.anthropic_claude_generation(retrieval_result)
            confidence = 0.94
        elif model == GenerationModel.GOOGLE_GEMINI:
            answer = self.google_gemini_generation(retrieval_result)
            confidence = 0.93
        else:
            answer = "Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª."
            confidence = 0.0
        
        return GenerationResult(
            answer=answer,
            model=model.value,
            context_used=retrieval_result.context_text,
            confidence=confidence
        )
    
    def simple_template_generation(self, retrieval_result: RetrievalResult) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø³Ø§Ø¯Ù‡ Ø¨Ø§ Ù‚Ø§Ù„Ø¨ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡"""
        query_lower = retrieval_result.query.lower()
        
        # ØªØ­Ù„ÛŒÙ„ Ù†ÙˆØ¹ Ø³ÙˆØ§Ù„
        question_type = self._analyze_question_type(query_lower)
        
        if question_type == "relationship":
            return self._generate_relationship_answer(retrieval_result)
        elif question_type == "drug_treatment":
            return self._generate_drug_treatment_answer(retrieval_result)
        elif question_type == "gene_function":
            return self._generate_gene_function_answer(retrieval_result)
        elif question_type == "disease_info":
            return self._generate_disease_info_answer(retrieval_result)
        elif question_type == "anatomy_expression":
            return self._generate_anatomy_expression_answer(retrieval_result)
        else:
            return self._generate_general_answer(retrieval_result)
    
    def _analyze_question_type(self, query_lower: str) -> str:
        """ØªØ­Ù„ÛŒÙ„ Ù†ÙˆØ¹ Ø³ÙˆØ§Ù„"""
        if any(word in query_lower for word in ["relationship", "relation", "connect", "link"]):
            return "relationship"
        elif any(word in query_lower for word in ["drug", "treat", "medicine", "therapy"]):
            return "drug_treatment"
        elif any(word in query_lower for word in ["gene", "function", "regulate", "express"]):
            return "gene_function"
        elif any(word in query_lower for word in ["disease", "disorder", "condition", "symptom"]):
            return "disease_info"
        elif any(word in query_lower for word in ["anatomy", "organ", "tissue", "heart", "brain", "liver"]):
            return "anatomy_expression"
        else:
            return "general"
    
    def _generate_relationship_answer(self, retrieval_result: RetrievalResult) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„Ø§Øª Ø±Ø§Ø¨Ø·Ù‡"""
        if not retrieval_result.edges:
            return "âŒ No direct relationships found between the specified entities in the knowledge graph."
        
        answer_parts = ["ğŸ”— RELATIONSHIPS FOUND:"]
        
        # Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø±ÙˆØ§Ø¨Ø·
        relations_by_type = {}
        for edge in retrieval_result.edges:
            if edge.relation not in relations_by_type:
                relations_by_type[edge.relation] = []
            relations_by_type[edge.relation].append(edge)
        
        for relation, edges in relations_by_type.items():
            answer_parts.append(f"\nğŸ“Œ {relation.upper()} ({len(edges)} connections):")
            for edge in edges[:5]:  # Ø­Ø¯Ø§Ú©Ø«Ø± 5 Ø±Ø§Ø¨Ø·Ù‡ Ø§Ø² Ù‡Ø± Ù†ÙˆØ¹
                source_name = next(n.name for n in retrieval_result.nodes if n.id == edge.source)
                target_name = next(n.name for n in retrieval_result.nodes if n.id == edge.target)
                answer_parts.append(f"  â€¢ {source_name} â†’ {target_name}")
        
        return "\n".join(answer_parts)
    
    def _generate_drug_treatment_answer(self, retrieval_result: RetrievalResult) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„Ø§Øª Ø¯Ø±Ù…Ø§Ù† Ø¯Ø§Ø±ÙˆÛŒÛŒ"""
        drug_nodes = [n for n in retrieval_result.nodes if n.kind in ['Drug', 'Compound']]
        disease_nodes = [n for n in retrieval_result.nodes if n.kind == 'Disease']
        
        if not drug_nodes and not disease_nodes:
            return "âŒ No drug or disease information found in the retrieved context."
        
        answer_parts = ["ğŸ’Š DRUG-DISEASE RELATIONSHIPS:"]
        
        # Ø±ÙˆØ§Ø¨Ø· Ø¯Ø±Ù…Ø§Ù†
        treatment_edges = [e for e in retrieval_result.edges if 'treat' in e.relation.lower() or 'therapy' in e.relation.lower()]
        if treatment_edges:
            answer_parts.append(f"\nâœ… TREATMENT RELATIONSHIPS ({len(treatment_edges)} found):")
            for edge in treatment_edges[:10]:
                source_name = next(n.name for n in retrieval_result.nodes if n.id == edge.source)
                target_name = next(n.name for n in retrieval_result.nodes if n.id == edge.target)
                answer_parts.append(f"  â€¢ {source_name} treats {target_name}")
        
        # Ø¯Ø§Ø±ÙˆÙ‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡
        if drug_nodes:
            answer_parts.append(f"\nğŸ’Š DRUGS FOUND ({len(drug_nodes)}):")
            for drug in drug_nodes[:10]:
                answer_parts.append(f"  â€¢ {drug.name}")
        
        # Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡
        if disease_nodes:
            answer_parts.append(f"\nğŸ¥ DISEASES FOUND ({len(disease_nodes)}):")
            for disease in disease_nodes[:10]:
                answer_parts.append(f"  â€¢ {disease.name}")
        
        return "\n".join(answer_parts)
    
    def _generate_gene_function_answer(self, retrieval_result: RetrievalResult) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„Ø§Øª Ø¹Ù…Ù„Ú©Ø±Ø¯ Ú˜Ù†"""
        gene_nodes = [n for n in retrieval_result.nodes if n.kind == 'Gene']
        process_nodes = [n for n in retrieval_result.nodes if n.kind == 'Biological Process']
        
        if not gene_nodes:
            return "âŒ No gene information found in the retrieved context."
        
        answer_parts = ["ğŸ§¬ GENE FUNCTION ANALYSIS:"]
        
        # Ú˜Ù†â€ŒÙ‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡
        answer_parts.append(f"\nğŸ§¬ GENES FOUND ({len(gene_nodes)}):")
        for gene in gene_nodes[:10]:
            answer_parts.append(f"  â€¢ {gene.name}")
        
        # ÙØ±Ø¢ÛŒÙ†Ø¯Ù‡Ø§ÛŒ Ø²ÛŒØ³ØªÛŒ Ù…Ø±ØªØ¨Ø·
        if process_nodes:
            answer_parts.append(f"\nâš™ï¸ BIOLOGICAL PROCESSES ({len(process_nodes)}):")
            for process in process_nodes[:10]:
                answer_parts.append(f"  â€¢ {process.name}")
        
        # Ø±ÙˆØ§Ø¨Ø· Ú˜Ù†-ÙØ±Ø¢ÛŒÙ†Ø¯
        gene_process_edges = [e for e in retrieval_result.edges if 'participate' in e.relation.lower() or 'regulate' in e.relation.lower()]
        if gene_process_edges:
            answer_parts.append(f"\nğŸ”— GENE-PROCESS RELATIONSHIPS ({len(gene_process_edges)}):")
            for edge in gene_process_edges[:10]:
                source_name = next(n.name for n in retrieval_result.nodes if n.id == edge.source)
                target_name = next(n.name for n in retrieval_result.nodes if n.id == edge.target)
                answer_parts.append(f"  â€¢ {source_name} â†’ {target_name}")
        
        return "\n".join(answer_parts)
    
    def _generate_disease_info_answer(self, retrieval_result: RetrievalResult) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„Ø§Øª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨ÛŒÙ…Ø§Ø±ÛŒ"""
        disease_nodes = [n for n in retrieval_result.nodes if n.kind == 'Disease']
        gene_nodes = [n for n in retrieval_result.nodes if n.kind == 'Gene']
        symptom_nodes = [n for n in retrieval_result.nodes if n.kind == 'Symptom']
        
        if not disease_nodes:
            return "âŒ No disease information found in the retrieved context."
        
        answer_parts = ["ğŸ¥ DISEASE ANALYSIS:"]
        
        # Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡
        answer_parts.append(f"\nğŸ¥ DISEASES FOUND ({len(disease_nodes)}):")
        for disease in disease_nodes[:10]:
            answer_parts.append(f"  â€¢ {disease.name}")
        
        # Ú˜Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·
        if gene_nodes:
            answer_parts.append(f"\nğŸ§¬ ASSOCIATED GENES ({len(gene_nodes)}):")
            for gene in gene_nodes[:10]:
                answer_parts.append(f"  â€¢ {gene.name}")
        
        # Ø¹Ù„Ø§Ø¦Ù… Ù…Ø±ØªØ¨Ø·
        if symptom_nodes:
            answer_parts.append(f"\nğŸ¤’ ASSOCIATED SYMPTOMS ({len(symptom_nodes)}):")
            for symptom in symptom_nodes[:10]:
                answer_parts.append(f"  â€¢ {symptom.name}")
        
        return "\n".join(answer_parts)
    
    def _generate_anatomy_expression_answer(self, retrieval_result: RetrievalResult) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„Ø§Øª Ø¨ÛŒØ§Ù† Ú˜Ù† Ø¯Ø± Ø¢Ù†Ø§ØªÙˆÙ…ÛŒ"""
        anatomy_nodes = [n for n in retrieval_result.nodes if n.kind == 'Anatomy']
        gene_nodes = [n for n in retrieval_result.nodes if n.kind == 'Gene']
        
        if not anatomy_nodes:
            return "âŒ No anatomy information found in the retrieved context."
        
        answer_parts = ["ğŸ«€ ANATOMY-GENE EXPRESSION:"]
        
        # Ø§Ù†Ø¯Ø§Ù…â€ŒÙ‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡
        answer_parts.append(f"\nğŸ«€ ANATOMICAL STRUCTURES ({len(anatomy_nodes)}):")
        for anatomy in anatomy_nodes[:10]:
            answer_parts.append(f"  â€¢ {anatomy.name}")
        
        # Ú˜Ù†â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ§Ù† Ø´Ø¯Ù‡
        if gene_nodes:
            answer_parts.append(f"\nğŸ§¬ EXPRESSED GENES ({len(gene_nodes)}):")
            for gene in gene_nodes[:10]:
                answer_parts.append(f"  â€¢ {gene.name}")
        
        # Ø±ÙˆØ§Ø¨Ø· Ø¨ÛŒØ§Ù†
        expression_edges = [e for e in retrieval_result.edges if 'express' in e.relation.lower()]
        if expression_edges:
            answer_parts.append(f"\nğŸ”— EXPRESSION RELATIONSHIPS ({len(expression_edges)}):")
            for edge in expression_edges[:10]:
                source_name = next(n.name for n in retrieval_result.nodes if n.id == edge.source)
                target_name = next(n.name for n in retrieval_result.nodes if n.id == edge.target)
                answer_parts.append(f"  â€¢ {source_name} expresses {target_name}")
        
        return "\n".join(answer_parts)
    
    def _generate_general_answer(self, retrieval_result: RetrievalResult) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¹Ù…ÙˆÙ…ÛŒ"""
        if not retrieval_result.nodes:
            return "âŒ No relevant information found in the knowledge graph for your query."
        
        answer_parts = ["ğŸ“Š GENERAL INFORMATION FOUND:"]
        
        # Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù†ÙˆØ¯Ù‡Ø§
        nodes_by_type = {}
        for node in retrieval_result.nodes:
            if node.kind not in nodes_by_type:
                nodes_by_type[node.kind] = []
            nodes_by_type[node.kind].append(node)
        
        for kind, nodes in nodes_by_type.items():
            answer_parts.append(f"\nğŸ”¹ {kind.upper()} ({len(nodes)} entities):")
            for node in nodes[:5]:
                answer_parts.append(f"  â€¢ {node.name}")
        
        # Ø±ÙˆØ§Ø¨Ø· Ù…Ù‡Ù…
        if retrieval_result.edges:
            answer_parts.append(f"\nğŸ”— KEY RELATIONSHIPS ({len(retrieval_result.edges)}):")
            for edge in retrieval_result.edges[:10]:
                source_name = next(n.name for n in retrieval_result.nodes if n.id == edge.source)
                target_name = next(n.name for n in retrieval_result.nodes if n.id == edge.target)
                answer_parts.append(f"  â€¢ {source_name} â†’ {target_name} ({edge.relation})")
        
        return "\n".join(answer_parts)
    
    def gpt_simulation_generation(self, retrieval_result: RetrievalResult) -> str:
        """Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® GPT Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡"""
        query = retrieval_result.query
        query_lower = query.lower()
        
        # ØªØ­Ù„ÛŒÙ„ Ù†ÙˆØ¹ Ø³ÙˆØ§Ù„
        question_type = self._analyze_question_type(query_lower)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ù‡Ù… Ø§Ø² Ù†ØªØ§ÛŒØ¬ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ
        gene_nodes = [n for n in retrieval_result.nodes if n.kind == 'Gene']
        disease_nodes = [n for n in retrieval_result.nodes if n.kind == 'Disease']
        drug_nodes = [n for n in retrieval_result.nodes if n.kind in ['Drug', 'Compound']]
        anatomy_nodes = [n for n in retrieval_result.nodes if n.kind == 'Anatomy']
        process_nodes = [n for n in retrieval_result.nodes if n.kind == 'Biological Process']
        
        # ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ø³ÙˆØ§Ù„
        if question_type == "relationship":
            return self._generate_intelligent_relationship_answer(retrieval_result, gene_nodes, disease_nodes, drug_nodes)
        elif question_type == "drug_treatment":
            return self._generate_intelligent_drug_answer(retrieval_result, drug_nodes, disease_nodes)
        elif question_type == "gene_function":
            return self._generate_intelligent_gene_answer(retrieval_result, gene_nodes, process_nodes)
        elif question_type == "disease_info":
            return self._generate_intelligent_disease_answer(retrieval_result, disease_nodes, gene_nodes)
        elif question_type == "anatomy_expression":
            return self._generate_intelligent_anatomy_answer(retrieval_result, anatomy_nodes, gene_nodes)
        else:
            return self._generate_intelligent_general_answer(retrieval_result, gene_nodes, disease_nodes, drug_nodes, anatomy_nodes, process_nodes)
    
    def _generate_intelligent_relationship_answer(self, retrieval_result: RetrievalResult, gene_nodes, disease_nodes, drug_nodes) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„Ø§Øª Ø±Ø§Ø¨Ø·Ù‡"""
        if not retrieval_result.edges:
            return "ğŸ” **ØªØ­Ù„ÛŒÙ„ Ø±Ø§Ø¨Ø·Ù‡:**\n\nÙ…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø±Ø§Ø¨Ø·Ù‡ Ù…Ø³ØªÙ‚ÛŒÙ…ÛŒ Ø¨ÛŒÙ† Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø¯Ø± Ú¯Ø±Ø§Ù Ø¯Ø§Ù†Ø´ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø§ÛŒÙ† Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨Ù‡ Ø¯Ù„ÛŒÙ„:\nâ€¢ ÙØ§ØµÙ„Ù‡ Ø²ÛŒØ§Ø¯ Ø¨ÛŒÙ† Ù†ÙˆØ¯Ù‡Ø§ Ø¯Ø± Ú¯Ø±Ø§Ù\nâ€¢ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¹Ù…ÛŒÙ‚â€ŒØªØ±\nâ€¢ Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø±Ø§Ø¨Ø·Ù‡ Ù…Ø³ØªÙ‚ÛŒÙ… Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯"
        
        answer_parts = ["ğŸ” **ØªØ­Ù„ÛŒÙ„ Ø±Ø§Ø¨Ø·Ù‡:**\n"]
        
        # ØªØ­Ù„ÛŒÙ„ Ø±ÙˆØ§Ø¨Ø· Ù…Ù‡Ù…
        important_relations = {}
        for edge in retrieval_result.edges:
            if edge.relation not in important_relations:
                important_relations[edge.relation] = []
            important_relations[edge.relation].append(edge)
        
        # Ù†Ù…Ø§ÛŒØ´ Ù…Ù‡Ù…â€ŒØªØ±ÛŒÙ† Ø±ÙˆØ§Ø¨Ø·
        answer_parts.append("**Ø±ÙˆØ§Ø¨Ø· Ù…Ù‡Ù… ÛŒØ§ÙØª Ø´Ø¯Ù‡:**\n")
        for relation, edges in sorted(important_relations.items(), key=lambda x: len(x[1]), reverse=True)[:3]:
            answer_parts.append(f"â€¢ **{relation}** ({len(edges)} Ø±Ø§Ø¨Ø·Ù‡):")
            for edge in edges[:3]:
                source_name = next(n.name for n in retrieval_result.nodes if n.id == edge.source)
                target_name = next(n.name for n in retrieval_result.nodes if n.id == edge.target)
                answer_parts.append(f"  - {source_name} â†’ {target_name}")
            answer_parts.append("")
        
        # ØªØ­Ù„ÛŒÙ„ Ø¢Ù…Ø§Ø±ÛŒ
        answer_parts.append("**Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ:**")
        answer_parts.append(f"â€¢ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø±ÙˆØ§Ø¨Ø·: {len(retrieval_result.edges)}")
        answer_parts.append(f"â€¢ Ø§Ù†ÙˆØ§Ø¹ Ø±ÙˆØ§Ø¨Ø·: {len(important_relations)}")
        answer_parts.append(f"â€¢ Ù†ÙˆØ¯Ù‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·: {len(retrieval_result.nodes)}")
        
        return "\n".join(answer_parts)
    
    def _generate_intelligent_drug_answer(self, retrieval_result: RetrievalResult, drug_nodes, disease_nodes) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„Ø§Øª Ø¯Ø§Ø±ÙˆÛŒÛŒ"""
        answer_parts = ["ğŸ’Š **ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø±ÙˆÛŒÛŒ:**\n"]
        
        if drug_nodes:
            answer_parts.append("**Ø¯Ø§Ø±ÙˆÙ‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡:**")
            for drug in drug_nodes[:5]:
                score_info = f" (Ø§Ù…ØªÛŒØ§Ø²: {drug.score:.2f})" if hasattr(drug, 'score') and drug.score != 1.0 else ""
                answer_parts.append(f"â€¢ {drug.name}{score_info}")
            answer_parts.append("")
        
        if disease_nodes:
            answer_parts.append("**Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·:**")
            for disease in disease_nodes[:5]:
                answer_parts.append(f"â€¢ {disease.name}")
            answer_parts.append("")
        
        # Ø±ÙˆØ§Ø¨Ø· Ø¯Ø±Ù…Ø§Ù†
        treatment_edges = [e for e in retrieval_result.edges if 'treat' in e.relation.lower() or 'therapy' in e.relation.lower()]
        if treatment_edges:
            answer_parts.append("**Ø±ÙˆØ§Ø¨Ø· Ø¯Ø±Ù…Ø§Ù†ÛŒ:**")
            for edge in treatment_edges[:5]:
                source_name = next(n.name for n in retrieval_result.nodes if n.id == edge.source)
                target_name = next(n.name for n in retrieval_result.nodes if n.id == edge.target)
                answer_parts.append(f"â€¢ {source_name} Ø¯Ø±Ù…Ø§Ù† Ù…ÛŒâ€ŒÚ©Ù†Ø¯: {target_name}")
        
        if not drug_nodes and not disease_nodes:
            answer_parts.append("âŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ø§Ø±ÙˆÛŒÛŒ ÛŒØ§ Ø¨ÛŒÙ…Ø§Ø±ÛŒ Ø¯Ø± Ù†ØªØ§ÛŒØ¬ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
        
        return "\n".join(answer_parts)
    
    def _generate_intelligent_gene_answer(self, retrieval_result: RetrievalResult, gene_nodes, process_nodes) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„Ø§Øª Ú˜Ù†"""
        answer_parts = ["ğŸ§¬ **ØªØ­Ù„ÛŒÙ„ Ú˜Ù†ØªÛŒÚ©ÛŒ:**\n"]
        
        if gene_nodes:
            answer_parts.append("**Ú˜Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù… ÛŒØ§ÙØª Ø´Ø¯Ù‡:**")
            # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù…ØªÛŒØ§Ø²
            sorted_genes = sorted(gene_nodes, key=lambda x: getattr(x, 'score', 1.0), reverse=True)
            for gene in sorted_genes[:5]:
                score_info = f" (Ø§Ù…ØªÛŒØ§Ø²: {gene.score:.2f})" if hasattr(gene, 'score') and gene.score != 1.0 else ""
                answer_parts.append(f"â€¢ {gene.name}{score_info}")
            answer_parts.append("")
        
        if process_nodes:
            answer_parts.append("**ÙØ±Ø¢ÛŒÙ†Ø¯Ù‡Ø§ÛŒ Ø²ÛŒØ³ØªÛŒ Ù…Ø±ØªØ¨Ø·:**")
            for process in process_nodes[:5]:
                answer_parts.append(f"â€¢ {process.name}")
            answer_parts.append("")
        
        # Ø±ÙˆØ§Ø¨Ø· Ú˜Ù†-ÙØ±Ø¢ÛŒÙ†Ø¯
        gene_process_edges = [e for e in retrieval_result.edges if 'participate' in e.relation.lower() or 'regulate' in e.relation.lower()]
        if gene_process_edges:
            answer_parts.append("**Ø±ÙˆØ§Ø¨Ø· Ú˜Ù†-ÙØ±Ø¢ÛŒÙ†Ø¯:**")
            for edge in gene_process_edges[:5]:
                source_name = next(n.name for n in retrieval_result.nodes if n.id == edge.source)
                target_name = next(n.name for n in retrieval_result.nodes if n.id == edge.target)
                answer_parts.append(f"â€¢ {source_name} â†’ {target_name}")
        
        if not gene_nodes:
            answer_parts.append("âŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú˜Ù†ØªÛŒÚ©ÛŒ Ø¯Ø± Ù†ØªØ§ÛŒØ¬ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
        
        return "\n".join(answer_parts)
    
    def _generate_intelligent_disease_answer(self, retrieval_result: RetrievalResult, disease_nodes, gene_nodes) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„Ø§Øª Ø¨ÛŒÙ…Ø§Ø±ÛŒ"""
        answer_parts = ["ğŸ¥ **ØªØ­Ù„ÛŒÙ„ Ø¨ÛŒÙ…Ø§Ø±ÛŒ:**\n"]
        
        if disease_nodes:
            answer_parts.append("**Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡:**")
            for disease in disease_nodes[:5]:
                answer_parts.append(f"â€¢ {disease.name}")
            answer_parts.append("")
        
        if gene_nodes:
            answer_parts.append("**Ú˜Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·:**")
            for gene in gene_nodes[:5]:
                answer_parts.append(f"â€¢ {gene.name}")
            answer_parts.append("")
        
        # Ø±ÙˆØ§Ø¨Ø· Ø¨ÛŒÙ…Ø§Ø±ÛŒ-Ú˜Ù†
        disease_gene_edges = [e for e in retrieval_result.edges if 'cause' in e.relation.lower() or 'associate' in e.relation.lower()]
        if disease_gene_edges:
            answer_parts.append("**Ø±ÙˆØ§Ø¨Ø· Ø¨ÛŒÙ…Ø§Ø±ÛŒ-Ú˜Ù†:**")
            for edge in disease_gene_edges[:5]:
                source_name = next(n.name for n in retrieval_result.nodes if n.id == edge.source)
                target_name = next(n.name for n in retrieval_result.nodes if n.id == edge.target)
                answer_parts.append(f"â€¢ {source_name} â†’ {target_name}")
        
        if not disease_nodes:
            answer_parts.append("âŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨ÛŒÙ…Ø§Ø±ÛŒ Ø¯Ø± Ù†ØªØ§ÛŒØ¬ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
        
        return "\n".join(answer_parts)
    
    def _generate_intelligent_anatomy_answer(self, retrieval_result: RetrievalResult, anatomy_nodes, gene_nodes) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„Ø§Øª Ø¢Ù†Ø§ØªÙˆÙ…ÛŒ"""
        answer_parts = ["ğŸ«€ **ØªØ­Ù„ÛŒÙ„ Ø¢Ù†Ø§ØªÙˆÙ…ÛŒÚ©:**\n"]
        
        if anatomy_nodes:
            answer_parts.append("**Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ø¢Ù†Ø§ØªÙˆÙ…ÛŒÚ©:**")
            for anatomy in anatomy_nodes[:5]:
                answer_parts.append(f"â€¢ {anatomy.name}")
            answer_parts.append("")
        
        if gene_nodes:
            answer_parts.append("**Ú˜Ù†â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ§Ù† Ø´Ø¯Ù‡:**")
            for gene in gene_nodes[:5]:
                answer_parts.append(f"â€¢ {gene.name}")
            answer_parts.append("")
        
        # Ø±ÙˆØ§Ø¨Ø· Ø¨ÛŒØ§Ù†
        expression_edges = [e for e in retrieval_result.edges if 'express' in e.relation.lower()]
        if expression_edges:
            answer_parts.append("**Ø±ÙˆØ§Ø¨Ø· Ø¨ÛŒØ§Ù† Ú˜Ù†:**")
            for edge in expression_edges[:5]:
                source_name = next(n.name for n in retrieval_result.nodes if n.id == edge.source)
                target_name = next(n.name for n in retrieval_result.nodes if n.id == edge.target)
                answer_parts.append(f"â€¢ {source_name} Ø¨ÛŒØ§Ù† Ù…ÛŒâ€ŒÚ©Ù†Ø¯: {target_name}")
        
        if not anatomy_nodes:
            answer_parts.append("âŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¢Ù†Ø§ØªÙˆÙ…ÛŒÚ© Ø¯Ø± Ù†ØªØ§ÛŒØ¬ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
        
        return "\n".join(answer_parts)
    
    def _generate_intelligent_general_answer(self, retrieval_result: RetrievalResult, gene_nodes, disease_nodes, drug_nodes, anatomy_nodes, process_nodes) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¹Ù…ÙˆÙ…ÛŒ"""
        answer_parts = ["ğŸ“Š **ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹:**\n"]
        
        # Ø®Ù„Ø§ØµÙ‡ Ø¢Ù…Ø§Ø±ÛŒ
        total_entities = len(retrieval_result.nodes)
        total_relationships = len(retrieval_result.edges)
        
        answer_parts.append(f"**Ø®Ù„Ø§ØµÙ‡ Ø¢Ù…Ø§Ø±ÛŒ:**")
        answer_parts.append(f"â€¢ Ú©Ù„ Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§: {total_entities}")
        answer_parts.append(f"â€¢ Ú©Ù„ Ø±ÙˆØ§Ø¨Ø·: {total_relationships}")
        answer_parts.append("")
        
        # Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§
        if gene_nodes:
            answer_parts.append(f"**Ú˜Ù†â€ŒÙ‡Ø§ ({len(gene_nodes)}):**")
            for gene in gene_nodes[:3]:
                score_info = f" (Ø§Ù…ØªÛŒØ§Ø²: {gene.score:.2f})" if hasattr(gene, 'score') and gene.score != 1.0 else ""
                answer_parts.append(f"â€¢ {gene.name}{score_info}")
            answer_parts.append("")
        
        if disease_nodes:
            answer_parts.append(f"**Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ ({len(disease_nodes)}):**")
            for disease in disease_nodes[:3]:
                answer_parts.append(f"â€¢ {disease.name}")
            answer_parts.append("")
        
        if drug_nodes:
            answer_parts.append(f"**Ø¯Ø§Ø±ÙˆÙ‡Ø§ ({len(drug_nodes)}):**")
            for drug in drug_nodes[:3]:
                answer_parts.append(f"â€¢ {drug.name}")
            answer_parts.append("")
        
        if anatomy_nodes:
            answer_parts.append(f"**Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ø¢Ù†Ø§ØªÙˆÙ…ÛŒÚ© ({len(anatomy_nodes)}):**")
            for anatomy in anatomy_nodes[:3]:
                answer_parts.append(f"â€¢ {anatomy.name}")
            answer_parts.append("")
        
        if process_nodes:
            answer_parts.append(f"**ÙØ±Ø¢ÛŒÙ†Ø¯Ù‡Ø§ÛŒ Ø²ÛŒØ³ØªÛŒ ({len(process_nodes)}):**")
            for process in process_nodes[:3]:
                answer_parts.append(f"â€¢ {process.name}")
            answer_parts.append("")
        
        # Ù…Ù‡Ù…â€ŒØªØ±ÛŒÙ† Ø±ÙˆØ§Ø¨Ø·
        if retrieval_result.edges:
            answer_parts.append("**Ù…Ù‡Ù…â€ŒØªØ±ÛŒÙ† Ø±ÙˆØ§Ø¨Ø·:**")
            # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ø±Ø§Ø¨Ø·Ù‡
            relations_count = {}
            for edge in retrieval_result.edges:
                relations_count[edge.relation] = relations_count.get(edge.relation, 0) + 1
            
            for relation, count in sorted(relations_count.items(), key=lambda x: x[1], reverse=True)[:3]:
                answer_parts.append(f"â€¢ {relation}: {count} Ø±Ø§Ø¨Ø·Ù‡")
        
        if not retrieval_result.nodes:
            answer_parts.append("âŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø·ÛŒ Ø¯Ø± Ú¯Ø±Ø§Ù Ø¯Ø§Ù†Ø´ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
        
        return "\n".join(answer_parts)
    
    def custom_generation(self, retrieval_result: RetrievalResult) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø³ÙØ§Ø±Ø´ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        query = retrieval_result.query
        query_lower = query.lower()
        
        # ØªØ­Ù„ÛŒÙ„ Ø¹Ù…ÛŒÙ‚â€ŒØªØ± Ø³ÙˆØ§Ù„
        question_type = self._analyze_question_type(query_lower)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ø§ Ø¬Ø²Ø¦ÛŒØ§Øª Ø¨ÛŒØ´ØªØ±
        gene_nodes = [n for n in retrieval_result.nodes if n.kind == 'Gene']
        disease_nodes = [n for n in retrieval_result.nodes if n.kind == 'Disease']
        drug_nodes = [n for n in retrieval_result.nodes if n.kind in ['Drug', 'Compound']]
        anatomy_nodes = [n for n in retrieval_result.nodes if n.kind == 'Anatomy']
        process_nodes = [n for n in retrieval_result.nodes if n.kind == 'Biological Process']
        symptom_nodes = [n for n in retrieval_result.nodes if n.kind == 'Symptom']
        
        # ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø³ÙØ§Ø±Ø´ÛŒ Ø¨Ø§ ØªØ­Ù„ÛŒÙ„ Ø¹Ù…ÛŒÙ‚
        answer_parts = ["ğŸ¯ **ØªØ­Ù„ÛŒÙ„ Ø³ÙØ§Ø±Ø´ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡:**\n"]
        
        # ØªØ­Ù„ÛŒÙ„ Ú©ÛŒÙÛŒØª Ù†ØªØ§ÛŒØ¬
        total_score = sum(getattr(n, 'score', 1.0) for n in retrieval_result.nodes)
        avg_score = total_score / len(retrieval_result.nodes) if retrieval_result.nodes else 0
        
        answer_parts.append(f"**Ú©ÛŒÙÛŒØª Ù†ØªØ§ÛŒØ¬:**")
        answer_parts.append(f"â€¢ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø²: {avg_score:.2f}")
        answer_parts.append(f"â€¢ ØªØ¹Ø¯Ø§Ø¯ Ù†ÙˆØ¯Ù‡Ø§ÛŒ Ø¨Ø§ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø§Ù„Ø§: {len([n for n in retrieval_result.nodes if getattr(n, 'score', 1.0) > 2.0])}")
        answer_parts.append("")
        
        # ØªØ­Ù„ÛŒÙ„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ø³ÙˆØ§Ù„
        if question_type == "relationship":
            answer_parts.extend(self._custom_relationship_analysis(retrieval_result, gene_nodes, disease_nodes, drug_nodes))
        elif question_type == "drug_treatment":
            answer_parts.extend(self._custom_drug_analysis(retrieval_result, drug_nodes, disease_nodes))
        elif question_type == "gene_function":
            answer_parts.extend(self._custom_gene_analysis(retrieval_result, gene_nodes, process_nodes))
        elif question_type == "disease_info":
            answer_parts.extend(self._custom_disease_analysis(retrieval_result, disease_nodes, gene_nodes, symptom_nodes))
        elif question_type == "anatomy_expression":
            answer_parts.extend(self._custom_anatomy_analysis(retrieval_result, anatomy_nodes, gene_nodes))
        else:
            answer_parts.extend(self._custom_general_analysis(retrieval_result, gene_nodes, disease_nodes, drug_nodes, anatomy_nodes, process_nodes))
        
        # ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ
        answer_parts.append("**ğŸ’¡ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ:**")
        if len(retrieval_result.nodes) < 5:
            answer_parts.append("â€¢ Ø§ÙØ²Ø§ÛŒØ´ Ø¹Ù…Ù‚ Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† Ù†ØªØ§ÛŒØ¬ Ø¨ÛŒØ´ØªØ±")
        if len(retrieval_result.edges) < 3:
            answer_parts.append("â€¢ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ ØªØ±Ú©ÛŒØ¨ÛŒ Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† Ø±ÙˆØ§Ø¨Ø· Ø¨ÛŒØ´ØªØ±")
        if avg_score < 2.0:
            answer_parts.append("â€¢ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø±ÙˆØ´ Ensemble Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ú©ÛŒÙÛŒØª Ù†ØªØ§ÛŒØ¬")
        
        return "\n".join(answer_parts)
    
    def _custom_relationship_analysis(self, retrieval_result: RetrievalResult, gene_nodes, disease_nodes, drug_nodes) -> List[str]:
        """ØªØ­Ù„ÛŒÙ„ Ø³ÙØ§Ø±Ø´ÛŒ Ø±ÙˆØ§Ø¨Ø·"""
        parts = []
        parts.append("**ğŸ” ØªØ­Ù„ÛŒÙ„ Ø±ÙˆØ§Ø¨Ø·:**")
        
        if retrieval_result.edges:
            # ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ø±ÙˆØ§Ø¨Ø·
            edge_types = {}
            for edge in retrieval_result.edges:
                edge_types[edge.relation] = edge_types.get(edge.relation, 0) + 1
            
            parts.append(f"â€¢ Ø§Ù†ÙˆØ§Ø¹ Ø±ÙˆØ§Ø¨Ø· ÛŒØ§ÙØª Ø´Ø¯Ù‡: {len(edge_types)}")
            parts.append(f"â€¢ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ø´Ø¨Ú©Ù‡: {len(retrieval_result.edges)} / {len(retrieval_result.nodes)} = {len(retrieval_result.edges)/len(retrieval_result.nodes):.2f}")
            
            # Ù…Ù‡Ù…â€ŒØªØ±ÛŒÙ† Ø±ÙˆØ§Ø¨Ø·
            most_common = max(edge_types.items(), key=lambda x: x[1])
            parts.append(f"â€¢ Ø±Ø§ÛŒØ¬â€ŒØªØ±ÛŒÙ† Ø±Ø§Ø¨Ø·Ù‡: {most_common[0]} ({most_common[1]} Ø¨Ø§Ø±)")
        else:
            parts.append("â€¢ Ù‡ÛŒÚ† Ø±Ø§Ø¨Ø·Ù‡ Ù…Ø³ØªÙ‚ÛŒÙ…ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")
        
        return parts
    
    def _custom_drug_analysis(self, retrieval_result: RetrievalResult, drug_nodes, disease_nodes) -> List[str]:
        """ØªØ­Ù„ÛŒÙ„ Ø³ÙØ§Ø±Ø´ÛŒ Ø¯Ø§Ø±ÙˆÛŒÛŒ"""
        parts = []
        parts.append("**ğŸ’Š ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø±ÙˆÛŒÛŒ:**")
        
        if drug_nodes:
            # ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø±ÙˆÙ‡Ø§ÛŒ Ù…Ù‡Ù…
            high_score_drugs = [d for d in drug_nodes if getattr(d, 'score', 1.0) > 2.0]
            parts.append(f"â€¢ Ø¯Ø§Ø±ÙˆÙ‡Ø§ÛŒ Ø¨Ø§ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø§Ù„Ø§: {len(high_score_drugs)}")
            
            if high_score_drugs:
                parts.append("â€¢ Ù…Ù‡Ù…â€ŒØªØ±ÛŒÙ† Ø¯Ø§Ø±ÙˆÙ‡Ø§:")
                for drug in high_score_drugs[:3]:
                    parts.append(f"  - {drug.name} (Ø§Ù…ØªÛŒØ§Ø²: {drug.score:.2f})")
        
        if disease_nodes:
            parts.append(f"â€¢ Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·: {len(disease_nodes)}")
        
        return parts
    
    def _custom_gene_analysis(self, retrieval_result: RetrievalResult, gene_nodes, process_nodes) -> List[str]:
        """ØªØ­Ù„ÛŒÙ„ Ø³ÙØ§Ø±Ø´ÛŒ Ú˜Ù†ØªÛŒÚ©ÛŒ"""
        parts = []
        parts.append("**ğŸ§¬ ØªØ­Ù„ÛŒÙ„ Ú˜Ù†ØªÛŒÚ©ÛŒ:**")
        
        if gene_nodes:
            # ØªØ­Ù„ÛŒÙ„ Ú˜Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù…
            sorted_genes = sorted(gene_nodes, key=lambda x: getattr(x, 'score', 1.0), reverse=True)
            parts.append(f"â€¢ Ú˜Ù†â€ŒÙ‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡: {len(gene_nodes)}")
            parts.append("â€¢ Ù…Ù‡Ù…â€ŒØªØ±ÛŒÙ† Ú˜Ù†â€ŒÙ‡Ø§:")
            for gene in sorted_genes[:3]:
                parts.append(f"  - {gene.name} (Ø§Ù…ØªÛŒØ§Ø²: {gene.score:.2f})")
        
        if process_nodes:
            parts.append(f"â€¢ ÙØ±Ø¢ÛŒÙ†Ø¯Ù‡Ø§ÛŒ Ø²ÛŒØ³ØªÛŒ: {len(process_nodes)}")
        
        return parts
    
    def _custom_disease_analysis(self, retrieval_result: RetrievalResult, disease_nodes, gene_nodes, symptom_nodes) -> List[str]:
        """ØªØ­Ù„ÛŒÙ„ Ø³ÙØ§Ø±Ø´ÛŒ Ø¨ÛŒÙ…Ø§Ø±ÛŒ"""
        parts = []
        parts.append("**ğŸ¥ ØªØ­Ù„ÛŒÙ„ Ø¨ÛŒÙ…Ø§Ø±ÛŒ:**")
        
        if disease_nodes:
            parts.append(f"â€¢ Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡: {len(disease_nodes)}")
        
        if gene_nodes:
            parts.append(f"â€¢ Ú˜Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·: {len(gene_nodes)}")
        
        if symptom_nodes:
            parts.append(f"â€¢ Ø¹Ù„Ø§Ø¦Ù… Ù…Ø±ØªØ¨Ø·: {len(symptom_nodes)}")
        
        return parts
    
    def _custom_anatomy_analysis(self, retrieval_result: RetrievalResult, anatomy_nodes, gene_nodes) -> List[str]:
        """ØªØ­Ù„ÛŒÙ„ Ø³ÙØ§Ø±Ø´ÛŒ Ø¢Ù†Ø§ØªÙˆÙ…ÛŒÚ©"""
        parts = []
        parts.append("**ğŸ«€ ØªØ­Ù„ÛŒÙ„ Ø¢Ù†Ø§ØªÙˆÙ…ÛŒÚ©:**")
        
        if anatomy_nodes:
            parts.append(f"â€¢ Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ø¢Ù†Ø§ØªÙˆÙ…ÛŒÚ©: {len(anatomy_nodes)}")
        
        if gene_nodes:
            parts.append(f"â€¢ Ú˜Ù†â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ§Ù† Ø´Ø¯Ù‡: {len(gene_nodes)}")
        
        return parts
    
    def _custom_general_analysis(self, retrieval_result: RetrievalResult, gene_nodes, disease_nodes, drug_nodes, anatomy_nodes, process_nodes) -> List[str]:
        """ØªØ­Ù„ÛŒÙ„ Ø³ÙØ§Ø±Ø´ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ"""
        parts = []
        parts.append("**ğŸ“Š ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹:**")
        
        # Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ
        parts.append(f"â€¢ Ú©Ù„ Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§: {len(retrieval_result.nodes)}")
        parts.append(f"â€¢ Ú©Ù„ Ø±ÙˆØ§Ø¨Ø·: {len(retrieval_result.edges)}")
        parts.append(f"â€¢ ØªØ±Ø§Ú©Ù… Ø´Ø¨Ú©Ù‡: {len(retrieval_result.edges)/max(len(retrieval_result.nodes), 1):.2f}")
        
        # ØªÙˆØ²ÛŒØ¹ Ø§Ù†ÙˆØ§Ø¹
        type_distribution = {}
        for node in retrieval_result.nodes:
            type_distribution[node.kind] = type_distribution.get(node.kind, 0) + 1
        
        parts.append("â€¢ ØªÙˆØ²ÛŒØ¹ Ø§Ù†ÙˆØ§Ø¹:")
        for kind, count in sorted(type_distribution.items(), key=lambda x: x[1], reverse=True):
            parts.append(f"  - {kind}: {count}")
        
        return parts
    
    def process_query(self, query: str, retrieval_method: RetrievalMethod, 
                     generation_model: GenerationModel, max_depth: int = 2) -> Dict[str, Any]:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ø§Ù…Ù„ ÛŒÚ© Ø³ÙˆØ§Ù„"""
        print(f"ğŸš€ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³ÙˆØ§Ù„: {query}")
        
        # Ù…Ø±Ø­Ù„Ù‡ 1: Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ
        retrieval_result = self.retrieve_information(query, retrieval_method, max_depth)
        
        # Ù…Ø±Ø­Ù„Ù‡ 2: ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®
        generation_result = self.generate_answer(retrieval_result, generation_model)
        
        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†ØªÛŒØ¬Ù‡
        result = {
            "query": query,
            "retrieval_method": retrieval_method.value,
            "generation_model": generation_model.value,
            "keywords": self.extract_keywords(query),
            "matched_nodes": {k: self.G.nodes[v]['name'] for k, v in self.match_tokens_to_nodes(self.extract_keywords(query)).items()},
            "retrieved_nodes": [
                {
                    "id": node.id,
                    "name": node.name,
                    "kind": node.kind,
                    "depth": node.depth,
                    "score": node.score
                } for node in retrieval_result.nodes
            ],
            "retrieved_edges": [
                {
                    "source": edge.source,
                    "target": edge.target,
                    "relation": edge.relation,
                    "weight": edge.weight
                } for edge in retrieval_result.edges
            ],
            "paths": retrieval_result.paths,
            "context_text": retrieval_result.context_text,
            "answer": generation_result.answer,
            "confidence": generation_result.confidence,
            "process_steps": [
                "1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø² Ø³ÙˆØ§Ù„",
                "2. ØªØ·Ø¨ÛŒÙ‚ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø§ Ù†ÙˆØ¯Ù‡Ø§ÛŒ Ú¯Ø±Ø§Ù",
                f"3. Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ø§ Ø±ÙˆØ´ {retrieval_method.value}",
                "4. Ø§ÛŒØ¬Ø§Ø¯ Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡ Ø§Ø² Ù†ØªØ§ÛŒØ¬",
                f"5. ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ Ù…Ø¯Ù„ {generation_model.value}"
            ]
        }
        
        return result
    
    def huggingface_generation(self, retrieval_result: RetrievalResult) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ HuggingFace (Ø±Ø§ÛŒÚ¯Ø§Ù†)"""
        try:
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø±Ø§ÛŒÚ¯Ø§Ù† HuggingFace
            from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
            import torch
            
            # Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø±Ø§ÛŒÚ¯Ø§Ù† Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯
            models = [
                "microsoft/DialoGPT-medium",  # Ú†Øªâ€ŒØ¨Ø§Øª
                "gpt2",  # GPT-2
                "distilgpt2",  # GPT-2 Ø³Ø¨Ú©
                "EleutherAI/gpt-neo-125M",  # GPT-Neo Ú©ÙˆÚ†Ú©
                "microsoft/DialoGPT-small"  # Ú†Øªâ€ŒØ¨Ø§Øª Ú©ÙˆÚ†Ú©
            ]
            
            # Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ù…ÙˆØ¬ÙˆØ¯
            selected_model = None
            for model_name in models:
                try:
                    tokenizer = AutoTokenizer.from_pretrained(model_name)
                    model = AutoModelForCausalLM.from_pretrained(model_name)
                    selected_model = model_name
                    break
                except:
                    continue
            
            if selected_model is None:
                return self._fallback_generation(retrieval_result, "HuggingFace")
            
            # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ
            prompt = self._create_advanced_prompt(retrieval_result)
            
            # ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®
            inputs = tokenizer.encode(prompt, return_tensors="pt", max_length=512, truncation=True)
            
            with torch.no_grad():
                outputs = model.generate(
                    inputs,
                    max_length=300,
                    num_return_sequences=1,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù¾Ø§Ø³Ø® Ø§Ø² Ø®Ø±ÙˆØ¬ÛŒ
            if len(response) > len(prompt):
                answer = response[len(prompt):].strip()
            else:
                answer = response.strip()
            
            return answer if answer else self._fallback_generation(retrieval_result, "HuggingFace")
            
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± HuggingFace: {e}")
            return self._fallback_generation(retrieval_result, "HuggingFace")
    
    def openai_gpt_generation(self, retrieval_result: RetrievalResult) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ OpenAI GPT (Ù†ÛŒØ§Ø² Ø¨Ù‡ API Key)"""
        try:
            from openai import OpenAI
            
            # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ API Key
            if not hasattr(self, 'openai_api_key') or not self.openai_api_key:
                return "ğŸ”‘ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² OpenAI GPTØŒ Ù„Ø·ÙØ§Ù‹ API Key Ø±Ø§ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯.\n\n" + self._fallback_generation(retrieval_result, "OpenAI")
            
            # Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù„Ø§ÛŒÙ†Øª OpenAI
            client = OpenAI(api_key=self.openai_api_key)
            
            # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ
            prompt = self._create_advanced_prompt(retrieval_result)
            
            # Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‡Ø²ÛŒÙ†Ù‡ Ùˆ Ú©ÛŒÙÛŒØª
            # gpt-3.5-turbo: Ø§Ø±Ø²Ø§Ù† Ùˆ Ø³Ø±ÛŒØ¹
            # gpt-4: Ú¯Ø±Ø§Ù†â€ŒØªØ± Ø§Ù…Ø§ Ú©ÛŒÙÛŒØª Ø¨Ù‡ØªØ±
            # gpt-4-turbo-preview: Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ùˆ Ø¨Ù‡ØªØ±ÛŒÙ†
            model_choice = "gpt-3.5-turbo"  # Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ù‡ gpt-4 ØªØºÛŒÛŒØ± Ø¯Ù‡ÛŒØ¯
            
            # Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ OpenAI
            response = client.chat.completions.create(
                model=model_choice,
                messages=[
                    {"role": "system", "content": "You are a biomedical expert analyzing knowledge graph data. Provide detailed, accurate, and well-structured answers in Persian with proper formatting and emojis."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=800,  # Ø§ÙØ²Ø§ÛŒØ´ ØªØ¹Ø¯Ø§Ø¯ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§
                temperature=0.7,
                presence_penalty=0.1,  # ØªØ´ÙˆÛŒÙ‚ Ø¨Ù‡ ØªÙ†ÙˆØ¹
                frequency_penalty=0.1   # Ú©Ø§Ù‡Ø´ ØªÚ©Ø±Ø§Ø±
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± OpenAI: {e}")
            return self._fallback_generation(retrieval_result, "OpenAI")
    
    def anthropic_claude_generation(self, retrieval_result: RetrievalResult) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ Anthropic Claude (Ù†ÛŒØ§Ø² Ø¨Ù‡ API Key)"""
        try:
            import anthropic
            
            # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ API Key
            if not hasattr(self, 'anthropic_api_key') or not self.anthropic_api_key:
                return "ğŸ”‘ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ClaudeØŒ Ù„Ø·ÙØ§Ù‹ API Key Ø±Ø§ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯.\n\n" + self._fallback_generation(retrieval_result, "Claude")
            
            client = anthropic.Anthropic(api_key=self.anthropic_api_key)
            
            # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ
            prompt = self._create_advanced_prompt(retrieval_result)
            
            # Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ Claude
            response = client.messages.create(
                model="claude-3-sonnet-20240229",
                max_tokens=500,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            
            return response.content[0].text.strip()
            
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Claude: {e}")
            return self._fallback_generation(retrieval_result, "Claude")
    
    def google_gemini_generation(self, retrieval_result: RetrievalResult) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ Google Gemini (Ù†ÛŒØ§Ø² Ø¨Ù‡ API Key)"""
        try:
            import google.generativeai as genai
            
            # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ API Key
            if not hasattr(self, 'gemini_api_key') or not self.gemini_api_key:
                return "ğŸ”‘ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² GeminiØŒ Ù„Ø·ÙØ§Ù‹ API Key Ø±Ø§ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯.\n\n" + self._fallback_generation(retrieval_result, "Gemini")
            
            genai.configure(api_key=self.gemini_api_key)
            model = genai.GenerativeModel('gemini-pro')
            
            # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ
            prompt = self._create_advanced_prompt(retrieval_result)
            
            # Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ Gemini
            response = model.generate_content(prompt)
            
            return response.text.strip()
            
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Gemini: {e}")
            return self._fallback_generation(retrieval_result, "Gemini")
    
    def _create_advanced_prompt(self, retrieval_result: RetrievalResult) -> str:
        """Ø§ÛŒØ¬Ø§Ø¯ Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ AI"""
        query = retrieval_result.query
        context = retrieval_result.context_text
        method = retrieval_result.method
        
        # ØªØ­Ù„ÛŒÙ„ Ù†ÙˆØ¹ Ø³ÙˆØ§Ù„
        question_type = self._analyze_question_type(query.lower())
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ Ø§Ø² Ú¯Ø±Ø§Ù Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ ÛŒØ§ Ù†Ù‡
        if method == "Ø¨Ø¯ÙˆÙ† Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ (ÙÙ‚Ø· Ù…Ø¯Ù„)":
            # ÙÙ‚Ø· Ù…Ø¯Ù„ - Ø¨Ø¯ÙˆÙ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú¯Ø±Ø§Ù
            system_prompt = """You are an expert biomedical AI assistant with comprehensive knowledge of:
            - Molecular biology and genetics
            - Drug discovery and pharmacology
            - Disease mechanisms and pathology
            - Biological pathways and networks
            - Clinical medicine and therapeutics
            
            Your task is to provide detailed, accurate, and well-structured answers to biomedical questions
            based on your training knowledge. Focus on:
            - Scientific accuracy and current understanding
            - Comprehensive analysis and insights
            - Practical implications and applications
            - Research directions and future possibilities
            
            Always answer in Persian with proper formatting and structure your response with clear sections.
            Do not use emojis in your response."""
            
            user_prompt = f"""
            **Ø³ÙˆØ§Ù„ Ù¾Ø²Ø´Ú©ÛŒ-Ø²ÛŒØ³ØªÛŒ:**
            {query}
            
            **Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„ Ù¾Ø§Ø³Ø®â€ŒØ¯Ù‡ÛŒ:**
            Ù„Ø·ÙØ§Ù‹ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ù†Ø´ ØªØ®ØµØµÛŒ Ø®ÙˆØ¯ Ø¯Ø± Ø²Ù…ÛŒÙ†Ù‡ Ø¹Ù„ÙˆÙ… Ø²ÛŒØ³ØªÛŒ Ùˆ Ù¾Ø²Ø´Ú©ÛŒØŒ Ù¾Ø§Ø³Ø® Ø¬Ø§Ù…Ø¹ Ùˆ Ø¯Ù‚ÛŒÙ‚ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯ Ú©Ù‡ Ø´Ø§Ù…Ù„:
            
            1. **ØªØ­Ù„ÛŒÙ„ Ù…ÙˆØ¶ÙˆØ¹:** Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ù‚ÛŒÙ‚ Ø³ÙˆØ§Ù„ Ùˆ Ø¬Ù†Ø¨Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¢Ù†
            2. **Ù…Ø¨Ø§Ù†ÛŒ Ø¹Ù„Ù…ÛŒ:** ØªÙˆØ¶ÛŒØ­ Ù…Ú©Ø§Ù†ÛŒØ²Ù…â€ŒÙ‡Ø§ Ùˆ ÙØ±Ø¢ÛŒÙ†Ø¯Ù‡Ø§ÛŒ Ø²ÛŒØ³ØªÛŒ Ù…Ø±ØªØ¨Ø·
            3. **Ø¬Ù†Ø¨Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ø±Ù…Ø§Ù†ÛŒ:** Ø¯Ø± ØµÙˆØ±Øª Ù…Ø±ØªØ¨Ø· Ø¨ÙˆØ¯Ù†ØŒ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ø¯Ø±Ù…Ø§Ù†ÛŒ Ùˆ Ø¯Ø§Ø±ÙˆÛŒÛŒ
            4. **ØªØ­Ù‚ÛŒÙ‚Ø§Øª:** ÙˆØ¶Ø¹ÛŒØª ÙØ¹Ù„ÛŒ ØªØ­Ù‚ÛŒÙ‚Ø§Øª Ùˆ Ù…Ø·Ø§Ù„Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø·
            5. **Ú†Ø´Ù…â€ŒØ§Ù†Ø¯Ø§Ø² Ø¢ÛŒÙ†Ø¯Ù‡:** Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ ØªØ­Ù‚ÛŒÙ‚Ø§ØªÛŒ Ùˆ Ù¾ÛŒØ´Ø±ÙØªâ€ŒÙ‡Ø§ÛŒ Ø¢ÛŒÙ†Ø¯Ù‡
            6. **ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ:** Ù†Ú©Ø§Øª Ù…Ù‡Ù… Ø¨Ø±Ø§ÛŒ Ù¾Ú˜ÙˆÙ‡Ø´Ú¯Ø±Ø§Ù† Ùˆ Ù¾Ø²Ø´Ú©Ø§Ù†
            
            Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø³Ø§Ø®ØªØ§Ø±ÛŒØ§ÙØªÙ‡ Ùˆ Ø¨Ø§ ÙØ±Ù…Øªâ€ŒØ¨Ù†Ø¯ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯.
            """
            
        else:
            # Ø¨Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú¯Ø±Ø§Ù
            system_prompt = """You are a biomedical knowledge graph expert analyzing data from a comprehensive
            biological knowledge graph containing information about:
            - Genes, proteins, and their functions
            - Diseases and their molecular mechanisms
            - Drugs, compounds, and their therapeutic effects
            - Biological processes and pathways
            - Anatomical structures and gene expression
            - Clinical relationships and treatment outcomes
            
            Your task is to analyze the retrieved information from the knowledge graph and provide:
            - Comprehensive interpretation of the data
            - Biological significance and implications
            - Clinical relevance and applications
            - Research insights and recommendations
            - Quality assessment of the retrieved information
            
            IMPORTANT: If the retrieved information is insufficient or limited, supplement your analysis
            with your general biomedical knowledge to provide a comprehensive and useful answer.
            Focus on providing valuable insights even when graph data is limited.
            
            Always answer in Persian with proper formatting and structure your response with clear sections.
            Do not use emojis in your response."""
            
            user_prompt = f"""
            **Ø³ÙˆØ§Ù„ Ù¾Ø²Ø´Ú©ÛŒ-Ø²ÛŒØ³ØªÛŒ:**
            {query}
            
            **Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù‡ Ø§Ø² Ú¯Ø±Ø§Ù Ø¯Ø§Ù†Ø´ Ø²ÛŒØ³ØªÛŒ:**
            Ø±ÙˆØ´ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ: {method}
            
            {context}
            
            **Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„ ØªØ­Ù„ÛŒÙ„:**
            Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù‡ Ø§Ø² Ú¯Ø±Ø§Ù Ø¯Ø§Ù†Ø´ØŒ ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹ Ùˆ Ø¯Ù‚ÛŒÙ‚ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯ Ú©Ù‡ Ø´Ø§Ù…Ù„:
            
            1. **Ø®Ù„Ø§ØµÙ‡ Ø¢Ù…Ø§Ø±ÛŒ:** Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù‡
            2. **ØªØ­Ù„ÛŒÙ„ Ø±ÙˆØ§Ø¨Ø·:** Ø¨Ø±Ø±Ø³ÛŒ Ø±ÙˆØ§Ø¨Ø· Ùˆ Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª ÛŒØ§ÙØª Ø´Ø¯Ù‡
            3. **Ø§Ù‡Ù…ÛŒØª Ø²ÛŒØ³ØªÛŒ:** ØªÙØ³ÛŒØ± Ø§Ù‡Ù…ÛŒØª Ø²ÛŒØ³ØªÛŒ Ùˆ Ù¾Ø²Ø´Ú©ÛŒ ÛŒØ§ÙØªÙ‡â€ŒÙ‡Ø§
            4. **Ø¬Ù†Ø¨Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ø±Ù…Ø§Ù†ÛŒ:** Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Ø¯Ø±Ù…Ø§Ù†ÛŒ Ùˆ Ø¯Ø§Ø±ÙˆÛŒÛŒ
            5. **Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§:** Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª Ùˆ Ø§Ø¹ØªØ¨Ø§Ø± Ø§Ø·Ù„Ø§Ø¹Ø§Øª
            6. **ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ:** Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ø±Ø§ÛŒ Ù¾Ú˜ÙˆÙ‡Ø´ Ùˆ Ú©Ø§Ø±Ø¨Ø±Ø¯
            7. **Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø¢ÛŒÙ†Ø¯Ù‡:** Ø¬Ù‡Øªâ€ŒÚ¯ÛŒØ±ÛŒâ€ŒÙ‡Ø§ÛŒ ØªØ­Ù‚ÛŒÙ‚Ø§ØªÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ
            
            **Ù†Ú©ØªÙ‡ Ù…Ù‡Ù…:** Ø§Ú¯Ø± Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù‡ Ù…Ø­Ø¯ÙˆØ¯ ÛŒØ§ Ù†Ø§Ú©Ø§ÙÛŒ Ø¨Ø§Ø´Ø¯ØŒ Ø§Ø² Ø¯Ø§Ù†Ø´ Ø¹Ù…ÙˆÙ…ÛŒ Ø®ÙˆØ¯ Ø¯Ø± Ø²Ù…ÛŒÙ†Ù‡ Ø¹Ù„ÙˆÙ… Ø²ÛŒØ³ØªÛŒ
            Ø¨Ø±Ø§ÛŒ ØªÚ©Ù…ÛŒÙ„ ØªØ­Ù„ÛŒÙ„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ Ùˆ Ù¾Ø§Ø³Ø® Ø¬Ø§Ù…Ø¹ Ùˆ Ù…ÙÛŒØ¯ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯. Ù‡Ø¯Ù Ø§Ø±Ø§Ø¦Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø±Ø²Ø´Ù…Ù†Ø¯
            Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø± Ø§Ø³ØªØŒ Ø­ØªÛŒ Ø§Ú¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú¯Ø±Ø§Ù Ù…Ø­Ø¯ÙˆØ¯ Ø¨Ø§Ø´Ù†Ø¯.
            
            Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø³Ø§Ø®ØªØ§Ø±ÛŒØ§ÙØªÙ‡ Ùˆ Ø¨Ø§ ÙØ±Ù…Øªâ€ŒØ¨Ù†Ø¯ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯.
            """
        
        return f"{system_prompt}\n\n{user_prompt}"
    
    def _fallback_generation(self, retrieval_result: RetrievalResult, model_name: str) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù¾Ø´ØªÛŒØ¨Ø§Ù† Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§"""
        return f"""ğŸ¤– **ØªØ­Ù„ÛŒÙ„ Ø¨Ø§ {model_name} (Ù¾Ø§Ø³Ø® Ù¾Ø´ØªÛŒØ¨Ø§Ù†):**

{self.gpt_simulation_generation(retrieval_result)}

---
ğŸ’¡ **Ù†Ú©ØªÙ‡:** Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡â€ŒØªØ±ØŒ Ù„Ø·ÙØ§Ù‹ API Key Ù…Ø±Ø¨ÙˆØ·Ù‡ Ø±Ø§ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯.
"""
    
    def set_openai_api_key(self, api_key: str):
        """ØªÙ†Ø¸ÛŒÙ… API Key Ø¨Ø±Ø§ÛŒ OpenAI"""
        self.openai_api_key = api_key
        print("âœ… OpenAI API Key ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯")
    
    def set_anthropic_api_key(self, api_key: str):
        """ØªÙ†Ø¸ÛŒÙ… API Key Ø¨Ø±Ø§ÛŒ Anthropic"""
        self.anthropic_api_key = api_key
        print("âœ… Anthropic API Key ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯")
    
    def set_gemini_api_key(self, api_key: str):
        """ØªÙ†Ø¸ÛŒÙ… API Key Ø¨Ø±Ø§ÛŒ Google Gemini"""
        self.gemini_api_key = api_key
        print("âœ… Gemini API Key ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯")

# Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡
if __name__ == "__main__":
    service = GraphRAGService()
    
    # ØªØ³Øª Ø³Ø±ÙˆÛŒØ³
    result = service.process_query(
        query="What is the relationship between HMGB3 and diabetes?",
        retrieval_method=RetrievalMethod.BFS,
        generation_model=GenerationModel.GPT_SIMULATION
    )
    
    print(json.dumps(result, indent=2, ensure_ascii=False)) 
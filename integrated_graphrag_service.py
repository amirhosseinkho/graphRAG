#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù…Ø§Ú˜ÙˆÙ„ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¯ØºØ§Ù… Ú©Ø§Ù…Ù„ EnhancedContextGenerator Ø¨Ø§ GraphRAGService
Ø§ÛŒÙ† Ù…Ø§Ú˜ÙˆÙ„ Ù…Ø´Ú©Ù„ Ø§ØµÙ„ÛŒ Ø³ÛŒØ³ØªÙ… ÙØ¹Ù„ÛŒ Ø±Ø§ Ø­Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
"""

import sys
import os
from typing import Dict, List, Tuple, Optional, Any
import json

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± Ù¾Ø±ÙˆÚ˜Ù‡
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from graphrag_service import GraphRAGService, RetrievalMethod, GenerationModel, RetrievalResult
from enhanced_context_generator import EnhancedContextGenerator

class IntegratedGraphRAGService:
    """Ø³Ø±ÙˆÛŒØ³ GraphRAG Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ Ø¨Ø§ Ø³ÛŒØ³ØªÙ… ØªØ¨Ø¯ÛŒÙ„ Ø´Ù†Ø§Ø³Ù‡â€ŒÙ‡Ø§"""
    
    def __init__(self, graph_data_path: str = None):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³Ø±ÙˆÛŒØ³ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡"""
        print("ğŸš€ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³Ø±ÙˆÛŒØ³ GraphRAG Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡...")
        
        # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³Ø±ÙˆÛŒØ³ Ø§ØµÙ„ÛŒ
        self.graphrag_service = GraphRAGService(graph_data_path)
        
        # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡
        self.enhanced_context_generator = EnhancedContextGenerator()
        
        print("âœ… Ø³Ø±ÙˆÛŒØ³ GraphRAG Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
    
    def process_query_enhanced(self, query: str, retrieval_method: RetrievalMethod, 
                             generation_model: GenerationModel, 
                             text_generation_type: str = 'INTELLIGENT',
                             context_type: str = 'INTELLIGENT',
                             max_depth: int = 2) -> Dict[str, Any]:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³ÙˆØ§Ù„ Ø¨Ø§ Ø³ÛŒØ³ØªÙ… Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡"""
        
        print(f"ğŸ” Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³ÙˆØ§Ù„ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡: {query}")
        print(f"ğŸ“ Ù†ÙˆØ¹ Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡: {context_type}")
        
        # Ù…Ø±Ø­Ù„Ù‡ 1: Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø§ Ø³Ø±ÙˆÛŒØ³ Ø§ØµÙ„ÛŒ
        retrieval_result = self.graphrag_service.retrieve_information(
            query, retrieval_method, max_depth
        )
        
        # Ù…Ø±Ø­Ù„Ù‡ 2: Ø¨Ù‡Ø¨ÙˆØ¯ Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡ Ø¨Ø§ Ø³ÛŒØ³ØªÙ… Ø¬Ø¯ÛŒØ¯
        enhanced_context = self.enhanced_context_generator.create_enhanced_context_text(
            retrieval_result, context_type
        )
        
        # Ù…Ø±Ø­Ù„Ù‡ 3: ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡
        # Ø§ÛŒØ¬Ø§Ø¯ RetrievalResult Ø¬Ø¯ÛŒØ¯ Ø¨Ø§ Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡
        enhanced_retrieval_result = RetrievalResult(
            nodes=retrieval_result.nodes,
            edges=retrieval_result.edges,
            paths=retrieval_result.paths,
            context_text=enhanced_context,  # Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡
            method=retrieval_result.method,
            query=retrieval_result.query
        )
        
        # ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®
        generation_result = self.graphrag_service.generate_answer(
            enhanced_retrieval_result, generation_model, text_generation_type
        )
        
        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†ØªÛŒØ¬Ù‡ Ù†Ù‡Ø§ÛŒÛŒ
        result = {
            "query": query,
            "retrieval_method": retrieval_method.value,
            "generation_model": generation_model.value,
            "context_type": context_type,
            "keywords": self.graphrag_service.extract_keywords(query),
            "matched_nodes": {k: self.graphrag_service.G.nodes[v]['name'] 
                            for k, v in self.graphrag_service.match_tokens_to_nodes(
                                self.graphrag_service.extract_keywords(query)
                            ).items()},
            "retrieved_nodes": [
                {
                    "id": node.id,
                    "name": node.name,
                    "kind": node.kind,
                    "depth": node.depth,
                    "score": node.score
                } for node in retrieval_result.nodes
            ],
            "retrieved_edges": [
                {
                    "source": edge.source,
                    "target": edge.target,
                    "relation": edge.relation,
                    "weight": edge.weight
                } for edge in retrieval_result.edges
            ],
            "paths": retrieval_result.paths,
            "original_context_text": retrieval_result.context_text,
            "enhanced_context_text": enhanced_context,  # Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡
            "answer": generation_result.answer,
            "confidence": generation_result.confidence,
            "process_steps": [
                "1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø² Ø³ÙˆØ§Ù„",
                "2. ØªØ·Ø¨ÛŒÙ‚ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø§ Ù†ÙˆØ¯Ù‡Ø§ÛŒ Ú¯Ø±Ø§Ù",
                f"3. Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ø§ Ø±ÙˆØ´ {retrieval_method.value}",
                "4. ØªØ¨Ø¯ÛŒÙ„ Ø´Ù†Ø§Ø³Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ù†Ø§Ø¯Ø§Ø±",
                f"5. Ø§ÛŒØ¬Ø§Ø¯ Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ ({context_type})",
                f"6. ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ Ù…Ø¯Ù„ {generation_model.value}"
            ]
        }
        
        return result
    
    def compare_context_quality(self, query: str, retrieval_method: RetrievalMethod = RetrievalMethod.INTELLIGENT) -> Dict[str, Any]:
        """Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©ÛŒÙÛŒØª Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡ Ø§ØµÙ„ÛŒ Ùˆ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡"""
        
        print(f"ğŸ” Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©ÛŒÙÛŒØª Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„: {query}")
        
        # Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
        retrieval_result = self.graphrag_service.retrieve_information(query, retrieval_method)
        
        # Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡ Ø§ØµÙ„ÛŒ
        original_context = retrieval_result.context_text
        
        # Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡
        enhanced_context = self.enhanced_context_generator.create_enhanced_context_text(
            retrieval_result, "INTELLIGENT"
        )
        
        # ØªØ­Ù„ÛŒÙ„ Ú©ÛŒÙÛŒØª
        comparison = {
            "query": query,
            "original_context_length": len(original_context),
            "enhanced_context_length": len(enhanced_context),
            "original_context": original_context,
            "enhanced_context": enhanced_context,
            "improvement_metrics": {
                "length_ratio": len(enhanced_context) / max(len(original_context), 1),
                "has_meaningful_names": "Gene::7157" not in enhanced_context,
                "has_biological_info": "Ù†Ù‚Ø´ Ø²ÛŒØ³ØªÛŒ" in enhanced_context,
                "has_relation_descriptions": "Ù…Ø´Ø§Ø±Ú©Øª Ø¯Ø± ÙØ±Ø¢ÛŒÙ†Ø¯ Ø²ÛŒØ³ØªÛŒ" in enhanced_context
            }
        }
        
        return comparison
    
    def test_enhanced_system(self, test_queries: List[str]) -> Dict[str, Any]:
        """ØªØ³Øª Ú©Ø§Ù…Ù„ Ø³ÛŒØ³ØªÙ… Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡"""
        
        print("ğŸ§ª ØªØ³Øª Ú©Ø§Ù…Ù„ Ø³ÛŒØ³ØªÙ… Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡")
        print("=" * 60)
        
        results = {
            "test_queries": test_queries,
            "results": [],
            "summary": {}
        }
        
        for i, query in enumerate(test_queries, 1):
            print(f"\nğŸ“ ØªØ³Øª {i}/{len(test_queries)}: {query}")
            
            try:
                # ØªØ³Øª Ø³ÛŒØ³ØªÙ… Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡
                enhanced_result = self.process_query_enhanced(
                    query=query,
                    retrieval_method=RetrievalMethod.INTELLIGENT,
                    generation_model=GenerationModel.GPT_SIMULATION,
                    text_generation_type='INTELLIGENT',
                    context_type='INTELLIGENT'
                )
                
                # Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©ÛŒÙÛŒØª
                comparison = self.compare_context_quality(query)
                
                result = {
                    "query": query,
                    "enhanced_result": enhanced_result,
                    "comparison": comparison,
                    "success": True
                }
                
                print(f"âœ… ØªØ³Øª {i} Ù…ÙˆÙÙ‚ÛŒØªâ€ŒØ¢Ù…ÛŒØ²")
                
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ³Øª {i}: {e}")
                result = {
                    "query": query,
                    "error": str(e),
                    "success": False
                }
            
            results["results"].append(result)
        
        # Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬
        successful_tests = [r for r in results["results"] if r["success"]]
        results["summary"] = {
            "total_tests": len(test_queries),
            "successful_tests": len(successful_tests),
            "success_rate": len(successful_tests) / len(test_queries),
            "average_context_improvement": sum(
                r["comparison"]["improvement_metrics"]["length_ratio"] 
                for r in successful_tests
            ) / len(successful_tests) if successful_tests else 0
        }
        
        print(f"\nğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬:")
        print(f"â€¢ Ú©Ù„ ØªØ³Øªâ€ŒÙ‡Ø§: {results['summary']['total_tests']}")
        print(f"â€¢ ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆÙÙ‚: {results['summary']['successful_tests']}")
        print(f"â€¢ Ù†Ø±Ø® Ù…ÙˆÙÙ‚ÛŒØª: {results['summary']['success_rate']:.2%}")
        print(f"â€¢ Ø¨Ù‡Ø¨ÙˆØ¯ Ù…ØªÙˆØ³Ø· Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡: {results['summary']['average_context_improvement']:.2f}x")
        
        return results

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ø³ÛŒØ³ØªÙ… Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡"""
    
    # Ø³ÙˆØ§Ù„Ø§Øª ØªØ³Øª
    test_queries = [
        "What is the relationship between TP53 and cancer?",
        "How does Carmustine treat brain cancer?",
        "What genes are involved in apoptosis?",
        "What drugs are used to treat glioma?",
        "How do genes regulate biological processes?"
    ]
    
    try:
        # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ…
        integrated_service = IntegratedGraphRAGService()
        
        # ØªØ³Øª Ø³ÛŒØ³ØªÙ…
        test_results = integrated_service.test_enhanced_system(test_queries)
        
        # Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ù†ØªØ§ÛŒØ¬
        if test_results["results"]:
            first_result = test_results["results"][0]
            if first_result["success"]:
                print(f"\nğŸ“„ Ù†Ù…ÙˆÙ†Ù‡ Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡:")
                print("-" * 40)
                enhanced_context = first_result["enhanced_result"]["enhanced_context_text"]
                print(enhanced_context[:1000] + "..." if len(enhanced_context) > 1000 else enhanced_context)
        
        print("\nğŸ‰ ØªØ³Øª Ø³ÛŒØ³ØªÙ… Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯!")
        return True
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ³Øª Ø³ÛŒØ³ØªÙ…: {e}")
        return False

if __name__ == "__main__":
    main() 
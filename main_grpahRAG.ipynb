{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8CGAyl746sQ"
      },
      "source": [
        "**Ø³Ù„Ø§Ù… Ø¹Ù„ÛŒÚ©Ù…**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5mHQCLaVku2J"
      },
      "outputs": [],
      "source": [
        "!rm hetionet-v1.0-nodes.tsv  # Ø§Ú¯Ù‡ ÙØ§ÛŒÙ„ Ù‚Ø¯ÛŒÙ…ÛŒ Ù‡Ø³ØªØŒ Ù¾Ø§Ú©Ø´ Ú©Ù†"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rzRFxUFdkxnu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0228350c-50fb-4cc5-b857-948864af1257"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-19 16:24:59--  https://github.com/hetio/hetionet/raw/master/hetnet/tsv/hetionet-v1.0-nodes.tsv\n",
            "Resolving github.com (github.com)... 140.82.116.4\n",
            "Connecting to github.com (github.com)|140.82.116.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/hetio/hetionet/master/hetnet/tsv/hetionet-v1.0-nodes.tsv [following]\n",
            "--2025-07-19 16:24:59--  https://raw.githubusercontent.com/hetio/hetionet/master/hetnet/tsv/hetionet-v1.0-nodes.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2509984 (2.4M) [text/plain]\n",
            "Saving to: â€˜hetionet-v1.0-nodes.tsvâ€™\n",
            "\n",
            "hetionet-v1.0-nodes 100%[===================>]   2.39M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-07-19 16:24:59 (51.4 MB/s) - â€˜hetionet-v1.0-nodes.tsvâ€™ saved [2509984/2509984]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/hetio/hetionet/raw/master/hetnet/tsv/hetionet-v1.0-nodes.tsv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7714pfslnGrJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3956158-3072-47a6-c303-dcb0d81e4414"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-19 16:24:59--  https://github.com/hetio/hetionet/raw/master/hetnet/tsv/hetionet-v1.0-edges.sif.gz\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://media.githubusercontent.com/media/hetio/hetionet/master/hetnet/tsv/hetionet-v1.0-edges.sif.gz [following]\n",
            "--2025-07-19 16:25:00--  https://media.githubusercontent.com/media/hetio/hetionet/master/hetnet/tsv/hetionet-v1.0-edges.sif.gz\n",
            "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12401407 (12M) [application/octet-stream]\n",
            "Saving to: â€˜hetionet-v1.0-edges.sif.gzâ€™\n",
            "\n",
            "hetionet-v1.0-edges 100%[===================>]  11.83M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-07-19 16:25:00 (156 MB/s) - â€˜hetionet-v1.0-edges.sif.gzâ€™ saved [12401407/12401407]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/hetio/hetionet/raw/master/hetnet/tsv/hetionet-v1.0-edges.sif.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UzJkqrbm-sK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "381cf166-3211-46b1-b110-3d7452003d1a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gzip: hetionet-v1.0-edges.sif already exists; do you wish to overwrite (y or n)? "
          ]
        }
      ],
      "source": [
        "!gunzip hetionet-v1.0-edges.sif.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5n3ivB2kzu8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "nodes = pd.read_csv('hetionet-v1.0-nodes.tsv', sep='\\t', encoding='utf-8-sig')\n",
        "\n",
        "print(nodes.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3fgOfsek1Ua"
      },
      "outputs": [],
      "source": [
        "edges = pd.read_csv('hetionet-v1.0-edges.sif', sep='\\t')\n",
        "print(edges.columns)\n",
        "print(edges.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AW5AcBzlmi6S"
      },
      "outputs": [],
      "source": [
        "print(\"ğŸ“¦ Nodes:\")\n",
        "print(nodes.head())\n",
        "\n",
        "print(\"ğŸ”— Edges:\")\n",
        "print(edges.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9bXIRVhnR8A"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Ø³Ø§Ø®Øª Ú¯Ø±Ø§Ù\n",
        "G = nx.Graph()\n",
        "\n",
        "# Ø§ÙØ²ÙˆØ¯Ù† Ù†ÙˆØ¯Ù‡Ø§\n",
        "for _, row in nodes.iterrows():\n",
        "    G.add_node(row['id'], name=row['name'], kind=row['kind'])\n",
        "\n",
        "# Ø§ÙØ²ÙˆØ¯Ù† ÛŒØ§Ù„â€ŒÙ‡Ø§\n",
        "for _, row in edges.iterrows():\n",
        "    G.add_edge(row['source'], row['target'], metaedge=row['metaedge'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPng97JMG4Z1"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8ufxlD8oZ3F"
      },
      "source": [
        " pkl Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø±Ø§Ù"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWPZPLcPn2YE"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"hetionet_graph.pkl\", \"wb\") as f:\n",
        "    pickle.dump(G, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwnrdZXaocxn"
      },
      "source": [
        "Ù„ÙˆØ¯ Ú¯Ø±Ø§Ù pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0O6WLv4n9be"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"hetionet_graph.pkl\", \"rb\") as f:\n",
        "    G = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL5d3f6ppCiY"
      },
      "source": [
        "# Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø§ Ø±ÙˆØ´ Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha7H6vHCpIPt"
      },
      "source": [
        "DFS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhLoLVBWpJ7F"
      },
      "outputs": [],
      "source": [
        "from networkx.algorithms.traversal.depth_first_search import dfs_edges\n",
        "\n",
        "def dfs_search(graph, start_node, max_depth=2):\n",
        "    visited = set()\n",
        "    result = []\n",
        "\n",
        "    def dfs(node, depth):\n",
        "        if depth > max_depth:\n",
        "            return\n",
        "        visited.add(node)\n",
        "        result.append((node, depth))\n",
        "        for neighbor in graph.neighbors(node):\n",
        "            if neighbor not in visited:\n",
        "                dfs(neighbor, depth + 1)\n",
        "\n",
        "    dfs(start_node, 0)\n",
        "    return result\n",
        "\n",
        "# Ù…Ø«Ø§Ù„:\n",
        "start_node = 'Gene::3149'\n",
        "dfs_result = dfs_search(G, start_node, max_depth=2)\n",
        "for node, depth in dfs_result:\n",
        "    print(f\"Depth {depth}: {G.nodes[node]['kind']} - {G.nodes[node]['name']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFopfpRNVA1E"
      },
      "source": [
        "Ù…Ø´Ú©Ù„:\n",
        "\n",
        "Ù†ÙˆØ¯ Ù‡Ø§ÛŒ Ø¨Ø±Ú¯Ø±Ø¯ÙˆÙ†Ø¯Ù‡ Ø´Ø¯Ù‡ Ø®ÛŒÙ„ÛŒ Ø²ÛŒØ§Ø¯Ù†"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlOuLPDdpPE1"
      },
      "source": [
        "BFS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rP-QYdWNpNwQ"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "def bfs_search(graph, start_node, max_depth=2):\n",
        "    visited = set()\n",
        "    queue = deque([(start_node, 0)])\n",
        "    result = []\n",
        "\n",
        "    while queue:\n",
        "        node, depth = queue.popleft()\n",
        "        if node in visited or depth > max_depth:\n",
        "            continue\n",
        "        visited.add(node)\n",
        "        result.append((node, depth))\n",
        "        for neighbor in graph.neighbors(node):\n",
        "            if neighbor not in visited:\n",
        "                queue.append((neighbor, depth + 1))\n",
        "\n",
        "    return result\n",
        "\n",
        "# Ù…Ø«Ø§Ù„:\n",
        "start_node = 'Gene::3149'\n",
        "bfs_result = bfs_search(G, start_node, max_depth=2)\n",
        "for node, depth in bfs_result:\n",
        "    print(f\"Depth {depth}: {G.nodes[node]['kind']} - {G.nodes[node]['name']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76bwW0pzpVAo"
      },
      "source": [
        "Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø¨ÛŒÙ† Ø¯Ùˆ Ú¯Ø±Ù‡ (Ú©ÙˆØªØ§Ù‡â€ŒØªØ±ÛŒÙ† Ù…Ø³ÛŒØ±Ù‡Ø§)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arQMbtzCpXL7"
      },
      "outputs": [],
      "source": [
        "def get_shortest_paths(graph, source, target, max_paths=3):\n",
        "    try:\n",
        "        paths = list(nx.all_shortest_paths(graph, source=source, target=target))\n",
        "        return paths[:max_paths]\n",
        "    except nx.NetworkXNoPath:\n",
        "        return []\n",
        "\n",
        "# Ù…Ø«Ø§Ù„:\n",
        "source = 'Gene::3149'\n",
        "target = 'Biological Process::GO:0008150'\n",
        "paths = get_shortest_paths(G, source, target)\n",
        "\n",
        "for i, path in enumerate(paths):\n",
        "    print(f\"\\nPath {i+1}:\")\n",
        "    for node in path:\n",
        "        print(f\"- {G.nodes[node]['kind']}: {G.nodes[node]['name']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6dnRMMZpavQ"
      },
      "source": [
        "Ú¯Ø±ÙØªÙ† Ù‡Ù…Ø³Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø§ Ù†ÙˆØ¹ Ø®Ø§Øµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAahYOnrpaVM"
      },
      "outputs": [],
      "source": [
        "def get_neighbors_by_type(graph, node_id, kind_filter=None):\n",
        "    neighbors = []\n",
        "    for neighbor in graph.neighbors(node_id):\n",
        "        kind = graph.nodes[neighbor].get('kind')\n",
        "        if kind_filter is None or kind == kind_filter:\n",
        "            neighbors.append((neighbor, graph.nodes[neighbor]['name']))\n",
        "    return neighbors\n",
        "\n",
        "# Ù…Ø«Ø§Ù„:\n",
        "neighbors = get_neighbors_by_type(G, 'Gene::3149', kind_filter='Biological Process')\n",
        "for nid, name in neighbors:\n",
        "    print(f\"{nid} - {name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISonB8sxpeSM"
      },
      "source": [
        "ØªØ¨Ø¯ÛŒÙ„ Ù†ØªØ§ÛŒØ¬ Ú¯Ø±Ø§Ù Ø¨Ù‡ Ø¬Ù…Ù„Ø§Øª Ù…ØªÙ†ÛŒ (Ø¨Ø±Ø§ÛŒ RAG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9lMMF_Mpe7U"
      },
      "outputs": [],
      "source": [
        "def get_sentences_from_edges(graph, node_id, max_depth=1):\n",
        "    bfs_result = bfs_search(graph, node_id, max_depth)\n",
        "    sentences = []\n",
        "    for node, depth in bfs_result:\n",
        "        for neighbor in graph.neighbors(node):\n",
        "            if node == neighbor:\n",
        "                continue\n",
        "            u = graph.nodes[node]\n",
        "            v = graph.nodes[neighbor]\n",
        "            edge_data = graph.get_edge_data(node, neighbor)\n",
        "            sentences.append(f\"{u['name']} --[{edge_data['metaedge']}]--> {v['name']}\")\n",
        "    return list(set(sentences))  # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§\n",
        "\n",
        "# Ù…Ø«Ø§Ù„:\n",
        "sentences = get_sentences_from_edges(G, 'Gene::3149', max_depth=2)\n",
        "for s in sentences[:10]:\n",
        "    print(s)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXI4p4RAqMVC"
      },
      "source": [
        "# Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆÚ©Ù† Ùˆ Ø¬Ø³Øª Ùˆ Ø¬Ùˆ Ø±ÙˆØ§Ø¨Ø·"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM5cMEMwqjDF"
      },
      "source": [
        "Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø² Ø³Ø¤Ø§Ù„ Ø¨Ø§ spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUybmN4IqkHp"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_keywords(text):\n",
        "    doc = nlp(text)\n",
        "    keywords = set()\n",
        "\n",
        "    # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Named Entities\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ not in {\"DATE\", \"TIME\", \"PERCENT\", \"MONEY\", \"QUANTITY\", \"ORDINAL\", \"CARDINAL\"}:\n",
        "            keywords.add(ent.text.lower())\n",
        "\n",
        "    # 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù…â€ŒÙ‡Ø§ Ùˆ Ø§Ø³Ù… Ø®Ø§Øµâ€ŒÙ‡Ø§ Ø¨Ø§ POS\n",
        "    for token in doc:\n",
        "        if (\n",
        "            token.pos_ in {\"NOUN\", \"PROPN\"} and\n",
        "            token.text.lower() not in STOP_WORDS and\n",
        "            token.is_alpha and\n",
        "            len(token.text) > 2\n",
        "        ):\n",
        "            keywords.add(token.text.lower())\n",
        "\n",
        "    # 3. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ø§Ø² dependency head Ø¯Ø± Ø³ÙˆØ§Ù„Ø§Øª\n",
        "    for token in doc:\n",
        "        if token.dep_ in {\"nsubj\", \"dobj\", \"pobj\", \"attr\", \"ROOT\"} and token.is_alpha:\n",
        "            if token.text.lower() not in STOP_WORDS:\n",
        "                keywords.add(token.text.lower())\n",
        "\n",
        "    return sorted(keywords)\n",
        "question = \"Does metformin treat type 2 diabetes?\"\n",
        "tokens = extract_keywords(question)\n",
        "print(\"ğŸ” Extracted tokens:\", tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBaBEHDNq8BJ"
      },
      "source": [
        "Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ Ø¯Ø± Ú¯Ø±Ø§Ù"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7aF0-aPq9Da"
      },
      "outputs": [],
      "source": [
        "def match_tokens_to_nodes(graph, tokens):\n",
        "    matched = {}\n",
        "    for token in tokens:\n",
        "        token_lower = token.lower()\n",
        "        for node_id, attrs in graph.nodes(data=True):\n",
        "            if token_lower in attrs['name'].lower():\n",
        "                matched[token] = node_id\n",
        "                break\n",
        "    return matched\n",
        "\n",
        "# Ù…Ø«Ø§Ù„:\n",
        "matches = match_tokens_to_nodes(G, tokens)\n",
        "print(\"âœ… Matched nodes in graph:\")\n",
        "for token, node_id in matches.items():\n",
        "    print(f\"{token} â†’ {node_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tINxmvZq_i-"
      },
      "source": [
        " Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø§Ø±ØªØ¨Ø§Ø· Ø¨ÛŒÙ† Ø¯Ùˆ Ù…ÙˆØ¬ÙˆØ¯ÛŒØª"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NC91aRq5rAjS"
      },
      "outputs": [],
      "source": [
        "def get_direct_or_shortest_relation(graph, node1, node2, max_depth=2):\n",
        "    if graph.has_edge(node1, node2):\n",
        "        return [(node1, node2)]\n",
        "    try:\n",
        "        path = nx.shortest_path(graph, source=node1, target=node2)\n",
        "        return list(zip(path[:-1], path[1:]))\n",
        "    except nx.NetworkXNoPath:\n",
        "        return []\n",
        "\n",
        "# Ø§Ú¯Ø± Ø¯Ùˆ ØªØ§ node Ø¯Ø§Ø±ÛŒÙ…:\n",
        "if len(matches) >= 2:\n",
        "    token1, token2 = list(matches.values())[:2]\n",
        "    relations = get_direct_or_shortest_relation(G, token1, token2)\n",
        "\n",
        "    print(f\"\\n Path between {token1} and {token2}:\")\n",
        "    for u, v in relations:\n",
        "        rel = G.get_edge_data(u, v)['metaedge']\n",
        "        print(f\"{G.nodes[u]['name']} --[{rel}]--> {G.nodes[v]['name']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr532vNhrCOy"
      },
      "source": [
        "Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÛŒ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjSMySedrD4f"
      },
      "outputs": [],
      "source": [
        "question = \"What is the relationship between HMGB3 and pulmonary valve formation?\"\n",
        "\n",
        "tokens = extract_keywords(question)\n",
        "matches = match_tokens_to_nodes(G, tokens)\n",
        "\n",
        "if len(matches) >= 2:\n",
        "    node_ids = list(matches.values())\n",
        "    relations = get_direct_or_shortest_relation(G, node_ids[0], node_ids[1])\n",
        "    if relations:\n",
        "        print(f\"\\n Path found between {tokens[0]} and {tokens[1]}:\\n\")\n",
        "        for u, v in relations:\n",
        "            rel = G.get_edge_data(u, v)['metaedge']\n",
        "            print(f\"{G.nodes[u]['name']} --[{rel}]--> {G.nodes[v]['name']}\")\n",
        "    else:\n",
        "        print(\"No path found between matched nodes.\")\n",
        "else:\n",
        "    print(\"Less than 2 tokens matched in the graph.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDEdPZq7LlHZ"
      },
      "source": [
        "1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø² Ø¬Ù…Ù„Ù‡ (Ø¹Ø¨Ø§Ø±Ø§Øª Ú†Ù†Ø¯Ú©Ù„Ù…Ù‡â€ŒØ§ÛŒ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2keF6ddLpMz"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_keywords(text):\n",
        "    doc = nlp(text)\n",
        "    keywords = set()\n",
        "\n",
        "    # Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ø³Ù…ÛŒ Ú©Ø§Ù…Ù„ Ù…Ø«Ù„ \"pulmonary valve formation\"\n",
        "    for chunk in doc.noun_chunks:\n",
        "        phrase = chunk.text.lower().strip()\n",
        "        if len(phrase.split()) <= 5 and not all(w in STOP_WORDS for w in phrase.split()):\n",
        "            keywords.add(phrase)\n",
        "\n",
        "    # Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù…â€ŒØ¯Ø§Ø± (NER)\n",
        "    for ent in doc.ents:\n",
        "        keywords.add(ent.text.lower())\n",
        "\n",
        "    # Ø§Ø³Ù…â€ŒÙ‡Ø§ Ùˆ Ø§ÙØ¹Ø§Ù„ Ú©Ù„ÛŒØ¯ÛŒ\n",
        "    for token in doc:\n",
        "        if token.pos_ in {\"NOUN\", \"PROPN\", \"VERB\"} and token.text.lower() not in STOP_WORDS:\n",
        "            keywords.add(token.text.lower())\n",
        "\n",
        "    return sorted(keywords)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OqKbqkFLsAI"
      },
      "source": [
        "2. ØªØ·Ø¨ÛŒÙ‚ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø§ Ù†ÙˆØ¯Ù‡Ø§ÛŒ Ú¯Ø±Ø§Ù (match_tokens_to_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbICgLxBLul8"
      },
      "outputs": [],
      "source": [
        "from difflib import get_close_matches\n",
        "\n",
        "def match_tokens_to_nodes(graph, tokens):\n",
        "    matches = {}\n",
        "    all_node_names = {n: graph.nodes[n]['name'].lower() for n in graph.nodes}\n",
        "\n",
        "    for token in tokens:\n",
        "        # Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØ·Ø¨ÛŒÙ‚ÛŒ (fuzzy match) Ø¨Ø§ node name\n",
        "        close = get_close_matches(token, all_node_names.values(), n=1, cutoff=0.8)\n",
        "        if close:\n",
        "            matched_name = close[0]\n",
        "            for node_id, name in all_node_names.items():\n",
        "                if name == matched_name:\n",
        "                    matches[token] = node_id\n",
        "                    break\n",
        "    return matches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBIT9heALxXx"
      },
      "source": [
        "3. Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± Ø¨ÛŒÙ† Ø¯Ùˆ Ù†ÙˆØ¯ Ø¯Ø± Ú¯Ø±Ø§Ù"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdXu-rusL1fg"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "\n",
        "def get_direct_or_shortest_relation(graph, source, target):\n",
        "    if graph.has_edge(source, target):\n",
        "        return [(source, target)]\n",
        "    try:\n",
        "        path = nx.shortest_path(graph, source, target)\n",
        "        return list(zip(path, path[1:]))\n",
        "    except nx.NetworkXNoPath:\n",
        "        return []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tC24R5ybL49S"
      },
      "outputs": [],
      "source": [
        "question = \"What is the relationship between HMGB3 and pulmonary valve formation?\"\n",
        "\n",
        "tokens = extract_keywords(question)\n",
        "print(\"Extracted Keywords:\", tokens)\n",
        "\n",
        "matches = match_tokens_to_nodes(G, tokens)\n",
        "print(\"Matched Tokens:\", {k: G.nodes[v]['name'] for k, v in matches.items()})\n",
        "\n",
        "if len(matches) >= 2:\n",
        "    node_ids = list(matches.values())\n",
        "    relations = get_direct_or_shortest_relation(G, node_ids[0], node_ids[1])\n",
        "    if relations:\n",
        "        print(f\"\\nPath found between '{G.nodes[node_ids[0]]['name']}' and '{G.nodes[node_ids[1]]['name']}':\\n\")\n",
        "        for u, v in relations:\n",
        "            rel = G.get_edge_data(u, v).get('metaedge', 'related_to')\n",
        "            print(f\"{G.nodes[u]['name']} --[{rel}]--> {G.nodes[v]['name']}\")\n",
        "    else:\n",
        "        print(\"No path found between matched nodes.\")\n",
        "else:\n",
        "    print(\"Less than 2 tokens matched in the graph.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acvemJFUMJIH"
      },
      "source": [
        "Ù…Ø´Ú©Ù„:\n",
        "Ú©Ù„Ù…Ù‡ \"formation\" Ø¨Ù‡ Ø§Ø´ØªØ¨Ø§Ù‡ Ø¨Ù‡ Formication (Ø§Ø­Ø³Ø§Ø³ Ø®Ø²ÛŒØ¯Ù† Ø±ÙˆÛŒ Ù¾ÙˆØ³Øª!) Ù…Ú† Ø´Ø¯Ù‡ØŒ Ú†ÙˆÙ† Ø§Ø² Ù†Ø¸Ø± Ù†ÙˆØ´ØªØ§Ø±ÛŒ Ø´Ø¨ÛŒÙ‡ Ù‡Ø³Øª. ÙˆÙ„ÛŒ Ø§ÛŒÙ† Ø±Ø¨Ø·ÛŒ Ø¨Ù‡ Ø¨ÛŒÙˆÙ„ÙˆÚ˜ÛŒ Ù‚Ù„Ø¨ ÛŒØ§ Ø¯Ø±ÛŒÚ†Ù‡ Ù†Ø¯Ø§Ø±Ù‡.\n",
        "Ù…Ø³ÛŒØ± ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡:\n",
        "\n",
        "Formication â†’ Aripiprazole â†’ PCNA â†’ HMGB3\n",
        " Ù…Ø´Ú©Ù„ Ø´Ø¯ÛŒØ¯:\n",
        "Ø§ÛŒÙ† Ù…Ø³ÛŒØ± Ø§Ø² ÛŒÚ© Ø§Ø­Ø³Ø§Ø³ Ù¾ÙˆØ³ØªÛŒ Ø¨Ù‡ ÛŒÚ© Ø¯Ø§Ø±Ùˆ Ø±ÙˆØ§Ù†â€ŒÙ¾Ø²Ø´Ú©ÛŒ ØªØ§ Ø¨Ø±Ø³Ù‡ Ø¨Ù‡ Ú˜Ù† â€” ÛŒØ¹Ù†ÛŒ Ú©Ø§Ù…Ù„Ø§Ù‹ Ø¨ÛŒâ€ŒØ±Ø¨Ø· Ø¨Ù‡ Ù…ÙˆØ¶ÙˆØ¹ Ø§ØµÙ„ÛŒ:\n",
        "\"Ø±Ø§Ø¨Ø·Ù‡ HMGB3 Ùˆ ØªØ´Ú©ÛŒÙ„ Ø¯Ø±ÛŒÚ†Ù‡ Ø±ÛŒÙˆÛŒ\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFVwefkwMa7U"
      },
      "source": [
        " Ø±Ø§Ù‡â€ŒØ­Ù„ 1: ÙÙ‚Ø· Ø¹Ø¨Ø§Ø±Ø§Øª Ø¹Ù„Ù…ÛŒ Ú†Ù†Ø¯Ú©Ù„Ù…Ù‡â€ŒØ§ÛŒ Ù…Ù‡Ù… Ø±Ø§ Ù†Ú¯Ù‡ Ø¯Ø§Ø±ÛŒÙ…"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsbHhou9MJfQ"
      },
      "outputs": [],
      "source": [
        "def extract_important_phrases(text):\n",
        "    doc = nlp(text)\n",
        "    keywords = set()\n",
        "\n",
        "    for chunk in doc.noun_chunks:\n",
        "        phrase = chunk.text.lower().strip()\n",
        "        if len(phrase.split()) > 1 and not any(w in STOP_WORDS for w in phrase.split()):\n",
        "            keywords.add(phrase)\n",
        "\n",
        "    # Ù‡Ù…Ú†Ù†ÛŒÙ† Ø§Ø¬Ø§Ø²Ù‡ Ø¨Ø¯Ù‡ÛŒÙ… Ø¨Ø±Ø®ÛŒ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø®ÛŒÙ„ÛŒ Ù…Ù‡Ù… ØªÚ©ÛŒ Ù‡Ù… ÙˆØ§Ø±Ø¯ Ø´ÙˆÙ†Ø¯\n",
        "    for token in doc:\n",
        "        if token.text.lower() in {'hmgb3'}:\n",
        "            keywords.add(token.text.lower())\n",
        "\n",
        "    return sorted(keywords)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aALoigusMiI_"
      },
      "source": [
        " Ø±Ø§Ù‡â€ŒØ­Ù„ 2: Ù‡Ù†Ú¯Ø§Ù… ØªØ·Ø¨ÛŒÙ‚ØŒ ÙÙ‚Ø· Ø¨Ù‡ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ match Ø¯Ø§Ø±Ù†Ø¯ Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø¯Ù‡"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqMyIJp0MkZM"
      },
      "outputs": [],
      "source": [
        "def match_tokens_to_nodes_strict(graph, tokens):\n",
        "    matches = {}\n",
        "    node_names = {n: graph.nodes[n]['name'].lower() for n in graph.nodes}\n",
        "\n",
        "    for token in tokens:\n",
        "        exact_matches = [n for n, name in node_names.items() if token == name]\n",
        "        if exact_matches:\n",
        "            matches[token] = exact_matches[0]  # Ø§ÙˆÙ„ÛŒÙ† ØªØ·Ø¨ÛŒÙ‚\n",
        "    return matches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jF_gjPsKMoeE"
      },
      "outputs": [],
      "source": [
        "question = \"What is the relationship between HMGB3 and pulmonary valve formation?\"\n",
        "\n",
        "tokens = extract_important_phrases(question)\n",
        "print(\"Filtered Keywords:\", tokens)\n",
        "\n",
        "matches = match_tokens_to_nodes_strict(G, tokens)\n",
        "print(\"Matched Tokens:\", {k: G.nodes[v]['name'] for k, v in matches.items()})\n",
        "\n",
        "if len(matches) >= 2:\n",
        "    node_ids = list(matches.values())\n",
        "    relations = get_direct_or_shortest_relation(G, node_ids[0], node_ids[1])\n",
        "    if relations:\n",
        "        print(f\"\\n Path found between '{G.nodes[node_ids[0]]['name']}' and '{G.nodes[node_ids[1]]['name']}':\\n\")\n",
        "        for u, v in relations:\n",
        "            rel = G.get_edge_data(u, v).get('metaedge', 'related_to')\n",
        "            print(f\"{G.nodes[u]['name']} --[{rel}]--> {G.nodes[v]['name']}\")\n",
        "    else:\n",
        "        print(\"No path found between matched nodes.\")\n",
        "else:\n",
        "    print(\"Less than 2 tokens matched in the graph.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXFPgr3VNEwz"
      },
      "source": [
        "Ù‡ÙˆØ´Ù…Ù†Ø¯ØªØ± Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ BioNLP ÛŒØ§ BioBERT Ø§Ù†Ø¬Ø§Ù… Ø¨Ø´Ù‡"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZys2p12N-H8"
      },
      "source": [
        "Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UKbjjllN9lH"
      },
      "outputs": [],
      "source": [
        "!pip install scispacy\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_bionlp13cg_md-0.4.0.tar.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uu8OqMk4OCAG"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Ù…Ø¯Ù„ NER ØªØ®ØµØµÛŒ Ø¨Ø±Ø§ÛŒ Ø¹Ù„ÙˆÙ… Ø²ÛŒØ³ØªÛŒ\n",
        "nlp = spacy.load(\"en_ner_bionlp13cg_md\")\n",
        "\n",
        "def extract_biomedical_entities(text):\n",
        "    doc = nlp(text)\n",
        "    keywords = set()\n",
        "\n",
        "    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ³ØªÛŒ\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in {\n",
        "            \"GENE_OR_GENE_PRODUCT\",\n",
        "            \"CELLULAR_COMPONENT\",\n",
        "            \"SIMPLE_CHEMICAL\",\n",
        "            \"DISEASE_OR_DISORDER\",\n",
        "            \"ANATOMICAL_SYSTEM\",\n",
        "            \"BIOLOGICAL_PROCESS\",\n",
        "            \"CELL\",\n",
        "            \"TISSUE\"\n",
        "        }:\n",
        "            keywords.add(ent.text.lower())\n",
        "\n",
        "    return sorted(keywords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCulTcaoOH3G"
      },
      "outputs": [],
      "source": [
        "question = \"What is the relationship between HMGB3 and pulmonary valve formation?\"\n",
        "tokens = extract_biomedical_entities(question)\n",
        "\n",
        "print(\"ğŸ§¬ Extracted Biomedical Tokens:\", tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIRXBnRpaPvo"
      },
      "source": [
        " Ø§ÙØ²ÙˆØ¯Ù† semantic similarity Ø¨Ø§ BioBERT ÛŒØ§ PubMedBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lj5Bl540aTdt"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXDSRuAGac_q"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ BioBERT ÛŒØ§ PubMedBERT\n",
        "model_name = \"dmis-lab/biobert-base-cased-v1.1\"  # ÛŒØ§: \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6vqbhOJafi2"
      },
      "outputs": [],
      "source": [
        "def get_bert_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=32)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² [CLS] ØªÙˆÚ©Ù† Ø¨Ø±Ø§ÛŒ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù„ Ø¬Ù…Ù„Ù‡\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "    return cls_embedding.squeeze().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQyl15yAalmp"
      },
      "outputs": [],
      "source": [
        "node_embeddings = {}\n",
        "for node_id, data in G.nodes(data=True):\n",
        "    name = data['name']\n",
        "    try:\n",
        "        node_embeddings[node_id] = get_bert_embedding(name)\n",
        "    except:\n",
        "        continue  # Ø§Ú¯Ø± Ø®Ø·Ø§ÛŒÛŒ Ø¯Ø± ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø¨ÙˆØ¯\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AXp8V8Fam5y"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def find_closest_node(token, node_embeddings, topk=3):\n",
        "    token_emb = get_bert_embedding(token).reshape(1, -1)\n",
        "    sims = {\n",
        "        node_id: cosine_similarity(token_emb, emb.reshape(1, -1))[0][0]\n",
        "        for node_id, emb in node_embeddings.items()\n",
        "    }\n",
        "    top_matches = sorted(sims.items(), key=lambda x: x[1], reverse=True)[:topk]\n",
        "    return top_matches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgyqKDj7aqgr"
      },
      "source": [
        "Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hl7u-pvnapU2"
      },
      "outputs": [],
      "source": [
        "token = \"pulmonary valve formation\"\n",
        "top_matches = find_closest_node(token, node_embeddings)\n",
        "\n",
        "print(f\"\\nğŸ” Top matches for: '{token}'\")\n",
        "for node_id, score in top_matches:\n",
        "    print(f\"{G.nodes[node_id]['name']} (score: {score:.3f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK7eu7UENJoB"
      },
      "source": [
        "ÙˆØ²Ù† Ù…Ø³ÛŒØ± (Ù…Ø«Ù„Ø§Ù‹ Ù…Ø¬Ù…ÙˆØ¹ Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¨Ù‡ ØªØ¹Ø§Ù…Ù„Ø§Øª)\n",
        "\n",
        "Ù†Ù…Ø§ÛŒØ´ Ø¨ØµØ±ÛŒ Ù…Ø³ÛŒØ±\n",
        "\n",
        "Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† (top-3 shortest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcFvjJt5UaV4"
      },
      "source": [
        "# Ú©Ø§Ø±Ù‡Ø§ÛŒ ÙˆØ§Ø¬Ø¨\n",
        "1.Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ù‡ØªØ± Ú©Ù†ÛŒÙ…\n",
        "\n",
        "2.dfs Ùˆ bfs Ø®Ø±ÙˆØ¬ÛŒØ´ÙˆÙ† Ø®ÛŒÙ„ÛŒ Ø²ÛŒØ§Ø¯Ù‡\n",
        "\n",
        "3.Ù…Ø¯Ù„ Ø¬Ù†Ø§Ø±ÙˆØªÙˆØ±\n",
        "\n",
        "4.Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ø±Ù‡Ø§ÛŒÛŒ Ú©Ù‡ ØªØ§ Ø§Ù„Ø§Ù† Ú©Ø±Ø¯ÛŒÙ…\n",
        "\n",
        "5.Ø§ÛŒÙ…ÛŒÙ„ Ø¨Ù‡ Ø¨ÛŒÚ¯ÛŒ\n",
        "\n",
        "6.Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ù‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø² Ú¯Ø±Ø§Ù(ØªØ±Ú©ÛŒØ¨ÛŒ Ùˆ ÙˆØ²Ù† Ø¯Ø§Ø± Ùˆ Ø¯Ø§ÛŒØ¬Ø³ØªØ±Ø§ Ùˆ ...)\n",
        "\n",
        "7.Ù‚Ø¨Ù„ Ø§Ø² Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù†ÙˆØ¯ ÛŒÙ‡ Ø¯ÙˆØ± Ø¨Ø¯ÛŒÙ… Ø¨Ù‡ ÛŒÙ‡ Ù…Ø¯Ù„ gpt"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
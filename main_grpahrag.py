# -*- coding: utf-8 -*-
"""main-grpahRAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I_Qpb6_fNU8gUZVRzeMF-6UytGGfanK4

**Ø³Ù„Ø§Ù… Ø¹Ù„ÛŒÚ©Ù…**
"""

!rm hetionet-v1.0-nodes.tsv  # Ø§Ú¯Ù‡ ÙØ§ÛŒÙ„ Ù‚Ø¯ÛŒÙ…ÛŒ Ù‡Ø³ØªØŒ Ù¾Ø§Ú©Ø´ Ú©Ù†

!wget https://github.com/hetio/hetionet/raw/master/hetnet/tsv/hetionet-v1.0-nodes.tsv

!wget https://github.com/hetio/hetionet/raw/master/hetnet/tsv/hetionet-v1.0-edges.sif.gz

!gunzip hetionet-v1.0-edges.sif.gz

import pandas as pd
nodes = pd.read_csv('hetionet-v1.0-nodes.tsv', sep='\t', encoding='utf-8-sig')

print(nodes.columns)

edges = pd.read_csv('hetionet-v1.0-edges.sif', sep='\t')
print(edges.columns)
print(edges.head())

print("ğŸ“¦ Nodes:")
print(nodes.head())

print("ğŸ”— Edges:")
print(edges.head())

import networkx as nx

# Ø³Ø§Ø®Øª Ú¯Ø±Ø§Ù
G = nx.Graph()

# Ø§ÙØ²ÙˆØ¯Ù† Ù†ÙˆØ¯Ù‡Ø§
for _, row in nodes.iterrows():
    G.add_node(row['id'], name=row['name'], kind=row['kind'])

# Ø§ÙØ²ÙˆØ¯Ù† ÛŒØ§Ù„â€ŒÙ‡Ø§
for _, row in edges.iterrows():
    G.add_edge(row['source'], row['target'], metaedge=row['metaedge'])

""" pkl Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø±Ø§Ù"""

import pickle

with open("hetionet_graph.pkl", "wb") as f:
    pickle.dump(G, f)

"""Ù„ÙˆØ¯ Ú¯Ø±Ø§Ù pkl"""

import pickle

with open("hetionet_graph.pkl", "rb") as f:
    G = pickle.load(f)

"""# Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø§ Ø±ÙˆØ´ Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù

DFS
"""

from networkx.algorithms.traversal.depth_first_search import dfs_edges

def dfs_search(graph, start_node, max_depth=2):
    visited = set()
    result = []

    def dfs(node, depth):
        if depth > max_depth:
            return
        visited.add(node)
        result.append((node, depth))
        for neighbor in graph.neighbors(node):
            if neighbor not in visited:
                dfs(neighbor, depth + 1)

    dfs(start_node, 0)
    return result

# Ù…Ø«Ø§Ù„:
start_node = 'Gene::3149'
dfs_result = dfs_search(G, start_node, max_depth=2)
for node, depth in dfs_result:
    print(f"Depth {depth}: {G.nodes[node]['kind']} - {G.nodes[node]['name']}")

"""Ù…Ø´Ú©Ù„:

Ù†ÙˆØ¯ Ù‡Ø§ÛŒ Ø¨Ø±Ú¯Ø±Ø¯ÙˆÙ†Ø¯Ù‡ Ø´Ø¯Ù‡ Ø®ÛŒÙ„ÛŒ Ø²ÛŒØ§Ø¯Ù†

BFS
"""

from collections import deque

def bfs_search(graph, start_node, max_depth=2):
    visited = set()
    queue = deque([(start_node, 0)])
    result = []

    while queue:
        node, depth = queue.popleft()
        if node in visited or depth > max_depth:
            continue
        visited.add(node)
        result.append((node, depth))
        for neighbor in graph.neighbors(node):
            if neighbor not in visited:
                queue.append((neighbor, depth + 1))

    return result

# Ù…Ø«Ø§Ù„:
start_node = 'Gene::3149'
bfs_result = bfs_search(G, start_node, max_depth=2)
for node, depth in bfs_result:
    print(f"Depth {depth}: {G.nodes[node]['kind']} - {G.nodes[node]['name']}")

"""Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø¨ÛŒÙ† Ø¯Ùˆ Ú¯Ø±Ù‡ (Ú©ÙˆØªØ§Ù‡â€ŒØªØ±ÛŒÙ† Ù…Ø³ÛŒØ±Ù‡Ø§)"""

def get_shortest_paths(graph, source, target, max_paths=3):
    try:
        paths = list(nx.all_shortest_paths(graph, source=source, target=target))
        return paths[:max_paths]
    except nx.NetworkXNoPath:
        return []

# Ù…Ø«Ø§Ù„:
source = 'Gene::3149'
target = 'Biological Process::GO:0008150'
paths = get_shortest_paths(G, source, target)

for i, path in enumerate(paths):
    print(f"\nPath {i+1}:")
    for node in path:
        print(f"- {G.nodes[node]['kind']}: {G.nodes[node]['name']}")

"""Ú¯Ø±ÙØªÙ† Ù‡Ù…Ø³Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø§ Ù†ÙˆØ¹ Ø®Ø§Øµ"""

def get_neighbors_by_type(graph, node_id, kind_filter=None):
    neighbors = []
    for neighbor in graph.neighbors(node_id):
        kind = graph.nodes[neighbor].get('kind')
        if kind_filter is None or kind == kind_filter:
            neighbors.append((neighbor, graph.nodes[neighbor]['name']))
    return neighbors

# Ù…Ø«Ø§Ù„:
neighbors = get_neighbors_by_type(G, 'Gene::3149', kind_filter='Biological Process')
for nid, name in neighbors:
    print(f"{nid} - {name}")

"""ØªØ¨Ø¯ÛŒÙ„ Ù†ØªØ§ÛŒØ¬ Ú¯Ø±Ø§Ù Ø¨Ù‡ Ø¬Ù…Ù„Ø§Øª Ù…ØªÙ†ÛŒ (Ø¨Ø±Ø§ÛŒ RAG)"""

def get_sentences_from_edges(graph, node_id, max_depth=1):
    bfs_result = bfs_search(graph, node_id, max_depth)
    sentences = []
    for node, depth in bfs_result:
        for neighbor in graph.neighbors(node):
            if node == neighbor:
                continue
            u = graph.nodes[node]
            v = graph.nodes[neighbor]
            edge_data = graph.get_edge_data(node, neighbor)
            sentences.append(f"{u['name']} --[{edge_data['metaedge']}]--> {v['name']}")
    return list(set(sentences))  # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§

# Ù…Ø«Ø§Ù„:
sentences = get_sentences_from_edges(G, 'Gene::3149', max_depth=2)
for s in sentences[:10]:
    print(s)

"""# Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆÚ©Ù† Ùˆ Ø¬Ø³Øª Ùˆ Ø¬Ùˆ Ø±ÙˆØ§Ø¨Ø·

Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø² Ø³Ø¤Ø§Ù„ Ø¨Ø§ spaCy
"""

import spacy
from spacy.lang.en.stop_words import STOP_WORDS

nlp = spacy.load("en_core_web_sm")

def extract_keywords(text):
    doc = nlp(text)
    keywords = set()

    # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Named Entities
    for ent in doc.ents:
        if ent.label_ not in {"DATE", "TIME", "PERCENT", "MONEY", "QUANTITY", "ORDINAL", "CARDINAL"}:
            keywords.add(ent.text.lower())

    # 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù…â€ŒÙ‡Ø§ Ùˆ Ø§Ø³Ù… Ø®Ø§Øµâ€ŒÙ‡Ø§ Ø¨Ø§ POS
    for token in doc:
        if (
            token.pos_ in {"NOUN", "PROPN"} and
            token.text.lower() not in STOP_WORDS and
            token.is_alpha and
            len(token.text) > 2
        ):
            keywords.add(token.text.lower())

    # 3. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ø§Ø² dependency head Ø¯Ø± Ø³ÙˆØ§Ù„Ø§Øª
    for token in doc:
        if token.dep_ in {"nsubj", "dobj", "pobj", "attr", "ROOT"} and token.is_alpha:
            if token.text.lower() not in STOP_WORDS:
                keywords.add(token.text.lower())

    return sorted(keywords)
question = "Does metformin treat type 2 diabetes?"
tokens = extract_keywords(question)
print("ğŸ” Extracted tokens:", tokens)

"""Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ Ø¯Ø± Ú¯Ø±Ø§Ù"""

def match_tokens_to_nodes(graph, tokens):
    matched = {}
    for token in tokens:
        token_lower = token.lower()
        for node_id, attrs in graph.nodes(data=True):
            if token_lower in attrs['name'].lower():
                matched[token] = node_id
                break
    return matched

# Ù…Ø«Ø§Ù„:
matches = match_tokens_to_nodes(G, tokens)
print("âœ… Matched nodes in graph:")
for token, node_id in matches.items():
    print(f"{token} â†’ {node_id}")

""" Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø§Ø±ØªØ¨Ø§Ø· Ø¨ÛŒÙ† Ø¯Ùˆ Ù…ÙˆØ¬ÙˆØ¯ÛŒØª"""

def get_direct_or_shortest_relation(graph, node1, node2, max_depth=2):
    if graph.has_edge(node1, node2):
        return [(node1, node2)]
    try:
        path = nx.shortest_path(graph, source=node1, target=node2)
        return list(zip(path[:-1], path[1:]))
    except nx.NetworkXNoPath:
        return []

# Ø§Ú¯Ø± Ø¯Ùˆ ØªØ§ node Ø¯Ø§Ø±ÛŒÙ…:
if len(matches) >= 2:
    token1, token2 = list(matches.values())[:2]
    relations = get_direct_or_shortest_relation(G, token1, token2)

    print(f"\n Path between {token1} and {token2}:")
    for u, v in relations:
        rel = G.get_edge_data(u, v)['metaedge']
        print(f"{G.nodes[u]['name']} --[{rel}]--> {G.nodes[v]['name']}")

"""Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÛŒ"""

question = "What is the relationship between HMGB3 and pulmonary valve formation?"

tokens = extract_keywords(question)
matches = match_tokens_to_nodes(G, tokens)

if len(matches) >= 2:
    node_ids = list(matches.values())
    relations = get_direct_or_shortest_relation(G, node_ids[0], node_ids[1])
    if relations:
        print(f"\n Path found between {tokens[0]} and {tokens[1]}:\n")
        for u, v in relations:
            rel = G.get_edge_data(u, v)['metaedge']
            print(f"{G.nodes[u]['name']} --[{rel}]--> {G.nodes[v]['name']}")
    else:
        print("No path found between matched nodes.")
else:
    print("Less than 2 tokens matched in the graph.")

"""1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø² Ø¬Ù…Ù„Ù‡ (Ø¹Ø¨Ø§Ø±Ø§Øª Ú†Ù†Ø¯Ú©Ù„Ù…Ù‡â€ŒØ§ÛŒ)"""

import spacy
from spacy.lang.en.stop_words import STOP_WORDS

nlp = spacy.load("en_core_web_sm")

def extract_keywords(text):
    doc = nlp(text)
    keywords = set()

    # Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ø³Ù…ÛŒ Ú©Ø§Ù…Ù„ Ù…Ø«Ù„ "pulmonary valve formation"
    for chunk in doc.noun_chunks:
        phrase = chunk.text.lower().strip()
        if len(phrase.split()) <= 5 and not all(w in STOP_WORDS for w in phrase.split()):
            keywords.add(phrase)

    # Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù…â€ŒØ¯Ø§Ø± (NER)
    for ent in doc.ents:
        keywords.add(ent.text.lower())

    # Ø§Ø³Ù…â€ŒÙ‡Ø§ Ùˆ Ø§ÙØ¹Ø§Ù„ Ú©Ù„ÛŒØ¯ÛŒ
    for token in doc:
        if token.pos_ in {"NOUN", "PROPN", "VERB"} and token.text.lower() not in STOP_WORDS:
            keywords.add(token.text.lower())

    return sorted(keywords)

"""2. ØªØ·Ø¨ÛŒÙ‚ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø§ Ù†ÙˆØ¯Ù‡Ø§ÛŒ Ú¯Ø±Ø§Ù (match_tokens_to_nodes)"""

from difflib import get_close_matches

def match_tokens_to_nodes(graph, tokens):
    matches = {}
    all_node_names = {n: graph.nodes[n]['name'].lower() for n in graph.nodes}

    for token in tokens:
        # Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØ·Ø¨ÛŒÙ‚ÛŒ (fuzzy match) Ø¨Ø§ node name
        close = get_close_matches(token, all_node_names.values(), n=1, cutoff=0.8)
        if close:
            matched_name = close[0]
            for node_id, name in all_node_names.items():
                if name == matched_name:
                    matches[token] = node_id
                    break
    return matches

"""3. Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± Ø¨ÛŒÙ† Ø¯Ùˆ Ù†ÙˆØ¯ Ø¯Ø± Ú¯Ø±Ø§Ù"""

import networkx as nx

def get_direct_or_shortest_relation(graph, source, target):
    if graph.has_edge(source, target):
        return [(source, target)]
    try:
        path = nx.shortest_path(graph, source, target)
        return list(zip(path, path[1:]))
    except nx.NetworkXNoPath:
        return []

question = "What is the relationship between HMGB3 and pulmonary valve formation?"

tokens = extract_keywords(question)
print("Extracted Keywords:", tokens)

matches = match_tokens_to_nodes(G, tokens)
print("Matched Tokens:", {k: G.nodes[v]['name'] for k, v in matches.items()})

if len(matches) >= 2:
    node_ids = list(matches.values())
    relations = get_direct_or_shortest_relation(G, node_ids[0], node_ids[1])
    if relations:
        print(f"\nPath found between '{G.nodes[node_ids[0]]['name']}' and '{G.nodes[node_ids[1]]['name']}':\n")
        for u, v in relations:
            rel = G.get_edge_data(u, v).get('metaedge', 'related_to')
            print(f"{G.nodes[u]['name']} --[{rel}]--> {G.nodes[v]['name']}")
    else:
        print("No path found between matched nodes.")
else:
    print("Less than 2 tokens matched in the graph.")

"""Ù…Ø´Ú©Ù„:
Ú©Ù„Ù…Ù‡ "formation" Ø¨Ù‡ Ø§Ø´ØªØ¨Ø§Ù‡ Ø¨Ù‡ Formication (Ø§Ø­Ø³Ø§Ø³ Ø®Ø²ÛŒØ¯Ù† Ø±ÙˆÛŒ Ù¾ÙˆØ³Øª!) Ù…Ú† Ø´Ø¯Ù‡ØŒ Ú†ÙˆÙ† Ø§Ø² Ù†Ø¸Ø± Ù†ÙˆØ´ØªØ§Ø±ÛŒ Ø´Ø¨ÛŒÙ‡ Ù‡Ø³Øª. ÙˆÙ„ÛŒ Ø§ÛŒÙ† Ø±Ø¨Ø·ÛŒ Ø¨Ù‡ Ø¨ÛŒÙˆÙ„ÙˆÚ˜ÛŒ Ù‚Ù„Ø¨ ÛŒØ§ Ø¯Ø±ÛŒÚ†Ù‡ Ù†Ø¯Ø§Ø±Ù‡.
Ù…Ø³ÛŒØ± ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡:

Formication â†’ Aripiprazole â†’ PCNA â†’ HMGB3
 Ù…Ø´Ú©Ù„ Ø´Ø¯ÛŒØ¯:
Ø§ÛŒÙ† Ù…Ø³ÛŒØ± Ø§Ø² ÛŒÚ© Ø§Ø­Ø³Ø§Ø³ Ù¾ÙˆØ³ØªÛŒ Ø¨Ù‡ ÛŒÚ© Ø¯Ø§Ø±Ùˆ Ø±ÙˆØ§Ù†â€ŒÙ¾Ø²Ø´Ú©ÛŒ ØªØ§ Ø¨Ø±Ø³Ù‡ Ø¨Ù‡ Ú˜Ù† â€” ÛŒØ¹Ù†ÛŒ Ú©Ø§Ù…Ù„Ø§Ù‹ Ø¨ÛŒâ€ŒØ±Ø¨Ø· Ø¨Ù‡ Ù…ÙˆØ¶ÙˆØ¹ Ø§ØµÙ„ÛŒ:
"Ø±Ø§Ø¨Ø·Ù‡ HMGB3 Ùˆ ØªØ´Ú©ÛŒÙ„ Ø¯Ø±ÛŒÚ†Ù‡ Ø±ÛŒÙˆÛŒ".

Ø±Ø§Ù‡â€ŒØ­Ù„ 1: ÙÙ‚Ø· Ø¹Ø¨Ø§Ø±Ø§Øª Ø¹Ù„Ù…ÛŒ Ú†Ù†Ø¯Ú©Ù„Ù…Ù‡â€ŒØ§ÛŒ Ù…Ù‡Ù… Ø±Ø§ Ù†Ú¯Ù‡ Ø¯Ø§Ø±ÛŒÙ…
"""

def extract_important_phrases(text):
    doc = nlp(text)
    keywords = set()

    for chunk in doc.noun_chunks:
        phrase = chunk.text.lower().strip()
        if len(phrase.split()) > 1 and not any(w in STOP_WORDS for w in phrase.split()):
            keywords.add(phrase)

    # Ù‡Ù…Ú†Ù†ÛŒÙ† Ø§Ø¬Ø§Ø²Ù‡ Ø¨Ø¯Ù‡ÛŒÙ… Ø¨Ø±Ø®ÛŒ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø®ÛŒÙ„ÛŒ Ù…Ù‡Ù… ØªÚ©ÛŒ Ù‡Ù… ÙˆØ§Ø±Ø¯ Ø´ÙˆÙ†Ø¯
    for token in doc:
        if token.text.lower() in {'hmgb3'}:
            keywords.add(token.text.lower())

    return sorted(keywords)

""" Ø±Ø§Ù‡â€ŒØ­Ù„ 2: Ù‡Ù†Ú¯Ø§Ù… ØªØ·Ø¨ÛŒÙ‚ØŒ ÙÙ‚Ø· Ø¨Ù‡ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ match Ø¯Ø§Ø±Ù†Ø¯ Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø¯Ù‡"""

def match_tokens_to_nodes_strict(graph, tokens):
    matches = {}
    node_names = {n: graph.nodes[n]['name'].lower() for n in graph.nodes}

    for token in tokens:
        exact_matches = [n for n, name in node_names.items() if token == name]
        if exact_matches:
            matches[token] = exact_matches[0]  # Ø§ÙˆÙ„ÛŒÙ† ØªØ·Ø¨ÛŒÙ‚
    return matches

question = "What is the relationship between HMGB3 and pulmonary valve formation?"

tokens = extract_important_phrases(question)
print("Filtered Keywords:", tokens)

matches = match_tokens_to_nodes_strict(G, tokens)
print("Matched Tokens:", {k: G.nodes[v]['name'] for k, v in matches.items()})

if len(matches) >= 2:
    node_ids = list(matches.values())
    relations = get_direct_or_shortest_relation(G, node_ids[0], node_ids[1])
    if relations:
        print(f"\n Path found between '{G.nodes[node_ids[0]]['name']}' and '{G.nodes[node_ids[1]]['name']}':\n")
        for u, v in relations:
            rel = G.get_edge_data(u, v).get('metaedge', 'related_to')
            print(f"{G.nodes[u]['name']} --[{rel}]--> {G.nodes[v]['name']}")
    else:
        print("No path found between matched nodes.")
else:
    print("Less than 2 tokens matched in the graph.")

"""Ù‡ÙˆØ´Ù…Ù†Ø¯ØªØ± Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ BioNLP ÛŒØ§ BioBERT Ø§Ù†Ø¬Ø§Ù… Ø¨Ø´Ù‡

Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§
"""

!pip install scispacy
!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_bionlp13cg_md-0.4.0.tar.gz

import spacy

# Ù…Ø¯Ù„ NER ØªØ®ØµØµÛŒ Ø¨Ø±Ø§ÛŒ Ø¹Ù„ÙˆÙ… Ø²ÛŒØ³ØªÛŒ
nlp = spacy.load("en_ner_bionlp13cg_md")

def extract_biomedical_entities(text):
    doc = nlp(text)
    keywords = set()

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ³ØªÛŒ
    for ent in doc.ents:
        if ent.label_ in {
            "GENE_OR_GENE_PRODUCT",
            "CELLULAR_COMPONENT",
            "SIMPLE_CHEMICAL",
            "DISEASE_OR_DISORDER",
            "ANATOMICAL_SYSTEM",
            "BIOLOGICAL_PROCESS",
            "CELL",
            "TISSUE"
        }:
            keywords.add(ent.text.lower())

    return sorted(keywords)

question = "What is the relationship between HMGB3 and pulmonary valve formation?"
tokens = extract_biomedical_entities(question)

print("ğŸ§¬ Extracted Biomedical Tokens:", tokens)

""" Ø§ÙØ²ÙˆØ¯Ù† semantic similarity Ø¨Ø§ BioBERT ÛŒØ§ PubMedBERT"""

!pip install transformers
!pip install torch

from transformers import AutoTokenizer, AutoModel
import torch

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ BioBERT ÛŒØ§ PubMedBERT
model_name = "dmis-lab/biobert-base-cased-v1.1"  # ÛŒØ§: "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

def get_bert_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=32)
    with torch.no_grad():
        outputs = model(**inputs)
    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² [CLS] ØªÙˆÚ©Ù† Ø¨Ø±Ø§ÛŒ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù„ Ø¬Ù…Ù„Ù‡
    cls_embedding = outputs.last_hidden_state[:, 0, :]
    return cls_embedding.squeeze().numpy()

node_embeddings = {}
for node_id, data in G.nodes(data=True):
    name = data['name']
    try:
        node_embeddings[node_id] = get_bert_embedding(name)
    except:
        continue  # Ø§Ú¯Ø± Ø®Ø·Ø§ÛŒÛŒ Ø¯Ø± ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø¨ÙˆØ¯

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def find_closest_node(token, node_embeddings, topk=3):
    token_emb = get_bert_embedding(token).reshape(1, -1)
    sims = {
        node_id: cosine_similarity(token_emb, emb.reshape(1, -1))[0][0]
        for node_id, emb in node_embeddings.items()
    }
    top_matches = sorted(sims.items(), key=lambda x: x[1], reverse=True)[:topk]
    return top_matches

"""Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡:"""

token = "pulmonary valve formation"
top_matches = find_closest_node(token, node_embeddings)

print(f"\nğŸ” Top matches for: '{token}'")
for node_id, score in top_matches:
    print(f"{G.nodes[node_id]['name']} (score: {score:.3f})")

"""ÙˆØ²Ù† Ù…Ø³ÛŒØ± (Ù…Ø«Ù„Ø§Ù‹ Ù…Ø¬Ù…ÙˆØ¹ Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¨Ù‡ ØªØ¹Ø§Ù…Ù„Ø§Øª)

Ù†Ù…Ø§ÛŒØ´ Ø¨ØµØ±ÛŒ Ù…Ø³ÛŒØ±

Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† (top-3 shortest)

# Ú©Ø§Ø±Ù‡Ø§ÛŒ ÙˆØ§Ø¬Ø¨
1.Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ù‡ØªØ± Ú©Ù†ÛŒÙ…

2.dfs Ùˆ bfs Ø®Ø±ÙˆØ¬ÛŒØ´ÙˆÙ† Ø®ÛŒÙ„ÛŒ Ø²ÛŒØ§Ø¯Ù‡

3.Ù…Ø¯Ù„ Ø¬Ù†Ø§Ø±ÙˆØªÙˆØ±

4.Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ø±Ù‡Ø§ÛŒÛŒ Ú©Ù‡ ØªØ§ Ø§Ù„Ø§Ù† Ú©Ø±Ø¯ÛŒÙ…

5.Ø§ÛŒÙ…ÛŒÙ„ Ø¨Ù‡ Ø¨ÛŒÚ¯ÛŒ

6.Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ù‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø² Ú¯Ø±Ø§Ù(ØªØ±Ú©ÛŒØ¨ÛŒ Ùˆ ÙˆØ²Ù† Ø¯Ø§Ø± Ùˆ Ø¯Ø§ÛŒØ¬Ø³ØªØ±Ø§ Ùˆ ...)

7.Ù‚Ø¨Ù„ Ø§Ø² Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù†ÙˆØ¯ ÛŒÙ‡ Ø¯ÙˆØ± Ø¨Ø¯ÛŒÙ… Ø¨Ù‡ ÛŒÙ‡ Ù…Ø¯Ù„ gpt
"""
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ØªØ³Øª Ø³Ø±ÙˆÛŒØ³ Ù¾ÛŒØ´Ø±ÙØªÙ‡ GraphRAG
"""

import sys
import os
import json
import time
from datetime import datetime

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± Ù¾Ø±ÙˆÚ˜Ù‡ Ø¨Ù‡ sys.path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from enhanced_graphrag_service import EnhancedGraphRAGService, TokenExtractionMethod, RetrievalAlgorithm, CommunityDetectionMethod

def test_enhanced_service():
    """ØªØ³Øª Ø³Ø±ÙˆÛŒØ³ Ù¾ÛŒØ´Ø±ÙØªÙ‡ GraphRAG"""
    
    print("ğŸ§ª Ø´Ø±ÙˆØ¹ ØªØ³Øª Ø³Ø±ÙˆÛŒØ³ Ù¾ÛŒØ´Ø±ÙØªÙ‡ GraphRAG")
    print("=" * 50)
    
    # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³Ø±ÙˆÛŒØ³
    print("ğŸ“¦ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³Ø±ÙˆÛŒØ³...")
    try:
        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙØ§ÛŒÙ„ Ú¯Ø±Ø§Ù
        graph_files = [f for f in os.listdir('.') if f.startswith('hetionet_graph_') and f.endswith('.pkl')]
        if graph_files:
            latest_graph_file = max(graph_files)
            print(f"ğŸ“Š Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú¯Ø±Ø§Ù: {latest_graph_file}")
            service = EnhancedGraphRAGService(graph_data_path=latest_graph_file)
        else:
            print("âš ï¸ ÙØ§ÛŒÙ„ Ú¯Ø±Ø§Ù ÛŒØ§ÙØª Ù†Ø´Ø¯ØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø³Ø±ÙˆÛŒØ³ Ø¨Ø¯ÙˆÙ† Ú¯Ø±Ø§Ù")
            service = EnhancedGraphRAGService()
        
        print("âœ… Ø³Ø±ÙˆÛŒØ³ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³Ø±ÙˆÛŒØ³: {e}")
        return
    
    # ØªØ³Øª ØªÙ†Ø¸ÛŒÙ…Ø§Øª
    print("\nğŸ”§ ØªØ³Øª ØªÙ†Ø¸ÛŒÙ…Ø§Øª...")
    test_config(service)
    
    # ØªØ³Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆÚ©Ù†
    print("\nğŸ” ØªØ³Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆÚ©Ù†...")
    test_token_extraction(service)
    
    # ØªØ³Øª Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ
    print("\nğŸ¯ ØªØ³Øª Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ...")
    test_retrieval_algorithms(service)
    
    # ØªØ³Øª Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø®ØªÙ„Ù
    print("\nâ“ ØªØ³Øª Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø®ØªÙ„Ù...")
    test_queries(service)
    
    print("\nğŸ‰ ØªØ³Øªâ€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯!")

def test_config(service):
    """ØªØ³Øª ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø³Ø±ÙˆÛŒØ³"""
    
    # Ø¯Ø±ÛŒØ§ÙØª ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙØ¹Ù„ÛŒ
    config = service.get_config()
    print(f"ğŸ“‹ ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙØ¹Ù„ÛŒ: {json.dumps(config, indent=2, ensure_ascii=False)}")
    
    # ØªØºÛŒÛŒØ± ØªÙ†Ø¸ÛŒÙ…Ø§Øª
    new_config = {
        'token_extraction_method': 'hybrid',
        'retrieval_algorithm': 'pagerank',
        'max_depth': 4,
        'max_nodes': 25,
        'similarity_threshold': 0.4
    }
    
    service.set_config(**new_config)
    updated_config = service.get_config()
    print(f"ğŸ“‹ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡â€ŒØ±ÙˆØ² Ø´Ø¯Ù‡: {json.dumps(updated_config, indent=2, ensure_ascii=False)}")
    
    # ØªØ³Øª Ø¢Ù…Ø§Ø± Ú¯Ø±Ø§Ù
    if service.G:
        stats = service.get_graph_statistics()
        print(f"ğŸ“Š Ø¢Ù…Ø§Ø± Ú¯Ø±Ø§Ù: {json.dumps(stats, indent=2, ensure_ascii=False)}")

def test_token_extraction(service):
    """ØªØ³Øª Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆÚ©Ù†"""
    
    test_queries = [
        "What genes are associated with diabetes?",
        "How does TP53 relate to cancer?",
        "What drugs treat heart disease?",
        "Which pathways are involved in metabolism?"
    ]
    
    methods = [
        TokenExtractionMethod.LLM_BASED,
        TokenExtractionMethod.RULE_BASED,
        TokenExtractionMethod.HYBRID,
        TokenExtractionMethod.SEMANTIC
    ]
    
    for query in test_queries:
        print(f"\nğŸ” Ø³ÙˆØ§Ù„: {query}")
        
        for method in methods:
            service.config.token_extraction_method = method
            try:
                answer_types, entities = service.extract_tokens(query)
                print(f"  ğŸ“ {method.value}:")
                print(f"    Ù†ÙˆØ¹ Ù¾Ø§Ø³Ø®: {answer_types}")
                print(f"    Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§: {entities}")
            except Exception as e:
                print(f"    âŒ Ø®Ø·Ø§: {e}")

def test_retrieval_algorithms(service):
    """ØªØ³Øª Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ"""
    
    if not service.G:
        print("âš ï¸ Ú¯Ø±Ø§Ù Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø´Ø¯Ù‡ØŒ ØªØ³Øª Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ Ø§Ù†Ø¬Ø§Ù… Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯")
        return
    
    test_query = "What genes are associated with diabetes?"
    start_nodes = ["DIABETES", "GENE", "TP53"]  # Ù†ÙˆØ¯Ù‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡
    
    algorithms = [
        RetrievalAlgorithm.BFS,
        RetrievalAlgorithm.DFS,
        RetrievalAlgorithm.PAGERANK,
        RetrievalAlgorithm.COMMUNITY_DETECTION,
        RetrievalAlgorithm.SEMANTIC_SIMILARITY,
        RetrievalAlgorithm.N_HOP,
        RetrievalAlgorithm.HYBRID
    ]
    
    for algorithm in algorithms:
        print(f"\nğŸ¯ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…: {algorithm.value}")
        
        service.config.retrieval_algorithm = algorithm
        service.config.max_nodes = 10
        service.config.max_depth = 3
        
        try:
            start_time = time.time()
            result = service.process_query(test_query, start_nodes)
            end_time = time.time()
            
            print(f"  â±ï¸ Ø²Ù…Ø§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´: {end_time - start_time:.2f} Ø«Ø§Ù†ÛŒÙ‡")
            print(f"  ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ù†ÙˆØ¯Ù‡Ø§: {len(result.get('nodes', []))}")
            print(f"  ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ ÛŒØ§Ù„â€ŒÙ‡Ø§: {len(result.get('edges', []))}")
            
            if 'query_analysis' in result:
                analysis = result['query_analysis']
                print(f"  ğŸ” ØªØ­Ù„ÛŒÙ„ Ø³ÙˆØ§Ù„:")
                print(f"    Ù†ÙˆØ¹ Ù¾Ø§Ø³Ø®: {analysis.get('answer_types', [])}")
                print(f"    Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§: {analysis.get('entities', [])}")
                print(f"    Ù†ÙˆØ¯Ù‡Ø§ÛŒ Ø´Ø±ÙˆØ¹: {analysis.get('start_nodes', [])}")
            
        except Exception as e:
            print(f"  âŒ Ø®Ø·Ø§: {e}")

def test_queries(service):
    """ØªØ³Øª Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø®ØªÙ„Ù"""
    
    if not service.G:
        print("âš ï¸ Ú¯Ø±Ø§Ù Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø´Ø¯Ù‡ØŒ ØªØ³Øª Ø³ÙˆØ§Ù„Ø§Øª Ø§Ù†Ø¬Ø§Ù… Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯")
        return
    
    test_cases = [
        {
            "query": "What genes are associated with diabetes?",
            "description": "Ø¬Ø³ØªØ¬ÙˆÛŒ Ú˜Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø¯ÛŒØ§Ø¨Øª",
            "expected_entities": ["DIABETES", "GENE"]
        },
        {
            "query": "How does TP53 relate to cancer?",
            "description": "ØªØ­Ù„ÛŒÙ„ Ø±Ø§Ø¨Ø·Ù‡ TP53 Ø¨Ø§ Ø³Ø±Ø·Ø§Ù†",
            "expected_entities": ["TP53", "CANCER"]
        },
        {
            "query": "What drugs treat heart disease?",
            "description": "Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¯Ø§Ø±ÙˆÙ‡Ø§ÛŒ Ø¯Ø±Ù…Ø§Ù† Ø¨ÛŒÙ…Ø§Ø±ÛŒ Ù‚Ù„Ø¨ÛŒ",
            "expected_entities": ["DRUG", "HEART", "DISEASE"]
        },
        {
            "query": "Which pathways are involved in metabolism?",
            "description": "ØªØ­Ù„ÛŒÙ„ Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ù…ØªØ§Ø¨ÙˆÙ„ÛŒØ³Ù…",
            "expected_entities": ["PATHWAY", "METABOLISM"]
        },
        {
            "query": "What are the side effects of aspirin?",
            "description": "Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¹ÙˆØ§Ø±Ø¶ Ø¬Ø§Ù†Ø¨ÛŒ Ø¢Ø³Ù¾Ø±ÛŒÙ†",
            "expected_entities": ["ASPIRIN", "SIDE_EFFECT"]
        }
    ]
    
    # ØªÙ†Ø¸ÛŒÙ… Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… ØªØ±Ú©ÛŒØ¨ÛŒ
    service.config.retrieval_algorithm = RetrievalAlgorithm.HYBRID
    service.config.token_extraction_method = TokenExtractionMethod.HYBRID
    service.config.max_nodes = 15
    service.config.max_depth = 3
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nğŸ§ª ØªØ³Øª {i}: {test_case['description']}")
        print(f"â“ Ø³ÙˆØ§Ù„: {test_case['query']}")
        
        try:
            start_time = time.time()
            result = service.process_query(test_case['query'])
            end_time = time.time()
            
            print(f"  â±ï¸ Ø²Ù…Ø§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´: {end_time - start_time:.2f} Ø«Ø§Ù†ÛŒÙ‡")
            
            if 'query_analysis' in result:
                analysis = result['query_analysis']
                print(f"  ğŸ” ØªØ­Ù„ÛŒÙ„ Ø³ÙˆØ§Ù„:")
                print(f"    Ù†ÙˆØ¹ Ù¾Ø§Ø³Ø®: {analysis.get('answer_types', [])}")
                print(f"    Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§: {analysis.get('entities', [])}")
                print(f"    Ù†ÙˆØ¯Ù‡Ø§ÛŒ Ø´Ø±ÙˆØ¹: {analysis.get('start_nodes', [])}")
            
            print(f"  ğŸ“Š Ù†ØªØ§ÛŒØ¬:")
            print(f"    Ù†ÙˆØ¯Ù‡Ø§: {len(result.get('nodes', []))}")
            print(f"    ÛŒØ§Ù„â€ŒÙ‡Ø§: {len(result.get('edges', []))}")
            
            if 'communities' in result and result['communities']:
                print(f"    Ø¬Ø§Ù…Ø¹Ù‡â€ŒÙ‡Ø§: {len(result['communities'])}")
            
            if 'similarities' in result and result['similarities']:
                print(f"    Ø´Ø¨Ø§Ù‡Øªâ€ŒÙ‡Ø§: {len(result['similarities'])}")
            
            if 'paths' in result and result['paths']:
                print(f"    Ù…Ø³ÛŒØ±Ù‡Ø§: {len(result['paths'])}")
            
        except Exception as e:
            print(f"  âŒ Ø®Ø·Ø§: {e}")

def test_performance():
    """ØªØ³Øª Ø¹Ù…Ù„Ú©Ø±Ø¯"""
    
    print("\nâš¡ ØªØ³Øª Ø¹Ù…Ù„Ú©Ø±Ø¯...")
    
    # ØªØ³Øª Ø³Ø±Ø¹Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆÚ©Ù†
    service = EnhancedGraphRAGService()
    
    test_queries = [
        "What genes are associated with diabetes?",
        "How does TP53 relate to cancer?",
        "What drugs treat heart disease?",
        "Which pathways are involved in metabolism?",
        "What are the side effects of aspirin?"
    ]
    
    methods = [
        TokenExtractionMethod.LLM_BASED,
        TokenExtractionMethod.RULE_BASED,
        TokenExtractionMethod.HYBRID,
        TokenExtractionMethod.SEMANTIC
    ]
    
    performance_results = {}
    
    for method in methods:
        service.config.token_extraction_method = method
        method_times = []
        
        for query in test_queries:
            try:
                start_time = time.time()
                service.extract_tokens(query)
                end_time = time.time()
                method_times.append(end_time - start_time)
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± {method.value}: {e}")
        
        if method_times:
            avg_time = sum(method_times) / len(method_times)
            performance_results[method.value] = {
                'average_time': avg_time,
                'total_time': sum(method_times),
                'count': len(method_times)
            }
    
    print("ğŸ“Š Ù†ØªØ§ÛŒØ¬ Ø¹Ù…Ù„Ú©Ø±Ø¯:")
    for method, results in performance_results.items():
        print(f"  {method}:")
        print(f"    Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø²Ù…Ø§Ù†: {results['average_time']:.4f} Ø«Ø§Ù†ÛŒÙ‡")
        print(f"    Ú©Ù„ Ø²Ù…Ø§Ù†: {results['total_time']:.4f} Ø«Ø§Ù†ÛŒÙ‡")
        print(f"    ØªØ¹Ø¯Ø§Ø¯ ØªØ³Øª: {results['count']}")

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    
    print("ğŸš€ Ø´Ø±ÙˆØ¹ ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø³Ø±ÙˆÛŒØ³ Ù¾ÛŒØ´Ø±ÙØªÙ‡ GraphRAG")
    print(f"ğŸ“… ØªØ§Ø±ÛŒØ®: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    try:
        # ØªØ³Øª Ø§ØµÙ„ÛŒ
        test_enhanced_service()
        
        # ØªØ³Øª Ø¹Ù…Ù„Ú©Ø±Ø¯
        test_performance()
        
        print("\nğŸ‰ ØªÙ…Ø§Ù… ØªØ³Øªâ€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯!")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ØªØ³Øª ØªÙˆØ³Ø· Ú©Ø§Ø±Ø¨Ø± Ù…ØªÙˆÙ‚Ù Ø´Ø¯")
    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 
# -*- coding: utf-8 -*-
"""
GraphRAG Web Application - Ø±Ø§Ø¨Ø· ÙˆØ¨ ØªØ¹Ø§Ù…Ù„ÛŒ
"""

from flask import Flask, render_template, request, jsonify, send_from_directory, redirect, url_for
from graphrag_service import GraphRAGService, RetrievalMethod, GenerationModel
from enhanced_graphrag_service import EnhancedGraphRAGService, TokenExtractionMethod, RetrievalAlgorithm, CommunityDetectionMethod
import json
import os
import shutil
from datetime import datetime
from werkzeug.utils import secure_filename
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import re
from difflib import SequenceMatcher

# OpenAI import for GPT-4o comparison
try:
    import openai
except ImportError:
    openai = None

# Simple text processing functions without external dependencies
def simple_tokenize(text):
    """Tokenize text without external dependencies"""
    return text.lower().split()

def simple_remove_punctuation(text):
    """Remove punctuation without external dependencies"""
    import string
    return text.translate(str.maketrans('', '', string.punctuation))

# Initialize sentence transformer for semantic similarity (optional)
sentence_transformer = None
try:
    from sentence_transformers import SentenceTransformer
    sentence_transformer = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
except:
    pass

app = Flask(__name__)

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„
UPLOAD_FOLDER = 'uploaded_graphs'
ALLOWED_EXTENSIONS = {'pkl', 'sif', 'tsv', 'csv', 'txt', 'gz'}
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER

# Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ Ø¢Ù¾Ù„ÙˆØ¯ Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

def allowed_file(filename):
    """Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¬Ø§Ø² Ø¨ÙˆØ¯Ù† Ù†ÙˆØ¹ ÙØ§ÛŒÙ„"""
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

# Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³Ø±ÙˆÛŒØ³ GraphRAG Ø¨Ø§ Ú¯Ø±Ø§Ù Hetionet
# Ø§Ø¨ØªØ¯Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú©Ù‡ Ø¢ÛŒØ§ ÙØ§ÛŒÙ„ Ú¯Ø±Ø§Ù Hetionet ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
graph_files = [f for f in os.listdir('.') if f.startswith('hetionet_graph_') and f.endswith('.pkl')]
if graph_files:
    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† ÙØ§ÛŒÙ„ Ú¯Ø±Ø§Ù
    latest_graph_file = max(graph_files)
    print(f"ğŸ”§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú¯Ø±Ø§Ù Hetionet: {latest_graph_file}")
    graphrag_service = GraphRAGService(graph_data_path=latest_graph_file)
    enhanced_graphrag_service = EnhancedGraphRAGService(graph_data_path=latest_graph_file)
else:
    print("âš ï¸ ÙØ§ÛŒÙ„ Ú¯Ø±Ø§Ù Hetionet ÛŒØ§ÙØª Ù†Ø´Ø¯ØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú¯Ø±Ø§Ù Ù†Ù…ÙˆÙ†Ù‡")
    graphrag_service = GraphRAGService()
    enhanced_graphrag_service = EnhancedGraphRAGService()

# ØªÙ†Ø¸ÛŒÙ… API Key Ù‡Ø§ÛŒ OpenAI
OPENAI_API_KEY = "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"
graphrag_service.set_openai_api_key(OPENAI_API_KEY)
print("âœ… OpenAI API Key ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯")

@app.route('/')
def index():
    """ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ"""
    return render_template('index.html')

@app.route('/upload_graph')
def upload_graph_page():
    """ØµÙØ­Ù‡ Ø¢Ù¾Ù„ÙˆØ¯ Ú¯Ø±Ø§Ù"""
    return render_template('upload_graph.html')

@app.route('/manage_graphs')
def manage_graphs_page():
    """ØµÙØ­Ù‡ Ù…Ø¯ÛŒØ±ÛŒØª Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§"""
    return render_template('manage_graphs.html')

@app.route('/evaluation')
def evaluation():
    return render_template('evaluation.html')

@app.route('/api/upload_graph', methods=['POST'])
def upload_graph():
    """Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ Ú¯Ø±Ø§Ù"""
    try:
        if 'graph_file' not in request.files:
            return jsonify({
                'success': False,
                'error': 'ÙØ§ÛŒÙ„ Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª'
            }), 400
        
        file = request.files['graph_file']
        if file.filename == '':
            return jsonify({
                'success': False,
                'error': 'ÙØ§ÛŒÙ„ Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª'
            }), 400
        
        if file and allowed_file(file.filename):
            filename = secure_filename(file.filename)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename_with_timestamp = f"{timestamp}_{filename}"
            filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename_with_timestamp)
            
            file.save(filepath)
            
            # Ø§Ú¯Ø± ÙØ§ÛŒÙ„ ÙØ´Ø±Ø¯Ù‡ Ø§Ø³ØªØŒ Ø¢Ù† Ø±Ø§ Ø¨Ø§Ø² Ú©Ù†
            if filename.endswith('.gz'):
                import gzip
                with gzip.open(filepath, 'rb') as f_in:
                    uncompressed_path = filepath[:-3]
                    with open(uncompressed_path, 'wb') as f_out:
                        shutil.copyfileobj(f_in, f_out)
                os.remove(filepath)
                filepath = uncompressed_path
                filename_with_timestamp = filename_with_timestamp[:-3]
            
            return jsonify({
                'success': True,
                'message': f'ÙØ§ÛŒÙ„ {filename} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯',
                'filename': filename_with_timestamp,
                'filepath': filepath
            })
        else:
            return jsonify({
                'success': False,
                'error': 'Ù†ÙˆØ¹ ÙØ§ÛŒÙ„ Ù…Ø¬Ø§Ø² Ù†ÛŒØ³Øª. ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ø¬Ø§Ø²: pkl, sif, tsv, csv, txt, gz'
            }), 400
    
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/list_graphs')
def list_graphs():
    """Ù„ÛŒØ³Øª Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯"""
    try:
        graphs = []
        
        # Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒ Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯Ù‡
        if os.path.exists(UPLOAD_FOLDER):
            for filename in os.listdir(UPLOAD_FOLDER):
                filepath = os.path.join(UPLOAD_FOLDER, filename)
                if os.path.isfile(filepath):
                    file_size = os.path.getsize(filepath)
                    file_date = datetime.fromtimestamp(os.path.getctime(filepath))
                    graphs.append({
                        'name': filename,
                        'path': filepath,
                        'size': file_size,
                        'date': file_date.isoformat(),
                        'type': 'uploaded'
                    })
        
        # Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù¾ÙˆØ´Ù‡ Ø§ØµÙ„ÛŒ
        for filename in os.listdir('.'):
            if filename.endswith('.pkl') and filename.startswith('hetionet_graph_'):
                filepath = os.path.join('.', filename)
                file_size = os.path.getsize(filepath)
                file_date = datetime.fromtimestamp(os.path.getctime(filepath))
                graphs.append({
                    'name': filename,
                    'path': filepath,
                    'size': file_size,
                    'date': file_date.isoformat(),
                    'type': 'builtin'
                })
        
        # Ù…Ø±ØªØ¨ Ú©Ø±Ø¯Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ§Ø±ÛŒØ® (Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ø§ÙˆÙ„)
        graphs.sort(key=lambda x: x['date'], reverse=True)
        
        return jsonify({
            'success': True,
            'graphs': graphs
        })
    
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/load_graph', methods=['POST'])
def load_graph():
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú¯Ø±Ø§Ù Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡"""
    try:
        data = request.get_json()
        graph_path = data.get('graph_path')
        
        if not graph_path or not os.path.exists(graph_path):
            return jsonify({
                'success': False,
                'error': 'Ù…Ø³ÛŒØ± Ú¯Ø±Ø§Ù Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª'
            }), 400
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú¯Ø±Ø§Ù Ø¬Ø¯ÛŒØ¯
        global graphrag_service
        graphrag_service = GraphRAGService(graph_data_path=graph_path)
        graphrag_service.set_openai_api_key(OPENAI_API_KEY)
        
        return jsonify({
            'success': True,
            'message': f'Ú¯Ø±Ø§Ù {os.path.basename(graph_path)} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯'
        })
    
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/delete_graph', methods=['POST'])
def delete_graph():
    """Ø­Ø°Ù Ú¯Ø±Ø§Ù"""
    try:
        data = request.get_json()
        graph_path = data.get('graph_path')
        
        if not graph_path or not os.path.exists(graph_path):
            return jsonify({
                'success': False,
                'error': 'Ù…Ø³ÛŒØ± Ú¯Ø±Ø§Ù Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª'
            }), 400
        
        # Ø­Ø°Ù ÙØ§ÛŒÙ„
        os.remove(graph_path)
        
        return jsonify({
            'success': True,
            'message': f'Ú¯Ø±Ø§Ù {os.path.basename(graph_path)} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø­Ø°Ù Ø´Ø¯'
        })
    
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/process_query', methods=['POST'])
def process_query():
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³ÙˆØ§Ù„ Ùˆ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ù†ØªÛŒØ¬Ù‡"""
    try:
        data = request.get_json()
        query = data.get('query', '')
        retrieval_method = data.get('retrieval_method', 'BFS')
        generation_model = data.get('generation_model', 'GPT_SIMULATION')
        text_generation_type = data.get('text_generation_type', 'INTELLIGENT')
        max_depth = data.get('max_depth', 2)
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø±Ø´ØªÙ‡ Ø¨Ù‡ enum
        retrieval_enum = RetrievalMethod[retrieval_method]
        generation_enum = GenerationModel[generation_model.replace(' ', '_')]
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³ÙˆØ§Ù„
        result = graphrag_service.process_query(
            query=query,
            retrieval_method=retrieval_enum,
            generation_model=generation_enum,
            text_generation_type=text_generation_type,
            max_depth=max_depth
        )
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† timestamp
        result['timestamp'] = datetime.now().isoformat()
        
        return jsonify({
            'success': True,
            'result': result
        })
    
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/enhanced_process_query', methods=['POST'])
def enhanced_process_query():
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³ÙˆØ§Ù„ Ø¨Ø§ Ø³Ø±ÙˆÛŒØ³ Ù¾ÛŒØ´Ø±ÙØªÙ‡ GraphRAG"""
    try:
        data = request.get_json()
        query = data.get('query', '')
        token_extraction_method = data.get('token_extraction_method', 'llm_based')
        retrieval_algorithm = data.get('retrieval_algorithm', 'hybrid')
        community_detection_method = data.get('community_detection_method', 'louvain')
        max_depth = data.get('max_depth', 3)
        max_nodes = data.get('max_nodes', 20)
        max_edges = data.get('max_edges', 40)
        similarity_threshold = data.get('similarity_threshold', 0.3)
        
        # ØªÙ†Ø¸ÛŒÙ… Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ø³Ø±ÙˆÛŒØ³ Ù¾ÛŒØ´Ø±ÙØªÙ‡
        enhanced_graphrag_service.set_config(
            token_extraction_method=token_extraction_method,
            retrieval_algorithm=retrieval_algorithm,
            community_detection_method=community_detection_method,
            max_depth=max_depth,
            max_nodes=max_nodes,
            max_edges=max_edges,
            similarity_threshold=similarity_threshold
        )
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³ÙˆØ§Ù„
        result = enhanced_graphrag_service.process_query(query)
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† timestamp
        result['timestamp'] = datetime.now().isoformat()
        result['config'] = enhanced_graphrag_service.get_config()
        
        return jsonify({
            'success': True,
            'result': result
        })
    
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/graph_info')
def graph_info():
    """Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú¯Ø±Ø§Ù"""
    try:
        G = graphrag_service.G
        if G:
            node_types = {}
            for node, attrs in G.nodes(data=True):
                kind = attrs.get('kind', 'Unknown')
                node_types[kind] = node_types.get(kind, 0) + 1
            
            return jsonify({
                'success': True,
                'total_nodes': G.number_of_nodes(),
                'total_edges': G.number_of_edges(),
                'node_types': node_types
            })
        else:
            return jsonify({
                'success': False,
                'error': 'Graph not loaded'
            })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/enhanced_graph_info')
def enhanced_graph_info():
    """Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú¯Ø±Ø§Ù Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
    try:
        stats = enhanced_graphrag_service.get_graph_statistics()
        if stats:
            return jsonify({
                'success': True,
                'statistics': stats
            })
        else:
            return jsonify({
                'success': False,
                'error': 'Enhanced graph not loaded'
            })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/token_extraction_methods')
def token_extraction_methods():
    """Ø¯Ø±ÛŒØ§ÙØª Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆÚ©Ù†"""
    methods = [
        {
            'value': 'llm_based',
            'label': 'Ø¨Ø± Ø§Ø³Ø§Ø³ LLM',
            'description': 'Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆÚ©Ù† Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø²Ø¨Ø§Ù†ÛŒ'
        },
        {
            'value': 'rule_based',
            'label': 'Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‚ÙˆØ§Ù†ÛŒÙ†',
            'description': 'Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆÚ©Ù† Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‚ÙˆØ§Ù†ÛŒÙ† Ø§Ø² Ù¾ÛŒØ´ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡'
        },
        {
            'value': 'hybrid',
            'label': 'ØªØ±Ú©ÛŒØ¨ÛŒ',
            'description': 'ØªØ±Ú©ÛŒØ¨ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ LLM Ùˆ Ù‚ÙˆØ§Ù†ÛŒÙ†'
        },
        {
            'value': 'semantic',
            'label': 'Ù…Ø¹Ù†Ø§ÛŒÛŒ',
            'description': 'Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆÚ©Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´Ø¨Ø§Ù‡Øª Ù…Ø¹Ù†Ø§ÛŒÛŒ'
        }
    ]
    return jsonify({'methods': methods})

@app.route('/api/retrieval_algorithms')
def retrieval_algorithms():
    """Ø¯Ø±ÛŒØ§ÙØª Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ"""
    algorithms = [
        {
            'value': 'bfs',
            'label': 'BFS (Ø¬Ø³ØªØ¬ÙˆÛŒ Ø³Ø·Ø­ Ø§ÙˆÙ„)',
            'description': 'Ø¬Ø³ØªØ¬ÙˆÛŒ Ø³Ø·Ø­ Ø§ÙˆÙ„ Ø¯Ø± Ú¯Ø±Ø§Ù'
        },
        {
            'value': 'dfs',
            'label': 'DFS (Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¹Ù…ÛŒÙ‚ Ø§ÙˆÙ„)',
            'description': 'Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¹Ù…ÛŒÙ‚ Ø§ÙˆÙ„ Ø¯Ø± Ú¯Ø±Ø§Ù'
        },
        {
            'value': 'pagerank',
            'label': 'PageRank',
            'description': 'Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… PageRank'
        },
        {
            'value': 'community_detection',
            'label': 'ØªØ´Ø®ÛŒØµ Ø¬Ø§Ù…Ø¹Ù‡',
            'description': 'Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ´Ø®ÛŒØµ Ø¬Ø§Ù…Ø¹Ù‡â€ŒÙ‡Ø§'
        },
        {
            'value': 'semantic_similarity',
            'label': 'Ø´Ø¨Ø§Ù‡Øª Ù…Ø¹Ù†Ø§ÛŒÛŒ',
            'description': 'Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´Ø¨Ø§Ù‡Øª Ù…Ø¹Ù†Ø§ÛŒÛŒ'
        },
        {
            'value': 'n_hop',
            'label': 'N-Hop',
            'description': 'Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ N-Hop'
        },
        {
            'value': 'hybrid',
            'label': 'ØªØ±Ú©ÛŒØ¨ÛŒ',
            'description': 'ØªØ±Ú©ÛŒØ¨ Ú†Ù†Ø¯ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…'
        }
    ]
    return jsonify({'algorithms': algorithms})

@app.route('/api/community_detection_methods')
def community_detection_methods():
    """Ø¯Ø±ÛŒØ§ÙØª Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø¬Ø§Ù…Ø¹Ù‡"""
    methods = [
        {
            'value': 'louvain',
            'label': 'Louvain',
            'description': 'Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Louvain Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø¬Ø§Ù…Ø¹Ù‡'
        },
        {
            'value': 'label_propagation',
            'label': 'Label Propagation',
            'description': 'Ø§Ù†ØªØ´Ø§Ø± Ø¨Ø±Ú†Ø³Ø¨ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø¬Ø§Ù…Ø¹Ù‡'
        },
        {
            'value': 'girvan_newman',
            'label': 'Girvan-Newman',
            'description': 'Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Girvan-Newman'
        },
        {
            'value': 'spectral',
            'label': 'Spectral',
            'description': 'Ø±ÙˆØ´ Ø·ÛŒÙÛŒ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø¬Ø§Ù…Ø¹Ù‡'
        }
    ]
    return jsonify({'methods': methods})

@app.route('/api/sample_queries')
def sample_queries():
    """Ø³ÙˆØ§Ù„Ø§Øª Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø§Ø®ØªØ§Ø± Hetionet Ùˆ Ø±ÙˆØ§Ø¨Ø· Ù…ÙˆØ¬ÙˆØ¯"""
    samples = [
        # Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¨ÛŒØ§Ù† Ú˜Ù† Ø¯Ø± Ø¨Ø§ÙØªâ€ŒÙ‡Ø§ (AeG, AuG, AdG)
        "What genes are expressed in the heart?",
        "Which genes are upregulated in the brain?",
        "What genes are downregulated in muscle tissue?",
        "How do genes express differently in liver vs kidney?",
        "What genes are expressed in the lung?",
        
        # Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ú˜Ù†â€ŒÙ‡Ø§ Ùˆ Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ (DaG, DuG, DdG)
        "What genes are associated with diabetes?",
        "How does TP53 relate to cancer?",
        "Which genes are upregulated in cancer?",
        "What genes are downregulated in heart disease?",
        "What genes are associated with Alzheimer's disease?",
        
        # Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¯Ø§Ø±ÙˆÙ‡Ø§ Ùˆ Ø¯Ø±Ù…Ø§Ù† (CtD, CuG, CdG)
        "What drugs treat diabetes?",
        "Which compounds upregulate TP53?",
        "What drugs downregulate cancer genes?",
        "How do drugs interact with genes?",
        "What compounds bind to insulin receptor?",
        
        # Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ ÙØ±Ø¢ÛŒÙ†Ø¯Ù‡Ø§ÛŒ Ø²ÛŒØ³ØªÛŒ (GpBP, GpMF, GpCC)
        "What genes participate in cell cycle regulation?",
        "Which genes are involved in apoptosis?",
        "What molecular functions does TP53 have?",
        "How do genes function in cellular components?",
        "What genes participate in DNA repair?",
        
        # Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø²ÛŒØ³ØªÛŒ (GpPW)
        "What pathways are involved in cancer progression?",
        "Which signaling pathways regulate metabolism?",
        "How do genes participate in immune pathways?",
        "What pathways control cell growth?",
        "Which pathways involve insulin signaling?",
        
        # Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ ØªØ¹Ø§Ù…Ù„ Ú˜Ù†â€ŒÙ‡Ø§ (GiG, Gr>G)
        "How do genes interact with each other?",
        "Which genes regulate TP53?",
        "What genes are regulated by TP53?",
        "How do genes covary in expression?",
        "What genes interact with BRCA1?",
        
        # Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ Ùˆ Ø¹Ù„Ø§Ø¦Ù… (DpS, DlA)
        "What symptoms are associated with diabetes?",
        "How does cancer affect different tissues?",
        "What diseases affect the heart?",
        "Which diseases localize to specific tissues?",
        "What diseases present similar symptoms?",
        
        # Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¹ÙˆØ§Ø±Ø¶ Ø¬Ø§Ù†Ø¨ÛŒ Ø¯Ø§Ø±ÙˆÙ‡Ø§ (CcSE)
        "What side effects does aspirin cause?",
        "How do drugs affect patient symptoms?",
        "What adverse reactions occur with diabetes drugs?",
        "Which compounds cause heart-related side effects?",
        "What side effects do cancer drugs cause?",
        
        # Ø³ÙˆØ§Ù„Ø§Øª Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ùˆ Ú†Ù†Ø¯Ù…Ø±Ø­Ù„Ù‡â€ŒØ§ÛŒ
        "How do drugs affect gene expression in heart tissue?",
        "What genes and pathways are involved in diabetes progression?",
        "How do genetic mutations lead to cancer development?",
        "What therapeutic targets exist for heart disease?",
        "How do genes participate in drug metabolism pathways?",
        "What biological processes are disrupted in cancer?",
        "What drugs treat diseases that affect the heart?",
        "How do genes that interact with TP53 relate to cancer?",
        "What compounds bind to genes expressed in the brain?",
        "Which diseases have symptoms related to diabetes?"
    ]
    return jsonify({'queries': samples})

@app.route('/api/config', methods=['GET', 'POST'])
def config_endpoint():
    """Ù…Ø¯ÛŒØ±ÛŒØª ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø³ÛŒØ³ØªÙ…"""
    if request.method == 'GET':
        # Ø¯Ø±ÛŒØ§ÙØª ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙØ¹Ù„ÛŒ
        try:
            config = graphrag_service.get_config()
            return jsonify({
                'success': True,
                'config': config
            })
        except Exception as e:
            return jsonify({
                'success': False,
                'error': str(e)
            }), 500
    
    elif request.method == 'POST':
        # ØªØºÛŒÛŒØ± ØªÙ†Ø¸ÛŒÙ…Ø§Øª
        try:
            data = request.get_json()
            new_config = data.get('config', {})
            
            # Ø§Ø¹Ù…Ø§Ù„ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¬Ø¯ÛŒØ¯
            graphrag_service.set_config(**new_config)
            
            # Ø¯Ø±ÛŒØ§ÙØª ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡â€ŒØ±ÙˆØ² Ø´Ø¯Ù‡
            updated_config = graphrag_service.get_config()
            
            return jsonify({
                'success': True,
                'message': 'ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡â€ŒØ±ÙˆØ² Ø´Ø¯',
                'config': updated_config
            })
        except Exception as e:
            return jsonify({
                'success': False,
                'error': str(e)
            }), 500

@app.route('/api/enhanced_config', methods=['GET', 'POST'])
def enhanced_config_endpoint():
    """Ù…Ø¯ÛŒØ±ÛŒØª ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø³Ø±ÙˆÛŒØ³ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
    if request.method == 'GET':
        # Ø¯Ø±ÛŒØ§ÙØª ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙØ¹Ù„ÛŒ
        try:
            config = enhanced_graphrag_service.get_config()
            return jsonify({
                'success': True,
                'config': config
            })
        except Exception as e:
            return jsonify({
                'success': False,
                'error': str(e)
            }), 500
    
    elif request.method == 'POST':
        # ØªØºÛŒÛŒØ± ØªÙ†Ø¸ÛŒÙ…Ø§Øª
        try:
            data = request.get_json()
            new_config = data.get('config', {})
            
            # Ø§Ø¹Ù…Ø§Ù„ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¬Ø¯ÛŒØ¯
            enhanced_graphrag_service.set_config(**new_config)
            
            # Ø¯Ø±ÛŒØ§ÙØª ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡â€ŒØ±ÙˆØ² Ø´Ø¯Ù‡
            updated_config = enhanced_graphrag_service.get_config()
            
            return jsonify({
                'success': True,
                'message': 'ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡â€ŒØ±ÙˆØ² Ø´Ø¯',
                'config': updated_config
            })
        except Exception as e:
            return jsonify({
                'success': False,
                'error': str(e)
            }), 500

@app.route('/api/config/presets', methods=['GET'])
def config_presets():
    """Ù¾ÛŒØ´â€ŒØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù…Ø§Ø¯Ù‡"""
    presets = {
        'fast': {
            'name': 'Ø³Ø±ÛŒØ¹',
            'description': 'Ù¾Ø§Ø³Ø® Ø³Ø±ÛŒØ¹ Ø¨Ø§ Ù…Ø­Ø¯ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ú©Ù…',
            'config': {
                'max_nodes': 5,
                'max_edges': 10,
                'max_depth': 2,
                'max_paths': 3,
                'max_context_length': 1000,
                'max_answer_tokens': 500,
                'max_prompt_tokens': 2000,
                'enable_verbose_logging': False,
                'enable_biological_enrichment': False,
                'enable_smart_filtering': True
            }
        },
        'balanced': {
            'name': 'Ù…ØªÙˆØ§Ø²Ù†',
            'description': 'ØªØ¹Ø§Ø¯Ù„ Ø¨ÛŒÙ† Ø³Ø±Ø¹Øª Ùˆ Ú©ÛŒÙÛŒØª',
            'config': {
                'max_nodes': 10,
                'max_edges': 20,
                'max_depth': 3,
                'max_paths': 5,
                'max_context_length': 2000,
                'max_answer_tokens': 1000,
                'max_prompt_tokens': 4000,
                'enable_verbose_logging': True,
                'enable_biological_enrichment': True,
                'enable_smart_filtering': True
            }
        },
        'comprehensive': {
            'name': 'Ø¬Ø§Ù…Ø¹',
            'description': 'Ù¾Ø§Ø³Ø® Ú©Ø§Ù…Ù„ Ø¨Ø§ Ø¬Ø²Ø¦ÛŒØ§Øª Ø¨ÛŒØ´ØªØ±',
            'config': {
                'max_nodes': 20,
                'max_edges': 40,
                'max_depth': 4,
                'max_paths': 10,
                'max_context_length': 3000,
                'max_answer_tokens': 1500,
                'max_prompt_tokens': 6000,
                'enable_verbose_logging': True,
                'enable_biological_enrichment': True,
                'enable_smart_filtering': True
            }
        },
        'research': {
            'name': 'ØªØ­Ù‚ÛŒÙ‚Ø§ØªÛŒ',
            'description': 'Ø¨Ø±Ø§ÛŒ ØªØ­Ù‚ÛŒÙ‚Ø§Øª Ùˆ ØªØ­Ù„ÛŒÙ„ Ø¹Ù…ÛŒÙ‚',
            'config': {
                'max_nodes': 30,
                'max_edges': 60,
                'max_depth': 5,
                'max_paths': 15,
                'max_context_length': 4000,
                'max_answer_tokens': 2000,
                'max_prompt_tokens': 8000,
                'enable_verbose_logging': True,
                'enable_biological_enrichment': True,
                'enable_smart_filtering': True
            }
        }
    }
    return jsonify({'presets': presets})

@app.route('/api/compare_texts', methods=['POST'])
def compare_texts():
    try:
        data = request.get_json()
        text1 = data.get('text1', '')
        text2 = data.get('text2', '')
        method = data.get('method', 'cosine_tfidf')
        
        if not text1 or not text2:
            return jsonify({'error': 'Ù‡Ø± Ø¯Ùˆ Ù…ØªÙ† Ø¨Ø§ÛŒØ¯ ÙˆØ§Ø±Ø¯ Ø´ÙˆÙ†Ø¯'}), 400
        
        # Preprocess texts
        text1_processed = preprocess_text(text1)
        text2_processed = preprocess_text(text2)
        
        # Calculate similarity based on selected method
        similarity_score = 0
        method_name = ""
        
        if method == 'cosine_tfidf':
            similarity_score = cosine_similarity_tfidf(text1_processed, text2_processed)
            method_name = "Ø´Ø¨Ø§Ù‡Øª Ú©Ø³ÛŒÙ†ÙˆØ³ÛŒ (TF-IDF)"
        elif method == 'cosine_sbert':
            similarity_score = cosine_similarity_sbert(text1, text2)
            method_name = "Ø´Ø¨Ø§Ù‡Øª Ú©Ø³ÛŒÙ†ÙˆØ³ÛŒ (SBERT)"
        elif method == 'jaccard':
            similarity_score = jaccard_similarity(text1_processed, text2_processed)
            method_name = "Ø´Ø¨Ø§Ù‡Øª Ø¬Ø§Ú©Ø§Ø±Ø¯"
        elif method == 'levenshtein':
            similarity_score = levenshtein_similarity(text1, text2)
            method_name = "Ø´Ø¨Ø§Ù‡Øª Ù„ÙˆÙ†Ø´ØªØ§ÛŒÙ†"
        elif method == 'sequence_matcher':
            similarity_score = sequence_matcher_similarity(text1, text2)
            method_name = "Ø´Ø¨Ø§Ù‡Øª Sequence Matcher"
        elif method == 'word_overlap':
            similarity_score = word_overlap_similarity(text1_processed, text2_processed)
            method_name = "Ø´Ø¨Ø§Ù‡Øª Ù‡Ù…Ù¾ÙˆØ´Ø§Ù†ÛŒ Ú©Ù„Ù…Ø§Øª"
        else:
            return jsonify({'error': 'Ø±ÙˆØ´ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª'}), 400
        
        # Determine quality level
        quality_level = get_quality_level(similarity_score)
        
        return jsonify({
            'similarity_score': round(similarity_score, 4),
            'method_name': method_name,
            'quality_level': quality_level,
            'text1_processed': text1_processed,
            'text2_processed': text2_processed
        })
        
    except Exception as e:
        return jsonify({'error': f'Ø®Ø·Ø§ Ø¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡: {str(e)}'}), 500

@app.route('/api/compare_with_gpt', methods=['POST'])
def compare_with_gpt():
    try:
        data = request.get_json()
        text1 = data.get('text1', '')
        text2 = data.get('text2', '')
        label1 = data.get('label1', 'Ø±ÙˆØ´ Ø§ÙˆÙ„')
        label2 = data.get('label2', 'Ø±ÙˆØ´ Ø¯ÙˆÙ…')
        comparison_type = data.get('comparison_type', 'comprehensive')
        gpt_model = data.get('gpt_model', 'gpt-4o')
        
        if not text1 or not text2:
            return jsonify({'error': 'Ù‡Ø± Ø¯Ùˆ Ù…ØªÙ† Ø¨Ø§ÛŒØ¯ ÙˆØ§Ø±Ø¯ Ø´ÙˆÙ†Ø¯'}), 400
        
        # Create a comprehensive prompt for GPT
        prompt = create_gpt_comparison_prompt(text1, text2, label1, label2, comparison_type)
        
        # Check if OpenAI is available
        if openai is None:
            return jsonify({'error': 'OpenAI Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ù†ØµØ¨ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª. Ù„Ø·ÙØ§Ù‹ Ø¨Ø§ Ø¯Ø³ØªÙˆØ± pip install openai Ø¢Ù† Ø±Ø§ Ù†ØµØ¨ Ú©Ù†ÛŒØ¯.'}), 500
        
        # Call OpenAI API
        try:
            # Set the API key for this request
            openai.api_key = OPENAI_API_KEY
            
            response = openai.chat.completions.create(
                model=gpt_model,
                messages=[
                    {"role": "system", "content": "Ø´Ù…Ø§ ÛŒÚ© Ù…ØªØ®ØµØµ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª Ù…ØªÙ† Ù‡Ø³ØªÛŒØ¯. ÙˆØ¸ÛŒÙÙ‡ Ø´Ù…Ø§ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¯Ùˆ Ù…ØªÙ† Ùˆ Ø§Ø±Ø§Ø¦Ù‡ ØªØ­Ù„ÛŒÙ„ Ø¯Ù‚ÛŒÙ‚ Ùˆ Ù…Ù†ØµÙØ§Ù†Ù‡ Ø§Ø³Øª."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=2000
            )
            
            gpt_response = response.choices[0].message.content
            
            # Parse the GPT response
            parsed_result = parse_gpt_comparison_response(gpt_response, label1, label2, comparison_type)
            parsed_result['gpt_model'] = gpt_model
            
            return jsonify(parsed_result)
            
        except Exception as e:
            return jsonify({'error': f'Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ {gpt_model}: {str(e)}'}), 500
        
    except Exception as e:
        return jsonify({'error': f'Ø®Ø·Ø§ Ø¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡: {str(e)}'}), 500

def create_gpt_comparison_prompt(text1, text2, label1, label2, comparison_type):
    """Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø±Ø§Ù…Ù¾ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ GPT-4o"""
    
    comparison_focus = {
        'comprehensive': 'Ú©ÛŒÙÛŒØª Ú©Ù„ÛŒØŒ Ø¯Ù‚ØªØŒ Ø¬Ø§Ù…Ø¹ÛŒØªØŒ ÙˆØ¶ÙˆØ­ØŒ Ùˆ Ù…Ø±ØªØ¨Ø· Ø¨ÙˆØ¯Ù†',
        'accuracy': 'Ø¯Ù‚Øª Ùˆ ØµØ­Øª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡',
        'completeness': 'Ø¬Ø§Ù…Ø¹ÛŒØª Ùˆ Ú©Ø§Ù…Ù„ Ø¨ÙˆØ¯Ù† Ù¾Ø§Ø³Ø®',
        'clarity': 'ÙˆØ¶ÙˆØ­ Ùˆ Ù‚Ø§Ø¨Ù„ ÙÙ‡Ù… Ø¨ÙˆØ¯Ù† Ù…ØªÙ†',
        'relevance': 'Ù…Ø±ØªØ¨Ø· Ø¨ÙˆØ¯Ù† Ø¨Ø§ Ø³ÙˆØ§Ù„ Ø§ØµÙ„ÛŒ'
    }
    
    focus = comparison_focus.get(comparison_type, comparison_focus['comprehensive'])
    
    prompt = f"""
Ù„Ø·ÙØ§Ù‹ Ø¯Ùˆ Ù…ØªÙ† Ø²ÛŒØ± Ø±Ø§ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø¯Ù‚ÛŒÙ‚ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯:

**Ù…ØªÙ† Ø§ÙˆÙ„ ({label1}):**
{text1}

**Ù…ØªÙ† Ø¯ÙˆÙ… ({label2}):**
{text2}

**Ù†ÙˆØ¹ Ù…Ù‚Ø§ÛŒØ³Ù‡:** {focus}

Ù„Ø·ÙØ§Ù‹ ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ¯ Ø±Ø§ Ø¯Ø± Ù‚Ø§Ù„Ø¨ Ø²ÛŒØ± Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯:

**Ø®Ù„Ø§ØµÙ‡ Ù…Ù‚Ø§ÛŒØ³Ù‡:**
[ÛŒÚ© Ø®Ù„Ø§ØµÙ‡ Ú©ÙˆØªØ§Ù‡ Ø§Ø² ØªÙØ§ÙˆØªâ€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ]

**Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ (Ø§Ø² 1 ØªØ§ 10):**
{label1}: [Ø§Ù…ØªÛŒØ§Ø²]/10
{label2}: [Ø§Ù…ØªÛŒØ§Ø²]/10

**ØªÙˆØ¶ÛŒØ­ Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ:**
[ØªÙˆØ¶ÛŒØ­ Ø¯Ù„ÛŒÙ„ Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ]

**Ù†Ù‚Ø§Ø· Ù‚ÙˆØª {label1}:**
[Ù„ÛŒØ³Øª Ù†Ù‚Ø§Ø· Ù‚ÙˆØª]

**Ù†Ù‚Ø§Ø· Ù‚ÙˆØª {label2}:**
[Ù„ÛŒØ³Øª Ù†Ù‚Ø§Ø· Ù‚ÙˆØª]

**Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù {label1}:**
[Ù„ÛŒØ³Øª Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù]

**Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù {label2}:**
[Ù„ÛŒØ³Øª Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù]

**ØªÙˆØµÛŒÙ‡ Ù†Ù‡Ø§ÛŒÛŒ:**
[ØªÙˆØµÛŒÙ‡ Ú©Ø¯Ø§Ù… Ø±ÙˆØ´ Ø¨Ù‡ØªØ± Ø§Ø³Øª Ùˆ Ú†Ø±Ø§]

Ù„Ø·ÙØ§Ù‹ ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯ Ùˆ ØµØ§Ø¯Ù‚Ø§Ù†Ù‡ Ùˆ Ù…Ù†ØµÙØ§Ù†Ù‡ Ù‚Ø¶Ø§ÙˆØª Ú©Ù†ÛŒØ¯.
"""
    
    return prompt

def parse_gpt_comparison_response(response, label1, label2, comparison_type):
    """ØªØ¬Ø²ÛŒÙ‡ Ùˆ ØªØ­Ù„ÛŒÙ„ Ù¾Ø§Ø³Ø® GPT-4o"""
    
    # Extract scores using regex
    import re
    
    # Find scores
    score1_match = re.search(rf'{label1}:\s*(\d+)/10', response)
    score2_match = re.search(rf'{label2}:\s*(\d+)/10', response)
    
    score1 = int(score1_match.group(1)) if score1_match else 5
    score2 = int(score2_match.group(1)) if score2_match else 5
    
    # Split response into sections
    sections = response.split('\n\n')
    
    summary = ""
    scoring_explanation = ""
    strengths1 = ""
    strengths2 = ""
    weaknesses1 = ""
    weaknesses2 = ""
    recommendation = ""
    
    for section in sections:
        if "Ø®Ù„Ø§ØµÙ‡ Ù…Ù‚Ø§ÛŒØ³Ù‡" in section:
            summary = section.replace("**Ø®Ù„Ø§ØµÙ‡ Ù…Ù‚Ø§ÛŒØ³Ù‡:**", "").strip()
        elif "ØªÙˆØ¶ÛŒØ­ Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ" in section:
            scoring_explanation = section.replace("**ØªÙˆØ¶ÛŒØ­ Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ:**", "").strip()
        elif f"Ù†Ù‚Ø§Ø· Ù‚ÙˆØª {label1}" in section:
            strengths1 = section.replace(f"**Ù†Ù‚Ø§Ø· Ù‚ÙˆØª {label1}:**", "").strip()
        elif f"Ù†Ù‚Ø§Ø· Ù‚ÙˆØª {label2}" in section:
            strengths2 = section.replace(f"**Ù†Ù‚Ø§Ø· Ù‚ÙˆØª {label2}:**", "").strip()
        elif f"Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù {label1}" in section:
            weaknesses1 = section.replace(f"**Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù {label1}:**", "").strip()
        elif f"Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù {label2}" in section:
            weaknesses2 = section.replace(f"**Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù {label2}:**", "").strip()
        elif "ØªÙˆØµÛŒÙ‡ Ù†Ù‡Ø§ÛŒÛŒ" in section:
            recommendation = section.replace("**ØªÙˆØµÛŒÙ‡ Ù†Ù‡Ø§ÛŒÛŒ:**", "").strip()
    
    # If sections are empty, use the full response
    if not summary:
        summary = response[:200] + "..." if len(response) > 200 else response
    
    return {
        'summary': summary,
        'score1': score1,
        'score2': score2,
        'scoring_explanation': scoring_explanation,
        'strengths1': strengths1,
        'strengths2': strengths2,
        'weaknesses1': weaknesses1,
        'weaknesses2': weaknesses2,
        'recommendation': recommendation,
        'label1': label1,
        'label2': label2,
        'comparison_type': comparison_type
    }

def preprocess_text(text):
    """Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡"""
    # Convert to lowercase
    text = text.lower()
    
    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)
    
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def cosine_similarity_tfidf(text1, text2):
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ú©Ø³ÛŒÙ†ÙˆØ³ÛŒ Ø¨Ø§ TF-IDF"""
    try:
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform([text1, text2])
        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
        return float(similarity)
    except:
        return 0.0

def cosine_similarity_sbert(text1, text2):
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ú©Ø³ÛŒÙ†ÙˆØ³ÛŒ Ø¨Ø§ SBERT"""
    try:
        if sentence_transformer is None:
            return 0.0
        
        embeddings = sentence_transformer.encode([text1, text2])
        similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
        return float(similarity)
    except:
        return 0.0

def jaccard_similarity(text1, text2):
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø¬Ø§Ú©Ø§Ø±Ø¯"""
    try:
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        if union == 0:
            return 0.0
        
        return intersection / union
    except:
        return 0.0

def levenshtein_similarity(text1, text2):
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ù„ÙˆÙ†Ø´ØªØ§ÛŒÙ†"""
    try:
        def levenshtein_distance(s1, s2):
            if len(s1) < len(s2):
                return levenshtein_distance(s2, s1)
            
            if len(s2) == 0:
                return len(s1)
            
            previous_row = list(range(len(s2) + 1))
            for i, c1 in enumerate(s1):
                current_row = [i + 1]
                for j, c2 in enumerate(s2):
                    insertions = previous_row[j + 1] + 1
                    deletions = current_row[j] + 1
                    substitutions = previous_row[j] + (c1 != c2)
                    current_row.append(min(insertions, deletions, substitutions))
                previous_row = current_row
            
            return previous_row[-1]
        
        distance = levenshtein_distance(text1, text2)
        max_len = max(len(text1), len(text2))
        
        if max_len == 0:
            return 1.0
        
        similarity = 1 - (distance / max_len)
        return similarity
    except:
        return 0.0

def sequence_matcher_similarity(text1, text2):
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø¨Ø§ Sequence Matcher"""
    try:
        similarity = SequenceMatcher(None, text1, text2).ratio()
        return similarity
    except:
        return 0.0

def word_overlap_similarity(text1, text2):
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‡Ù…Ù¾ÙˆØ´Ø§Ù†ÛŒ Ú©Ù„Ù…Ø§Øª"""
    try:
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        if len(words1) == 0 and len(words2) == 0:
            return 1.0
        
        intersection = len(words1.intersection(words2))
        min_length = min(len(words1), len(words2))
        
        if min_length == 0:
            return 0.0
        
        return intersection / min_length
    except:
        return 0.0

def get_quality_level(similarity_score):
    """ØªØ¹ÛŒÛŒÙ† Ø³Ø·Ø­ Ú©ÛŒÙÛŒØª Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ù…Ø±Ù‡ Ø´Ø¨Ø§Ù‡Øª"""
    if similarity_score >= 0.9:
        return "Ø¹Ø§Ù„ÛŒ"
    elif similarity_score >= 0.8:
        return "Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ¨"
    elif similarity_score >= 0.7:
        return "Ø®ÙˆØ¨"
    elif similarity_score >= 0.6:
        return "Ù…ØªÙˆØ³Ø·"
    elif similarity_score >= 0.5:
        return "Ø¶Ø¹ÛŒÙ"
    else:
        return "Ø®ÛŒÙ„ÛŒ Ø¶Ø¹ÛŒÙ"

if __name__ == '__main__':
    # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ templates Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯
    os.makedirs('templates', exist_ok=True)
    os.makedirs('static', exist_ok=True)
    os.makedirs('static/css', exist_ok=True)
    os.makedirs('static/js', exist_ok=True)
    
    app.run(debug=True, host='0.0.0.0', port=5000) 
# -*- coding: utf-8 -*-
"""
GraphRAG Web Application - Ø±Ø§Ø¨Ø· ÙˆØ¨ ØªØ¹Ø§Ù…Ù„ÛŒ
"""

from flask import Flask, render_template, request, jsonify, send_from_directory, redirect, url_for
try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass
from graphrag_service import GraphRAGService, RetrievalMethod, GenerationModel
from enhanced_graphrag_service import EnhancedGraphRAGService, TokenExtractionMethod, RetrievalAlgorithm, CommunityDetectionMethod
from text_to_graph_service import TextToGraphService
import json
import os
import shutil
import logging
from datetime import datetime
from werkzeug.utils import secure_filename
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import re
from difflib import SequenceMatcher

# OpenAI import for GPT-4o comparison
try:
    import openai
except ImportError:
    openai = None

# Simple text processing functions without external dependencies
def simple_tokenize(text):
    """Tokenize text without external dependencies"""
    return text.lower().split()

def simple_remove_punctuation(text):
    """Remove punctuation without external dependencies"""
    import string
    return text.translate(str.maketrans('', '', string.punctuation))

# Initialize sentence transformer for semantic similarity (optional)
sentence_transformer = None
try:
    from sentence_transformers import SentenceTransformer
    sentence_transformer = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
except:
    pass

app = Flask(__name__)

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª CORS Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø®Ø·Ø§ÛŒ Failed to fetch
@app.after_request
def after_request(response):
    """Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† header Ù‡Ø§ÛŒ CORS"""
    response.headers.add('Access-Control-Allow-Origin', '*')
    response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization')
    response.headers.add('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE,OPTIONS')
    return response

# Error handler Ø¨Ø±Ø§ÛŒ Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ
@app.errorhandler(Exception)
def handle_exception(e):
    """Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ"""
    logging.error(f"Unhandled exception: {e}", exc_info=True)
    return jsonify({
        'success': False,
        'error': f'Ø®Ø·Ø§ÛŒ Ø³Ø±ÙˆØ±: {str(e)}',
        'error_type': type(e).__name__
    }), 500

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„
UPLOAD_FOLDER = 'uploaded_graphs'
ALLOWED_EXTENSIONS = {'pkl', 'sif', 'tsv', 'csv', 'txt', 'gz'}
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER

# Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ Ø¢Ù¾Ù„ÙˆØ¯ Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

def allowed_file(filename):
    """Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¬Ø§Ø² Ø¨ÙˆØ¯Ù† Ù†ÙˆØ¹ ÙØ§ÛŒÙ„"""
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

# Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³Ø±ÙˆÛŒØ³ GraphRAG Ø¨Ø§ Ú¯Ø±Ø§Ù Hetionet
# Ø§Ø¨ØªØ¯Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú©Ù‡ Ø¢ÛŒØ§ ÙØ§ÛŒÙ„ Ú¯Ø±Ø§Ù Hetionet ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
graph_files = [f for f in os.listdir('.') if f.startswith('hetionet_graph_') and f.endswith('.pkl')]
if graph_files:
    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† ÙØ§ÛŒÙ„ Ú¯Ø±Ø§Ù
    latest_graph_file = max(graph_files)
    print(f"ğŸ”§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú¯Ø±Ø§Ù Hetionet: {latest_graph_file}")
    graphrag_service = GraphRAGService(graph_data_path=latest_graph_file)
    enhanced_graphrag_service = EnhancedGraphRAGService(graph_data_path=latest_graph_file)
else:
    print("âš ï¸ ÙØ§ÛŒÙ„ Ú¯Ø±Ø§Ù Hetionet ÛŒØ§ÙØª Ù†Ø´Ø¯ØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú¯Ø±Ø§Ù Ù†Ù…ÙˆÙ†Ù‡")
    graphrag_service = GraphRAGService()
    enhanced_graphrag_service = EnhancedGraphRAGService()

# ØªÙ†Ø¸ÛŒÙ… API Key Ù‡Ø§ÛŒ OpenAI (Ø§Ø² Ù…ØªØºÛŒØ± Ù…Ø­ÛŒØ·ÛŒ ÛŒØ§ secrets.json)
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")

# ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø¯Ù† Ø§Ø² secrets.json Ø§Ú¯Ø± Ù…ØªØºÛŒØ± Ù…Ø­ÛŒØ·ÛŒ Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯
if not OPENAI_API_KEY:
    try:
        import json as _json
        if os.path.exists('secrets.json'):
            with open('secrets.json', 'r', encoding='utf-8') as _sf:
                _secrets = _json.load(_sf) or {}
                OPENAI_API_KEY = _secrets.get('OPENAI_API_KEY', '')
    except Exception as _e:
        pass

# Ø§Ú¯Ø± Ù‡Ù†ÙˆØ² API key ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡ØŒ Ø§Ø² API key Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù† - ÙÙ‚Ø· Ù‡Ø´Ø¯Ø§Ø± Ø¨Ø¯Ù‡
if not OPENAI_API_KEY:
    print("âš ï¸ OPENAI_API_KEY ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡ Ø§Ø³ØªØ› ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ OpenAI ØºÛŒØ±ÙØ¹Ø§Ù„ Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯")

if OPENAI_API_KEY:
    graphrag_service.set_openai_api_key(OPENAI_API_KEY)
    print("âœ… OpenAI API Key ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯")
else:
    print("âš ï¸ OPENAI_API_KEY ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡ Ø§Ø³ØªØ› ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ OpenAI ØºÛŒØ±ÙØ¹Ø§Ù„ Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯")

@app.route('/')
def index():
    """ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ"""
    return render_template('index.html')

@app.route('/upload_graph')
def upload_graph_page():
    """ØµÙØ­Ù‡ Ø¢Ù¾Ù„ÙˆØ¯ Ú¯Ø±Ø§Ù"""
    return render_template('upload_graph.html')

@app.route('/view_graph')
def view_graph_page():
    """ØµÙØ­Ù‡ Ù†Ù…Ø§ÛŒØ´ Ú¯Ø±Ø§Ù Ø§Ù†ØªØ®Ø§Ø¨â€ŒØ´Ø¯Ù‡ (ÙˆÛŒÚ˜ÙˆØ§Ù„ + Ø¢Ù…Ø§Ø±)"""
    graph_name = request.args.get('graph_name', '')
    graph_path = request.args.get('graph_path', '')
    
    # Decode URL-encoded path
    if graph_path:
        from urllib.parse import unquote
        graph_path = unquote(graph_path)
    
    # Validate path exists
    if graph_path and not os.path.exists(graph_path):
        # Try to resolve relative paths
        if not os.path.isabs(graph_path):
            uploaded_path = os.path.join(UPLOAD_FOLDER, graph_path)
            if os.path.exists(uploaded_path):
                graph_path = uploaded_path
            elif os.path.exists(os.path.join('.', graph_path)):
                graph_path = os.path.join('.', graph_path)
    
    return render_template('view_graph.html', graph_name=graph_name, graph_path=graph_path)

@app.route('/manage_graphs')
def manage_graphs_page():
    """ØµÙØ­Ù‡ Ù…Ø¯ÛŒØ±ÛŒØª Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§"""
    return render_template('manage_graphs.html')

@app.route('/evaluation')
def evaluation():
    return render_template('evaluation.html')

@app.route('/api/upload_graph', methods=['POST'])
def upload_graph():
    """Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ Ú¯Ø±Ø§Ù"""
    try:
        if 'graph_file' not in request.files:
            return jsonify({
                'success': False,
                'error': 'ÙØ§ÛŒÙ„ Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª'
            }), 400
        
        file = request.files['graph_file']
        if file.filename == '':
            return jsonify({
                'success': False,
                'error': 'ÙØ§ÛŒÙ„ Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª'
            }), 400
        
        if file and allowed_file(file.filename):
            filename = secure_filename(file.filename)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename_with_timestamp = f"{timestamp}_{filename}"
            filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename_with_timestamp)
            
            file.save(filepath)
            
            # Ø§Ú¯Ø± ÙØ§ÛŒÙ„ ÙØ´Ø±Ø¯Ù‡ Ø§Ø³ØªØŒ Ø¢Ù† Ø±Ø§ Ø¨Ø§Ø² Ú©Ù†
            if filename.endswith('.gz'):
                import gzip
                with gzip.open(filepath, 'rb') as f_in:
                    uncompressed_path = filepath[:-3]
                    with open(uncompressed_path, 'wb') as f_out:
                        shutil.copyfileobj(f_in, f_out)
                os.remove(filepath)
                filepath = uncompressed_path
                filename_with_timestamp = filename_with_timestamp[:-3]
            
            return jsonify({
                'success': True,
                'message': f'ÙØ§ÛŒÙ„ {filename} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯',
                'filename': filename_with_timestamp,
                'filepath': filepath
            })
        else:
            return jsonify({
                'success': False,
                'error': 'Ù†ÙˆØ¹ ÙØ§ÛŒÙ„ Ù…Ø¬Ø§Ø² Ù†ÛŒØ³Øª. ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ø¬Ø§Ø²: pkl, sif, tsv, csv, txt, gz'
            }), 400
    
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/text_to_graph', methods=['POST'])
def text_to_graph():
    """ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ú¯Ø±Ø§Ù Ø¯Ø§Ù†Ø´"""
    try:
        data = request.get_json()
        
        # Import URL extractor
        try:
            from url_extractor import extract_text_from_url, is_valid_url
            URL_EXTRACTOR_AVAILABLE = True
        except ImportError:
            URL_EXTRACTOR_AVAILABLE = False
            logging.warning("URL extractor not available")
        
        # Import Wikipedia extractor
        try:
            from wikipedia_extractor import WikipediaExtractor
            WIKIPEDIA_EXTRACTOR_AVAILABLE = True
        except ImportError:
            WIKIPEDIA_EXTRACTOR_AVAILABLE = False
            logging.warning("Wikipedia extractor not available")
        
        # Validate input - check for text or URL
        if not data:
            return jsonify({
                'success': False,
                'error': 'Ù…ØªÙ† ÛŒØ§ URL ÙˆØ±ÙˆØ¯ÛŒ Ø§Ù„Ø²Ø§Ù…ÛŒ Ø§Ø³Øª'
            }), 400
        
        text = data.get('text', '').strip()
        url = data.get('url', '').strip()
        use_wikipedia_extraction = data.get('use_wikipedia_extraction', True)  # Ù¾ÛŒØ´â€ŒÙØ±Ø¶: ÙØ¹Ø§Ù„
        
        # Ø§Ú¯Ø± URL Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ØŒ Ù…ØªÙ† Ø±Ø§ Ø§Ø² URL Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†
        if url:
            if not URL_EXTRACTOR_AVAILABLE:
                return jsonify({
                    'success': False,
                    'error': 'Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² URL Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª. Ù„Ø·ÙØ§Ù‹ Ù…ØªÙ† Ø±Ø§ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯.'
                }), 400
            
            if not is_valid_url(url):
                return jsonify({
                    'success': False,
                    'error': 'URL Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª'
                }), 400
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ URL ÙˆÛŒÚ©ÛŒâ€ŒÙ¾Ø¯ÛŒØ§ Ø§Ø³Øª
            is_wikipedia = 'wikipedia.org' in url.lower()
            
            if is_wikipedia and WIKIPEDIA_EXTRACTOR_AVAILABLE and use_wikipedia_extraction:
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ®ØµØµÛŒ ÙˆÛŒÚ©ÛŒâ€ŒÙ¾Ø¯ÛŒØ§
                try:
                    wiki_extractor = WikipediaExtractor(language='fa' if 'fa.wikipedia' in url else 'en')
                    wiki_data = wiki_extractor.extract_from_url(url)
                    
                    if "error" in wiki_data:
                        # Fallback Ø¨Ù‡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ø§Ø¯ÛŒ Ø¨Ø§ clean_content
                        extracted_text = extract_text_from_url(url, clean_content=True, max_length=10000)
                        if not extracted_text:
                            return jsonify({
                                'success': False,
                                'error': wiki_data.get("error", "Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² ÙˆÛŒÚ©ÛŒâ€ŒÙ¾Ø¯ÛŒØ§")
                            }), 400
                        text = extracted_text
                    else:
                        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ø§Ø² ÙˆÛŒÚ©ÛŒâ€ŒÙ¾Ø¯ÛŒØ§
                        text = wiki_data.get("text", "")
                        if not text:
                            text = wiki_extractor.get_full_text(wiki_data.get("title", ""))
                        
                        # Ø§Ú¯Ø± Ù…ØªÙ† Ø®Ø§Ù„ÛŒ Ø§Ø³ØªØŒ Ø§Ø² API Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†
                        if not text or len(text.strip()) < 100:
                            # ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯ Ø¨Ø§ API
                            api_result = wiki_extractor._extract_via_api(wiki_data.get("title", ""))
                            if api_result and api_result.get("text"):
                                text = api_result.get("text")
                        
                        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ Ùˆ Ø±ÙˆØ§Ø¨Ø· Ø§Ø² ÙˆÛŒÚ©ÛŒâ€ŒÙ¾Ø¯ÛŒØ§ Ø¨Ù‡ extraction_params
                        if "entities" in wiki_data and "relationships" in wiki_data:
                            # Ø§ÛŒÙ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø¹Ø¯Ø§Ù‹ Ø¯Ø± process_text_to_graph Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
                            data['wikipedia_entities'] = wiki_data.get("entities", [])
                            data['wikipedia_relationships'] = wiki_data.get("relationships", [])
                        
                        logging.info(f"Wikipedia data extracted from URL: {url} ({len(text)} characters, {len(wiki_data.get('entities', []))} entities)")
                except Exception as e:
                    logging.warning(f"Wikipedia extraction failed, falling back to regular extraction: {e}")
                    extracted_text = extract_text_from_url(url, clean_content=True, max_length=10000)
                    if not extracted_text:
                        return jsonify({
                            'success': False,
                            'error': f'Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² URL: {str(e)}'
                        }), 400
                    text = extracted_text
            else:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ø§Ø¯ÛŒ Ø§Ø² URL Ø¨Ø§ clean_content=True Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù Ù…Ø­ØªÙˆØ§ÛŒ ØºÛŒØ±Ø¶Ø±ÙˆØ±ÛŒ
                extracted_text = extract_text_from_url(url, clean_content=True, max_length=10000)
                if not extracted_text:
                    return jsonify({
                        'success': False,
                        'error': 'Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø§Ø² URL. Ù„Ø·ÙØ§Ù‹ URL Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.'
                    }), 400
                
                text = extracted_text
                logging.info(f"Text extracted from URL: {url} ({len(text)} characters)")
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ù…ØªÙ† ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
        if not text:
            return jsonify({
                'success': False,
                'error': 'Ù…ØªÙ† Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø®Ø§Ù„ÛŒ Ø¨Ø§Ø´Ø¯'
            }), 400
        
        # Get extraction parameters
        method = data.get('method', 'simple')
        max_entities = data.get('max_entities', 100)
        max_relationships = data.get('max_relationships', 200)
        llm_model = data.get('llm_model', 'mistralai/Mistral-7B-Instruct-v0.2')
        confidence_threshold = data.get('confidence_threshold', 0.5)
        hf_token = data.get('hf_token')  # Get token from request
        max_gleanings = data.get('max_gleanings', 2)
        enable_entity_resolution = data.get('enable_entity_resolution', True)
        enable_relationship_weighting = data.get('enable_relationship_weighting', True)
        min_relationship_weight = data.get('min_relationship_weight', 0.0)
        remove_isolated_nodes = data.get('remove_isolated_nodes', False)
        hybrid_methods = data.get('hybrid_methods', ['spacy', 'llm'])
        
        # New parameters for Persian and advanced features
        language = data.get('language', 'auto')  # auto/fa/en
        enable_coreference = data.get('enable_coreference', False)
        chunking_strategy = data.get('chunking_strategy', 'smart')  # smart/sliding_window/sentence/paragraph
        chunk_overlap = data.get('chunk_overlap', 0.2)  # 0.0 to 1.0
        max_tokens = data.get('max_tokens', 512)
        span_model_type = data.get('span_model_type', 'biobert')  # biobert/scibert/auto
        enable_preprocessing = data.get('enable_preprocessing', False)  # Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† (Ø­Ø°Ù stop words)
        
        # Validate method
        valid_methods = ['simple', 'spacy', 'spacy_svo_enhanced', 'llm', 'llm_multipass', 'hybrid',
                        'persian', 'span_based', 'with_coreference', 'long_text',
                        'joint_er', 'autoregressive', 'edc', 'incremental']
        if method not in valid_methods:
            return jsonify({
                'success': False,
                'error': f'Ø±ÙˆØ´ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª. Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù…Ø¬Ø§Ø²: {", ".join(valid_methods)}'
            }), 400
        
        # Initialize text to graph service
        try:
            # Ensure environment variables are loaded
            try:
                from dotenv import load_dotenv
                load_dotenv(override=True)
            except Exception:
                pass
            
            # Set HF_TOKEN from request if provided
            if hf_token:
                os.environ['HF_TOKEN'] = hf_token
                logging.info(f"HF_TOKEN set from request (length: {len(hf_token)})")
            
            text_to_graph_service = TextToGraphService(
                openai_api_key=OPENAI_API_KEY,
                spacy_model='en_core_web_sm',
                hf_token=hf_token  # Pass token directly to service
            )
        except Exception as e:
            return jsonify({
                'success': False,
                'error': f'Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³Ø±ÙˆÛŒØ³: {str(e)}'
            }), 500
        
        # Prepare extraction parameters
        extraction_params = {
            'max_entities': max_entities,
            'max_relationships': max_relationships
        }
        
        if method in ['llm', 'llm_multipass', 'autoregressive', 'edc']:
            extraction_params['model'] = llm_model
            extraction_params['confidence_threshold'] = confidence_threshold
        
        if method == 'llm_multipass':
            extraction_params['max_gleanings'] = max_gleanings
        
        if method == 'hybrid':
            extraction_params['methods'] = hybrid_methods
            extraction_params['confidence_threshold'] = confidence_threshold
        
        # New method-specific parameters
        if method == 'joint_er':
            extraction_params['structure_iterations'] = data.get('structure_iterations', 3)
        
        if method == 'autoregressive':
            extraction_params['max_generation_length'] = data.get('max_generation_length', 2048)
        
        if method == 'edc':
            extraction_params['use_rag'] = data.get('use_rag', True)
        
        if method == 'incremental':
            extraction_params['chunk_size'] = data.get('chunk_size', 500)
            extraction_params['overlap'] = data.get('overlap', 100)
            extraction_params['base_method'] = data.get('base_method', 'spacy')
        
        # New method-specific parameters
        if method == 'persian':
            extraction_params['enable_coreference'] = enable_coreference
        
        if method == 'span_based':
            extraction_params['model_type'] = span_model_type
        
        if method == 'with_coreference':
            extraction_params['base_method'] = data.get('base_method', 'spacy')
        
        if method == 'long_text':
            extraction_params['chunking_strategy'] = chunking_strategy
            extraction_params['chunk_overlap'] = chunk_overlap
            extraction_params['max_tokens'] = max_tokens
            extraction_params['base_method'] = data.get('base_method', 'spacy')
        
        # Prepare processing parameters
        processing_params = {
            'enable_entity_resolution': enable_entity_resolution,
            'enable_relationship_weighting': enable_relationship_weighting,
            'min_relationship_weight': min_relationship_weight,
            'remove_isolated_nodes': remove_isolated_nodes,
            'enable_preprocessing': enable_preprocessing,
            'language': language
        }
        
        # Extract and build graph
        try:
            # Ø§Ú¯Ø± URL Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ØŒ Ø§Ø² process_url_to_graph Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
            if url:
                result = text_to_graph_service.process_url_to_graph(
                    url=url,
                    method=method,
                    use_wikipedia_extraction=use_wikipedia_extraction,
                    save=True,
                    output_dir=UPLOAD_FOLDER,
                    **extraction_params,
                    **processing_params
                )
            else:
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² process_text_to_graph Ø¨Ø±Ø§ÛŒ Ù…ØªÙ† Ù…Ø³ØªÙ‚ÛŒÙ…
                result = text_to_graph_service.process_text_to_graph(
                    text=text,
                    method=method,
                    save=True,
                    output_dir=UPLOAD_FOLDER,
                    **extraction_params,
                    **processing_params
                )
            
            # Ø§Ú¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆÛŒÚ©ÛŒâ€ŒÙ¾Ø¯ÛŒØ§ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŒ Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø§ Ù†ØªØ§ÛŒØ¬ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø¯ØºØ§Ù… Ú©Ù†
            if 'wikipedia_entities' in data and 'wikipedia_relationships' in data:
                wiki_entities = data.get('wikipedia_entities', [])
                wiki_relationships = data.get('wikipedia_relationships', [])
                
                if wiki_entities or wiki_relationships:
                    extraction_result = result.get('extraction_result', {})
                    existing_entities = extraction_result.get('entities', [])
                    existing_relationships = extraction_result.get('relationships', [])
                    
                    # Ø§ÛŒØ¬Ø§Ø¯ map Ø¨Ø±Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ (Ø¨Ø± Ø§Ø³Ø§Ø³ name)
                    entity_map = {}
                    entity_id_map = {}  # name -> id
                    for ent in existing_entities:
                        ent_name = ent.get('name', '').lower().strip()
                        if ent_name:
                            entity_map[ent_name] = ent
                            entity_id_map[ent_name] = ent.get('id')
                    
                    # Ø§Ø¯ØºØ§Ù… Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ ÙˆÛŒÚ©ÛŒâ€ŒÙ¾Ø¯ÛŒØ§
                    next_id = len(existing_entities)
                    for wiki_ent in wiki_entities:
                        ent_name = wiki_ent.get('name', '').strip()
                        if not ent_name:
                            continue
                        
                        ent_name_lower = ent_name.lower()
                        
                        # Ø¨Ø±Ø±Ø³ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨ÙˆØ¯Ù†
                        if ent_name_lower in entity_map:
                            # Ù…ÙˆØ¬ÙˆØ¯ÛŒØª Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª - Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ attributes
                            existing_ent = entity_map[ent_name_lower]
                            if 'wikipedia' not in existing_ent.get('attributes', {}).get('source', ''):
                                # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙˆÛŒÚ©ÛŒâ€ŒÙ¾Ø¯ÛŒØ§
                                if 'attributes' not in existing_ent:
                                    existing_ent['attributes'] = {}
                                existing_ent['attributes']['wikipedia_source'] = True
                        else:
                            # Ù…ÙˆØ¬ÙˆØ¯ÛŒØª Ø¬Ø¯ÛŒØ¯ - Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù†
                            wiki_ent['id'] = f"ENTITY_{next_id}"
                            wiki_ent['attributes'] = wiki_ent.get('attributes', {})
                            wiki_ent['attributes']['source'] = 'wikipedia'
                            existing_entities.append(wiki_ent)
                            entity_map[ent_name_lower] = wiki_ent
                            entity_id_map[ent_name_lower] = wiki_ent['id']
                            next_id += 1
                    
                    # Ø§Ø¯ØºØ§Ù… Ø±ÙˆØ§Ø¨Ø·
                    rel_set = set()
                    for rel in existing_relationships:
                        source = rel.get('source', '')
                        target = rel.get('target', '')
                        metaedge = rel.get('metaedge', '')
                        rel_set.add((source, target, metaedge))
                    
                    # ØªØ¨Ø¯ÛŒÙ„ source/target Ø¯Ø± Ø±ÙˆØ§Ø¨Ø· ÙˆÛŒÚ©ÛŒâ€ŒÙ¾Ø¯ÛŒØ§ Ø¨Ù‡ ID
                    for wiki_rel in wiki_relationships:
                        source_name = wiki_rel.get('source', '').strip()
                        target_name = wiki_rel.get('target', '').strip()
                        
                        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ID Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§
                        source_id = None
                        target_id = None
                        
                        # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± entity_id_map
                        for name, eid in entity_id_map.items():
                            if source_name.lower() == name or source_name.lower() in name or name in source_name.lower():
                                source_id = eid
                            if target_name.lower() == name or target_name.lower() in name or name in target_name.lower():
                                target_id = eid
                        
                        # Ø§Ú¯Ø± Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ø§Ø² ID Ù…Ø³ØªÙ‚ÛŒÙ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
                        if not source_id:
                            source_id = wiki_rel.get('source', '')
                        if not target_id:
                            target_id = wiki_rel.get('target', '')
                        
                        if source_id and target_id:
                            key = (source_id, target_id, wiki_rel.get('metaedge', ''))
                            if key not in rel_set:
                                wiki_rel['source'] = source_id
                                wiki_rel['target'] = target_id
                                wiki_rel['attributes'] = wiki_rel.get('attributes', {})
                                wiki_rel['attributes']['source'] = 'wikipedia'
                                existing_relationships.append(wiki_rel)
                                rel_set.add(key)
                    
                    # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ extraction_result
                    extraction_result['entities'] = existing_entities
                    extraction_result['relationships'] = existing_relationships
                    extraction_result['wikipedia_extracted'] = True
                    extraction_result['wikipedia_stats'] = {
                        'wiki_entities': len(wiki_entities),
                        'wiki_relationships': len(wiki_relationships),
                        'merged_entities': len(existing_entities),
                        'merged_relationships': len(existing_relationships)
                    }
                    
                    # Ø³Ø§Ø®Øª Ù…Ø¬Ø¯Ø¯ Ú¯Ø±Ø§Ù Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¯ØºØ§Ù… Ø´Ø¯Ù‡
                    try:
                        graph = text_to_graph_service.build_graph(extraction_result)
                        result['graph'] = graph
                        result['extraction_result'] = extraction_result
                        logging.info(f"Merged Wikipedia data: {len(wiki_entities)} entities, {len(wiki_relationships)} relationships")
                    except Exception as e:
                        logging.warning(f"Failed to rebuild graph with Wikipedia data: {e}")
                        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú¯Ø±Ø§Ù Ù‚Ø¨Ù„ÛŒ
            
            # Get filename from filepath
            filename = os.path.basename(result['filepath']) if result.get('filepath') else None
            
            # Graph is saved and ready to be loaded via /api/load_graph endpoint
            # The graph will appear in the list of available graphs
            
            # Extract graph data for preview from the newly created graph
            graph = result.get('graph')
            extraction_result = result.get('extraction_result', {})
            graph_data = {
                'nodes': [],
                'edges': []
            }
            
            if graph:
                # Extract nodes from the graph with kind information
                for node_id, node_data in graph.nodes(data=True):
                    # Get kind from node data (prefer 'kind' over 'type')
                    node_kind = node_data.get('kind') or node_data.get('type', 'Unknown')
                    node_name = node_data.get('name', node_id)
                    
                    graph_data['nodes'].append({
                        'id': node_id,
                        'label': node_name,
                        'type': node_data.get('type', 'Unknown'),
                        'kind': node_kind,
                        'title': f"Ù†Ø§Ù…: {node_name}\nÙ†ÙˆØ¹: {node_data.get('type', 'Unknown')}\nKind: {node_kind}"
                    })
                
                # Extract edges from the graph with relationship information
                for source, target, edge_data in graph.edges(data=True):
                    # Get metaedge/relation from edge data
                    metaedge = edge_data.get('metaedge', 'related_to')
                    relation = edge_data.get('relation') or metaedge
                    
                    # Get relationship meaning/description
                    relation_meaning = edge_data.get('relation_meaning') or edge_data.get('description') or relation
                    
                    graph_data['edges'].append({
                        'from': source,
                        'to': target,
                        'label': relation,
                        'metaedge': metaedge,
                        'relation': relation,
                        'relation_meaning': relation_meaning,
                        'title': f"Ø±Ø§Ø¨Ø·Ù‡: {relation}\nÙ…ÙÙ‡ÙˆÙ…: {relation_meaning}"
                    })
            
            # Also include extraction result data for reference
            extraction_entities = extraction_result.get('entities', [])
            extraction_relationships = extraction_result.get('relationships', [])
            
            return jsonify({
                'success': True,
                'message': 'Ú¯Ø±Ø§Ù Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯',
                'filename': filename,
                'filepath': result.get('filepath'),
                'stats': result.get('stats', {}),
                'extraction_method': method,
                'resolution_summary': result.get('resolution_summary'),
                'load_url': '/api/load_graph',  # Hint for frontend to optionally load the graph
                'graph_data': graph_data,  # Graph data for preview (from newly created graph)
                'extraction_data': {  # Original extraction data for reference
                    'entities': extraction_entities,
                    'relationships': extraction_relationships
                }
            })
            
        except ValueError as e:
            # Handle validation errors - include more details
            error_msg = str(e)
            logging.error(f"Validation error in text to graph conversion: {error_msg}")
            logging.error(f"Error type: {type(e).__name__}")
            
            # Try to extract more context from the error
            import traceback
            error_trace = traceback.format_exc()
            logging.error(f"Error traceback: {error_trace}")
            
            return jsonify({
                'success': False,
                'error': error_msg,
                'error_type': 'validation_error',
                'method': method
            }), 400
        except Exception as e:
            # Handle other errors - include full traceback
            import traceback
            error_msg = str(e)
            error_trace = traceback.format_exc()
            logging.error(f"Error in text to graph conversion: {error_msg}")
            logging.error(f"Error type: {type(e).__name__}")
            logging.error(f"Full traceback: {error_trace}")
            
            return jsonify({
                'success': False,
                'error': f'Ø®Ø·Ø§ Ø¯Ø± ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ú¯Ø±Ø§Ù: {error_msg}',
                'error_type': 'server_error',
                'method': method,
                'details': error_trace[-500:] if len(error_trace) > 500 else error_trace  # Last 500 chars
            }), 500
    
    except Exception as e:
        logging.error(f"Unexpected error in text_to_graph endpoint: {e}")
        return jsonify({
            'success': False,
            'error': f'Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡: {str(e)}'
        }), 500

@app.route('/api/list_graphs')
def list_graphs():
    """Ù„ÛŒØ³Øª Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯"""
    try:
        graphs = []
        
        # Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒ Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯Ù‡
        if os.path.exists(UPLOAD_FOLDER):
            for filename in os.listdir(UPLOAD_FOLDER):
                filepath = os.path.join(UPLOAD_FOLDER, filename)
                if os.path.isfile(filepath):
                    file_size = os.path.getsize(filepath)
                    file_date = datetime.fromtimestamp(os.path.getctime(filepath))
                    graphs.append({
                        'name': filename,
                        'path': filepath,
                        'size': file_size,
                        'date': file_date.isoformat(),
                        'type': 'uploaded'
                    })
        
        # Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù¾ÙˆØ´Ù‡ Ø§ØµÙ„ÛŒ
        for filename in os.listdir('.'):
            if filename.endswith('.pkl') and filename.startswith('hetionet_graph_'):
                filepath = os.path.join('.', filename)
                file_size = os.path.getsize(filepath)
                file_date = datetime.fromtimestamp(os.path.getctime(filepath))
                graphs.append({
                    'name': filename,
                    'path': filepath,
                    'size': file_size,
                    'date': file_date.isoformat(),
                    'type': 'builtin'
                })
        
        # Ù…Ø±ØªØ¨ Ú©Ø±Ø¯Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ§Ø±ÛŒØ® (Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ø§ÙˆÙ„)
        graphs.sort(key=lambda x: x['date'], reverse=True)
        
        return jsonify({
            'success': True,
            'graphs': graphs
        })
    
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/load_graph', methods=['POST'])
def load_graph():
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú¯Ø±Ø§Ù Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡"""
    try:
        data = request.get_json()
        graph_path = data.get('graph_path')
        
        if not graph_path or not os.path.exists(graph_path):
            return jsonify({
                'success': False,
                'error': 'Ù…Ø³ÛŒØ± Ú¯Ø±Ø§Ù Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª'
            }), 400
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú¯Ø±Ø§Ù Ø¬Ø¯ÛŒØ¯
        global graphrag_service
        graphrag_service = GraphRAGService(graph_data_path=graph_path)
        if OPENAI_API_KEY:
            graphrag_service.set_openai_api_key(OPENAI_API_KEY)
        
        return jsonify({
            'success': True,
            'message': f'Ú¯Ø±Ø§Ù {os.path.basename(graph_path)} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯'
        })
    
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/delete_graph', methods=['POST'])
def delete_graph():
    """Ø­Ø°Ù Ú¯Ø±Ø§Ù"""
    try:
        data = request.get_json()
        graph_path = data.get('graph_path')
        
        if not graph_path or not os.path.exists(graph_path):
            return jsonify({
                'success': False,
                'error': 'Ù…Ø³ÛŒØ± Ú¯Ø±Ø§Ù Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª'
            }), 400
        
        # Ø­Ø°Ù ÙØ§ÛŒÙ„
        os.remove(graph_path)
        
        return jsonify({
            'success': True,
            'message': f'Ú¯Ø±Ø§Ù {os.path.basename(graph_path)} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø­Ø°Ù Ø´Ø¯'
        })
    
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/graph_view_data', methods=['POST'])
def graph_view_data():
    """Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú¯Ø±Ø§Ù Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ ÙˆÛŒÚ˜ÙˆØ§Ù„ Ùˆ Ø¢Ù…Ø§Ø±ÛŒ Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ú¯Ø±Ø§Ù ÙØ¹Ø§Ù„ Ø³ÛŒØ³ØªÙ…"""
    try:
        data = request.get_json() or {}
        graph_path = data.get('graph_path')

        if not graph_path:
            return jsonify({
                'success': False,
                'error': 'Ù…Ø³ÛŒØ± Ú¯Ø±Ø§Ù Ù…Ø´Ø®Øµ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª'
            }), 400

        # Normalize path - handle both relative and absolute paths
        graph_path = str(graph_path).strip()

        # Decode URL-encoded path if needed
        from urllib.parse import unquote
        if '%' in graph_path:
            graph_path = unquote(graph_path)

        # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ Ù…Ø´Ú©Ù„Ø§Øª Ù…Ø³ÛŒØ± (Ù…Ø«Ù„ ØªÚ©Ø±Ø§Ø± Ù†Ø§Ù… Ù¾ÙˆØ´Ù‡ ÛŒØ§ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø¹Ø¬ÛŒØ¨)
        original_graph_path = graph_path

        # Ø§Ú¯Ø± ÙØ§ÛŒÙ„ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŒ Ù‡Ù…Ø§Ù† Ø±Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
        if not os.path.exists(graph_path):
            # Ù‡Ù…ÛŒØ´Ù‡ ÛŒÚ© Ø¨Ø§Ø± ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ù‡Ù… Ø§Ù…ØªØ­Ø§Ù† Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            filename_only = os.path.basename(graph_path)

            candidate_paths = []

            # 1) uploaded_graphs/filename
            candidate_paths.append(os.path.join(UPLOAD_FOLDER, filename_only))

            # 2) ./filename Ø¯Ø± Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ ÙØ¹Ù„ÛŒ
            candidate_paths.append(os.path.join('.', filename_only))

            # 3) Ø§Ú¯Ø± Ø¯Ø± Ù…Ø³ÛŒØ± Ø±Ø´ØªÙ‡â€ŒÛŒ uploaded_graphs Ø¢Ù…Ø¯Ù‡ØŒ Ø¨Ø¹Ø¯ Ø§Ø² Ø¢Ù† Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø¨Ú¯ÛŒØ±
            lower_path = graph_path.lower()
            marker = 'uploaded_graphs'
            if marker in lower_path:
                idx = lower_path.rfind(marker)
                tail = graph_path[idx + len(marker):]
                tail = tail.lstrip('\\/._\u0082')
                if tail:
                    candidate_paths.append(os.path.join(UPLOAD_FOLDER, tail))

            # Ø§ÙˆÙ„ÛŒÙ† Ù…Ø³ÛŒØ±ÛŒ Ú©Ù‡ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†
            resolved = None
            for c in candidate_paths:
                if os.path.exists(c):
                    resolved = c
                    break

            if resolved is None:
                # ØªÙ„Ø§Ø´ ÙˆÛŒÚ˜Ù‡ Ø¨Ø±Ø§ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ text_graph:
                # Ø§Ú¯Ø± Ø¨Ø®Ø´ÛŒ Ø¹Ø¯Ø¯ÛŒ Ù…Ø«Ù„ 60107_115532 Ø¯Ø± Ù†Ø§Ù… Ø¨Ø§Ø´Ø¯ØŒ Ø¯Ø± Ø¨ÛŒÙ† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ uploaded_graphs Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØ·Ø§Ø¨Ù‚ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…
                import re as _re
                numeric_match = _re.search(r'(\d{5}_\d{6})', filename_only)
                if numeric_match:
                    numeric_part = numeric_match.group(1)
                    try:
                        for f in os.listdir(UPLOAD_FOLDER):
                            # Ø¨Ù‡â€ŒØ¯Ù†Ø¨Ø§Ù„ Ù‡Ù…Ø§Ù† Ø¨Ø®Ø´ Ø¹Ø¯Ø¯ÛŒ ÙˆØ³Ø· Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ù…ÛŒâ€ŒÚ¯Ø±Ø¯ÛŒÙ…
                            if numeric_part in f and f.endswith('_text_graph.pkl'):
                                candidate = os.path.join(UPLOAD_FOLDER, f)
                                if os.path.exists(candidate):
                                    resolved = candidate
                                    candidate_paths.append(candidate)
                                    break
                    except Exception as e:
                        logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ ÙØ§ÛŒÙ„ Ø¯Ø± uploaded_graphs: {str(e)}")

                if resolved is None:
                    logging.error(f"Ù…Ø³ÛŒØ± Ú¯Ø±Ø§Ù ÛŒØ§ÙØª Ù†Ø´Ø¯. ÙˆØ±ÙˆØ¯ÛŒ: {original_graph_path}, Ø§Ù…ØªØ­Ø§Ù†â€ŒØ´Ø¯Ù‡: {candidate_paths}")
                    return jsonify({
                        'success': False,
                        'error': f'Ù…Ø³ÛŒØ± Ú¯Ø±Ø§Ù Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª: {original_graph_path}'
                    }), 400

            graph_path = resolved

        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ÙˆÙ‚Øª Ú¯Ø±Ø§Ù ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± graphrag_service Ø³Ø±Ø§Ø³Ø±ÛŒ)
        try:
            temp_service = GraphRAGService(graph_data_path=graph_path)
            G = getattr(temp_service, 'G', None)

            if G is None:
                return jsonify({
                    'success': False,
                    'error': 'Ú¯Ø±Ø§Ù Ø¯Ø± ÙØ§ÛŒÙ„ ÛŒØ§ÙØª Ù†Ø´Ø¯ ÛŒØ§ ÙØ§ÛŒÙ„ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª'
                }), 500
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú¯Ø±Ø§Ù Ø§Ø² Ù…Ø³ÛŒØ± {graph_path}: {str(e)}")
            return jsonify({
                'success': False,
                'error': f'Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú¯Ø±Ø§Ù: {str(e)}'
            }), 500

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± Ù¾Ø§ÛŒÙ‡ Ú¯Ø±Ø§Ù
        num_nodes = G.number_of_nodes()
        num_edges = G.number_of_edges()
        avg_degree = (2 * num_edges / num_nodes) if num_nodes > 0 else 0
        try:
            import networkx as nx  # Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú†Ú¯Ø§Ù„ÛŒ Ø§Ú¯Ø± Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø¨Ø§Ø´Ø¯
            density = nx.density(G)
        except Exception:
            density = None

        # Ø´Ù…Ø§Ø±Ø´ Ø§Ù†ÙˆØ§Ø¹ Ù†ÙˆØ¯ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙÛŒÙ„Ø¯ kind ÛŒØ§ type
        node_types = {}
        for node_id, node_data in G.nodes(data=True):
            kind = node_data.get('kind') or node_data.get('type', 'Unknown')
            node_types[kind] = node_types.get(kind, 0) + 1

        stats = {
            'num_nodes': num_nodes,
            'num_edges': num_edges,
            'avg_degree': avg_degree,
            'density': density
        }

        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú¯Ø±Ø§Ù Ø¨Ø±Ø§ÛŒ ÙˆÛŒÚ˜ÙˆØ§Ù„â€ŒØ³Ø§Ø²ÛŒ
        graph_data = {
            'nodes': [],
            'edges': []
        }

        for node_id, node_data in G.nodes(data=True):
            node_kind = node_data.get('kind') or node_data.get('type', 'Unknown')
            node_name = node_data.get('name', node_id)

            graph_data['nodes'].append({
                'id': node_id,
                'label': node_name,
                'type': node_data.get('type', 'Unknown'),
                'kind': node_kind,
                'title': f"Ù†Ø§Ù…: {node_name}\nÙ†ÙˆØ¹: {node_data.get('type', 'Unknown')}\nKind: {node_kind}"
            })

        for source, target, edge_data in G.edges(data=True):
            metaedge = edge_data.get('metaedge', 'related_to')
            relation = edge_data.get('relation') or metaedge
            relation_meaning = edge_data.get('relation_meaning') or edge_data.get('description') or relation

            graph_data['edges'].append({
                'from': source,
                'to': target,
                'label': relation,
                'metaedge': metaedge,
                'relation': relation,
                'relation_meaning': relation_meaning,
                'title': f"Ø±Ø§Ø¨Ø·Ù‡: {relation}\nÙ…ÙÙ‡ÙˆÙ…: {relation_meaning}"
            })

        return jsonify({
            'success': True,
            'stats': stats,
            'node_types': node_types,
            'graph_data': graph_data
        })

    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/process_query', methods=['POST'])
def process_query():
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³ÙˆØ§Ù„ Ùˆ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ù†ØªÛŒØ¬Ù‡"""
    try:
        data = request.get_json()
        query = data.get('query', '')
        retrieval_method = data.get('retrieval_method', 'BFS')
        generation_model = data.get('generation_model', 'GPT_SIMULATION')
        text_generation_type = data.get('text_generation_type', 'INTELLIGENT')
        max_depth = data.get('max_depth', 2)
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
        max_nodes = data.get('max_nodes', 20)
        max_edges = data.get('max_edges', 40)
        similarity_threshold = data.get('similarity_threshold', 0.3)
        community_detection_method = data.get('community_detection_method', 'louvain')
        advanced_retrieval_algorithm = data.get('advanced_retrieval_algorithm', 'hybrid')
        advanced_token_extraction_method = data.get('advanced_token_extraction_method', 'llm_based')
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø±Ø´ØªÙ‡ Ø¨Ù‡ enum
        retrieval_enum = RetrievalMethod[retrieval_method]
        generation_enum = GenerationModel[generation_model.replace(' ', '_')]
        
        # ØªÙ†Ø¸ÛŒÙ… Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø§Ú¯Ø± Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
        if any([max_nodes != 20, max_edges != 40, similarity_threshold != 0.3]):
            graphrag_service.set_config(
                max_nodes=max_nodes,
                max_edges=max_edges,
                similarity_threshold=similarity_threshold,
                community_detection_method=community_detection_method,
                advanced_retrieval_algorithm=advanced_retrieval_algorithm,
                advanced_token_extraction_method=advanced_token_extraction_method
            )
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³ÙˆØ§Ù„
        result = graphrag_service.process_query(
            query=query,
            retrieval_method=retrieval_enum,
            generation_model=generation_enum,
            text_generation_type=text_generation_type,
            max_depth=max_depth
        )
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† timestamp Ùˆ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡
        result['timestamp'] = datetime.now().isoformat()
        result['advanced_settings'] = {
            'max_nodes': max_nodes,
            'max_edges': max_edges,
            'similarity_threshold': similarity_threshold,
            'community_detection_method': community_detection_method,
            'advanced_retrieval_algorithm': advanced_retrieval_algorithm,
            'advanced_token_extraction_method': advanced_token_extraction_method
        }
        
        return jsonify({
            'success': True,
            'result': result
        })
    
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/enhanced_process_query', methods=['POST'])
def enhanced_process_query():
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³ÙˆØ§Ù„ Ø¨Ø§ Ø³Ø±ÙˆÛŒØ³ Ù¾ÛŒØ´Ø±ÙØªÙ‡ GraphRAG"""
    try:
        data = request.get_json()
        query = data.get('query', '')
        token_extraction_method = data.get('token_extraction_method', 'llm_based')
        retrieval_algorithm = data.get('retrieval_algorithm', 'hybrid')
        community_detection_method = data.get('community_detection_method', 'louvain')
        max_depth = data.get('max_depth', 3)
        max_nodes = data.get('max_nodes', 20)
        max_edges = data.get('max_edges', 40)
        similarity_threshold = data.get('similarity_threshold', 0.3)
        
        # ØªÙ†Ø¸ÛŒÙ… Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ø³Ø±ÙˆÛŒØ³ Ù¾ÛŒØ´Ø±ÙØªÙ‡
        enhanced_graphrag_service.set_config(
            token_extraction_method=token_extraction_method,
            retrieval_algorithm=retrieval_algorithm,
            community_detection_method=community_detection_method,
            max_depth=max_depth,
            max_nodes=max_nodes,
            max_edges=max_edges,
            similarity_threshold=similarity_threshold
        )
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³ÙˆØ§Ù„
        result = enhanced_graphrag_service.process_query(query)
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† timestamp
        result['timestamp'] = datetime.now().isoformat()
        result['config'] = enhanced_graphrag_service.get_config()
        
        return jsonify({
            'success': True,
            'result': result
        })
    
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/graph_info')
def graph_info():
    """Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú¯Ø±Ø§Ù"""
    try:
        G = graphrag_service.G
        if G:
            node_types = {}
            for node, attrs in G.nodes(data=True):
                kind = attrs.get('kind', 'Unknown')
                node_types[kind] = node_types.get(kind, 0) + 1
            
            return jsonify({
                'success': True,
                'total_nodes': G.number_of_nodes(),
                'total_edges': G.number_of_edges(),
                'node_types': node_types
            })
        else:
            return jsonify({
                'success': False,
                'error': 'Graph not loaded'
            })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/enhanced_graph_info')
def enhanced_graph_info():
    """Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú¯Ø±Ø§Ù Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
    try:
        stats = enhanced_graphrag_service.get_graph_statistics()
        if stats:
            return jsonify({
                'success': True,
                'statistics': stats
            })
        else:
            return jsonify({
                'success': False,
                'error': 'Enhanced graph not loaded'
            })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/token_extraction_methods')
def token_extraction_methods():
    """Ø¯Ø±ÛŒØ§ÙØª Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆÚ©Ù†"""
    methods = [
        {
            'value': 'llm_based',
            'label': 'Ø¨Ø± Ø§Ø³Ø§Ø³ LLM',
            'description': 'Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆÚ©Ù† Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø²Ø¨Ø§Ù†ÛŒ'
        },
        {
            'value': 'rule_based',
            'label': 'Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‚ÙˆØ§Ù†ÛŒÙ†',
            'description': 'Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆÚ©Ù† Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‚ÙˆØ§Ù†ÛŒÙ† Ø§Ø² Ù¾ÛŒØ´ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡'
        },
        {
            'value': 'hybrid',
            'label': 'ØªØ±Ú©ÛŒØ¨ÛŒ',
            'description': 'ØªØ±Ú©ÛŒØ¨ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ LLM Ùˆ Ù‚ÙˆØ§Ù†ÛŒÙ†'
        },
        {
            'value': 'semantic',
            'label': 'Ù…Ø¹Ù†Ø§ÛŒÛŒ',
            'description': 'Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆÚ©Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´Ø¨Ø§Ù‡Øª Ù…Ø¹Ù†Ø§ÛŒÛŒ'
        }
    ]
    return jsonify({'methods': methods})

@app.route('/api/retrieval_algorithms')
def retrieval_algorithms():
    """Ø¯Ø±ÛŒØ§ÙØª Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ"""
    algorithms = [
        # Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
        {
            'value': 'semantic_similarity',
            'label': 'Ø´Ø¨Ø§Ù‡Øª Ù…Ø¹Ù†Ø§ÛŒÛŒ (Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡)',
            'description': 'Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´Ø¨Ø§Ù‡Øª Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¨Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‚Øª Ùˆ Ù¾ÙˆØ´Ø´ Ú˜Ù†â€ŒÙ‡Ø§'
        },
        {
            'value': 'hybrid',
            'label': 'ØªØ±Ú©ÛŒØ¨ÛŒ (Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡)',
            'description': 'ØªØ±Ú©ÛŒØ¨ Ú†Ù†Ø¯ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø¨Ø§ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ùˆ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‚Øª'
        },
        {
            'value': 'pagerank',
            'label': 'PageRank (Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡)',
            'description': 'Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… PageRank Ø¨Ø§ ØªÙ…Ø±Ú©Ø² Ø±ÙˆÛŒ Ú˜Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù…'
        },
        {
            'value': 'community_detection',
            'label': 'ØªØ´Ø®ÛŒØµ Ø¬Ø§Ù…Ø¹Ù‡ (Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡)',
            'description': 'Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ´Ø®ÛŒØµ Ø¬Ø§Ù…Ø¹Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‚Øª Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ'
        },
        {
            'value': 'bfs',
            'label': 'BFS (Ø¬Ø³ØªØ¬ÙˆÛŒ Ø³Ø·Ø­ Ø§ÙˆÙ„ - Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡)',
            'description': 'Ø¬Ø³ØªØ¬ÙˆÛŒ Ø³Ø·Ø­ Ø§ÙˆÙ„ Ø¯Ø± Ú¯Ø±Ø§Ù Ø¨Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ø¹Ù…Ù‚ Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ'
        },
        {
            'value': 'dfs',
            'label': 'DFS (Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¹Ù…ÛŒÙ‚ Ø§ÙˆÙ„ - Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡)',
            'description': 'Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¹Ù…ÛŒÙ‚ Ø§ÙˆÙ„ Ø¯Ø± Ú¯Ø±Ø§Ù Ø¨Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‚Øª Ùˆ Ú©Ø§Ù‡Ø´ Ø²Ù…Ø§Ù†'
        },
        {
            'value': 'n_hop',
            'label': 'N-Hop (Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡)',
            'description': 'Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ N-Hop Ø¨Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ø¹Ù…Ù‚ Ùˆ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯'
        }
    ]
    return jsonify({'algorithms': algorithms})

@app.route('/api/community_detection_methods')
def community_detection_methods():
    """Ø¯Ø±ÛŒØ§ÙØª Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø¬Ø§Ù…Ø¹Ù‡"""
    methods = [
        {
            'value': 'louvain',
            'label': 'Louvain',
            'description': 'Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Louvain Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø¬Ø§Ù…Ø¹Ù‡'
        },
        {
            'value': 'label_propagation',
            'label': 'Label Propagation',
            'description': 'Ø§Ù†ØªØ´Ø§Ø± Ø¨Ø±Ú†Ø³Ø¨ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø¬Ø§Ù…Ø¹Ù‡'
        },
        {
            'value': 'girvan_newman',
            'label': 'Girvan-Newman',
            'description': 'Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Girvan-Newman'
        },
        {
            'value': 'spectral',
            'label': 'Spectral',
            'description': 'Ø±ÙˆØ´ Ø·ÛŒÙÛŒ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø¬Ø§Ù…Ø¹Ù‡'
        }
    ]
    return jsonify({'methods': methods})

@app.route('/api/sample_queries')
def sample_queries():
    """Ø³ÙˆØ§Ù„Ø§Øª Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø§Ø®ØªØ§Ø± Hetionet Ùˆ Ø±ÙˆØ§Ø¨Ø· Ù…ÙˆØ¬ÙˆØ¯"""
    samples = [
        # Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¨ÛŒØ§Ù† Ú˜Ù† Ø¯Ø± Ø¨Ø§ÙØªâ€ŒÙ‡Ø§ (AeG, AuG, AdG)
        "What genes are expressed in the heart?",
        "Which genes are upregulated in the brain?",
        "What genes are downregulated in muscle tissue?",
        "How do genes express differently in liver vs kidney?",
        "What genes are expressed in the lung?",
        
        # Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ú˜Ù†â€ŒÙ‡Ø§ Ùˆ Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ (DaG, DuG, DdG)
        "What genes are associated with diabetes?",
        "How does TP53 relate to cancer?",
        "Which genes are upregulated in cancer?",
        "What genes are downregulated in heart disease?",
        "What genes are associated with Alzheimer's disease?",
        
        # Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¯Ø§Ø±ÙˆÙ‡Ø§ Ùˆ Ø¯Ø±Ù…Ø§Ù† (CtD, CuG, CdG)
        "What drugs treat diabetes?",
        "Which compounds upregulate TP53?",
        "What drugs downregulate cancer genes?",
        "How do drugs interact with genes?",
        "What compounds bind to insulin receptor?",
        
        # Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ ÙØ±Ø¢ÛŒÙ†Ø¯Ù‡Ø§ÛŒ Ø²ÛŒØ³ØªÛŒ (GpBP, GpMF, GpCC)
        "What genes participate in cell cycle regulation?",
        "Which genes are involved in apoptosis?",
        "What molecular functions does TP53 have?",
        "How do genes function in cellular components?",
        "What genes participate in DNA repair?",
        
        # Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø²ÛŒØ³ØªÛŒ (GpPW)
        "What pathways are involved in cancer progression?",
        "Which signaling pathways regulate metabolism?",
        "How do genes participate in immune pathways?",
        "What pathways control cell growth?",
        "Which pathways involve insulin signaling?",
        
        # Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ ØªØ¹Ø§Ù…Ù„ Ú˜Ù†â€ŒÙ‡Ø§ (GiG, Gr>G)
        "How do genes interact with each other?",
        "Which genes regulate TP53?",
        "What genes are regulated by TP53?",
        "How do genes covary in expression?",
        "What genes interact with BRCA1?",
        
        # Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ Ùˆ Ø¹Ù„Ø§Ø¦Ù… (DpS, DlA)
        "What symptoms are associated with diabetes?",
        "How does cancer affect different tissues?",
        "What diseases affect the heart?",
        "Which diseases localize to specific tissues?",
        "What diseases present similar symptoms?",
        
        # Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¹ÙˆØ§Ø±Ø¶ Ø¬Ø§Ù†Ø¨ÛŒ Ø¯Ø§Ø±ÙˆÙ‡Ø§ (CcSE)
        "What side effects does aspirin cause?",
        "How do drugs affect patient symptoms?",
        "What adverse reactions occur with diabetes drugs?",
        "Which compounds cause heart-related side effects?",
        "What side effects do cancer drugs cause?",
        
        # Ø³ÙˆØ§Ù„Ø§Øª Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ùˆ Ú†Ù†Ø¯Ù…Ø±Ø­Ù„Ù‡â€ŒØ§ÛŒ
        "How do drugs affect gene expression in heart tissue?",
        "What genes and pathways are involved in diabetes progression?",
        "How do genetic mutations lead to cancer development?",
        "What therapeutic targets exist for heart disease?",
        "How do genes participate in drug metabolism pathways?",
        "What biological processes are disrupted in cancer?",
        "What drugs treat diseases that affect the heart?",
        "How do genes that interact with TP53 relate to cancer?",
        "What compounds bind to genes expressed in the brain?",
        "Which diseases have symptoms related to diabetes?"
    ]
    return jsonify({'queries': samples})

@app.route('/api/config', methods=['GET', 'POST'])
def config_endpoint():
    """Ù…Ø¯ÛŒØ±ÛŒØª ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø³ÛŒØ³ØªÙ…"""
    if request.method == 'GET':
        # Ø¯Ø±ÛŒØ§ÙØª ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙØ¹Ù„ÛŒ
        try:
            config = graphrag_service.get_config()
            return jsonify({
                'success': True,
                'config': config
            })
        except Exception as e:
            return jsonify({
                'success': False,
                'error': str(e)
            }), 500
    
    elif request.method == 'POST':
        # ØªØºÛŒÛŒØ± ØªÙ†Ø¸ÛŒÙ…Ø§Øª
        try:
            data = request.get_json()
            new_config = data.get('config', {})
            
            # Ø§Ø¹Ù…Ø§Ù„ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¬Ø¯ÛŒØ¯
            graphrag_service.set_config(**new_config)
            
            # Ø¯Ø±ÛŒØ§ÙØª ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡â€ŒØ±ÙˆØ² Ø´Ø¯Ù‡
            updated_config = graphrag_service.get_config()
            
            return jsonify({
                'success': True,
                'message': 'ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡â€ŒØ±ÙˆØ² Ø´Ø¯',
                'config': updated_config
            })
        except Exception as e:
            return jsonify({
                'success': False,
                'error': str(e)
            }), 500

@app.route('/api/enhanced_config', methods=['GET', 'POST'])
def enhanced_config_endpoint():
    """Ù…Ø¯ÛŒØ±ÛŒØª ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø³Ø±ÙˆÛŒØ³ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
    if request.method == 'GET':
        # Ø¯Ø±ÛŒØ§ÙØª ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙØ¹Ù„ÛŒ
        try:
            config = enhanced_graphrag_service.get_config()
            return jsonify({
                'success': True,
                'config': config
            })
        except Exception as e:
            return jsonify({
                'success': False,
                'error': str(e)
            }), 500
    
    elif request.method == 'POST':
        # ØªØºÛŒÛŒØ± ØªÙ†Ø¸ÛŒÙ…Ø§Øª
        try:
            data = request.get_json()
            new_config = data.get('config', {})
            
            # Ø§Ø¹Ù…Ø§Ù„ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¬Ø¯ÛŒØ¯
            enhanced_graphrag_service.set_config(**new_config)
            
            # Ø¯Ø±ÛŒØ§ÙØª ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡â€ŒØ±ÙˆØ² Ø´Ø¯Ù‡
            updated_config = enhanced_graphrag_service.get_config()
            
            return jsonify({
                'success': True,
                'message': 'ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡â€ŒØ±ÙˆØ² Ø´Ø¯',
                'config': updated_config
            })
        except Exception as e:
            return jsonify({
                'success': False,
                'error': str(e)
            }), 500

@app.route('/api/config/presets', methods=['GET'])
def config_presets():
    """Ù¾ÛŒØ´â€ŒØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù…Ø§Ø¯Ù‡"""
    presets = {
        'fast': {
            'name': 'Ø³Ø±ÛŒØ¹',
            'description': 'Ù¾Ø§Ø³Ø® Ø³Ø±ÛŒØ¹ Ø¨Ø§ Ù…Ø­Ø¯ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ú©Ù…',
            'config': {
                'max_nodes': 5,
                'max_edges': 10,
                'max_depth': 2,
                'max_paths': 3,
                'max_context_length': 1000,
                'max_answer_tokens': 500,
                'max_prompt_tokens': 2000,
                'enable_verbose_logging': False,
                'enable_biological_enrichment': False,
                'enable_smart_filtering': True
            }
        },
        'balanced': {
            'name': 'Ù…ØªÙˆØ§Ø²Ù†',
            'description': 'ØªØ¹Ø§Ø¯Ù„ Ø¨ÛŒÙ† Ø³Ø±Ø¹Øª Ùˆ Ú©ÛŒÙÛŒØª',
            'config': {
                'max_nodes': 10,
                'max_edges': 20,
                'max_depth': 3,
                'max_paths': 5,
                'max_context_length': 2000,
                'max_answer_tokens': 1000,
                'max_prompt_tokens': 4000,
                'enable_verbose_logging': True,
                'enable_biological_enrichment': True,
                'enable_smart_filtering': True
            }
        },
        'comprehensive': {
            'name': 'Ø¬Ø§Ù…Ø¹',
            'description': 'Ù¾Ø§Ø³Ø® Ú©Ø§Ù…Ù„ Ø¨Ø§ Ø¬Ø²Ø¦ÛŒØ§Øª Ø¨ÛŒØ´ØªØ±',
            'config': {
                'max_nodes': 20,
                'max_edges': 40,
                'max_depth': 4,
                'max_paths': 10,
                'max_context_length': 3000,
                'max_answer_tokens': 1500,
                'max_prompt_tokens': 6000,
                'enable_verbose_logging': True,
                'enable_biological_enrichment': True,
                'enable_smart_filtering': True
            }
        },
        'research': {
            'name': 'ØªØ­Ù‚ÛŒÙ‚Ø§ØªÛŒ',
            'description': 'Ø¨Ø±Ø§ÛŒ ØªØ­Ù‚ÛŒÙ‚Ø§Øª Ùˆ ØªØ­Ù„ÛŒÙ„ Ø¹Ù…ÛŒÙ‚',
            'config': {
                'max_nodes': 30,
                'max_edges': 60,
                'max_depth': 5,
                'max_paths': 15,
                'max_context_length': 4000,
                'max_answer_tokens': 2000,
                'max_prompt_tokens': 8000,
                'enable_verbose_logging': True,
                'enable_biological_enrichment': True,
                'enable_smart_filtering': True
            }
        }
    }
    return jsonify({'presets': presets})

@app.route('/api/compare_texts', methods=['POST'])
def compare_texts():
    try:
        data = request.get_json()
        text1 = data.get('text1', '')
        text2 = data.get('text2', '')
        method = data.get('method', 'cosine_tfidf')
        
        if not text1 or not text2:
            return jsonify({'error': 'Ù‡Ø± Ø¯Ùˆ Ù…ØªÙ† Ø¨Ø§ÛŒØ¯ ÙˆØ§Ø±Ø¯ Ø´ÙˆÙ†Ø¯'}), 400
        
        # Preprocess texts
        text1_processed = preprocess_text(text1)
        text2_processed = preprocess_text(text2)
        
        # Calculate similarity based on selected method
        similarity_score = 0
        method_name = ""
        
        if method == 'cosine_tfidf':
            similarity_score = cosine_similarity_tfidf(text1_processed, text2_processed)
            method_name = "Ø´Ø¨Ø§Ù‡Øª Ú©Ø³ÛŒÙ†ÙˆØ³ÛŒ (TF-IDF)"
        elif method == 'cosine_sbert':
            similarity_score = cosine_similarity_sbert(text1, text2)
            method_name = "Ø´Ø¨Ø§Ù‡Øª Ú©Ø³ÛŒÙ†ÙˆØ³ÛŒ (SBERT)"
        elif method == 'jaccard':
            similarity_score = jaccard_similarity(text1_processed, text2_processed)
            method_name = "Ø´Ø¨Ø§Ù‡Øª Ø¬Ø§Ú©Ø§Ø±Ø¯"
        elif method == 'levenshtein':
            similarity_score = levenshtein_similarity(text1, text2)
            method_name = "Ø´Ø¨Ø§Ù‡Øª Ù„ÙˆÙ†Ø´ØªØ§ÛŒÙ†"
        elif method == 'sequence_matcher':
            similarity_score = sequence_matcher_similarity(text1, text2)
            method_name = "Ø´Ø¨Ø§Ù‡Øª Sequence Matcher"
        elif method == 'word_overlap':
            similarity_score = word_overlap_similarity(text1_processed, text2_processed)
            method_name = "Ø´Ø¨Ø§Ù‡Øª Ù‡Ù…Ù¾ÙˆØ´Ø§Ù†ÛŒ Ú©Ù„Ù…Ø§Øª"
        else:
            return jsonify({'error': 'Ø±ÙˆØ´ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª'}), 400
        
        # Determine quality level
        quality_level = get_quality_level(similarity_score)
        
        return jsonify({
            'similarity_score': round(similarity_score, 4),
            'method_name': method_name,
            'quality_level': quality_level,
            'text1_processed': text1_processed,
            'text2_processed': text2_processed
        })
        
    except Exception as e:
        return jsonify({'error': f'Ø®Ø·Ø§ Ø¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡: {str(e)}'}), 500

@app.route('/api/compare_with_gpt', methods=['POST'])
def compare_with_gpt():
    try:
        data = request.get_json()
        text1 = data.get('text1', '')
        text2 = data.get('text2', '')
        label1 = data.get('label1', 'Ø±ÙˆØ´ Ø§ÙˆÙ„')
        label2 = data.get('label2', 'Ø±ÙˆØ´ Ø¯ÙˆÙ…')
        comparison_type = data.get('comparison_type', 'comprehensive')
        gpt_model = data.get('gpt_model', 'gpt-4o')
        
        if not text1 or not text2:
            return jsonify({'error': 'Ù‡Ø± Ø¯Ùˆ Ù…ØªÙ† Ø¨Ø§ÛŒØ¯ ÙˆØ§Ø±Ø¯ Ø´ÙˆÙ†Ø¯'}), 400
        
        # Create a comprehensive prompt for GPT
        prompt = create_gpt_comparison_prompt(text1, text2, label1, label2, comparison_type)
        
        # Check if OpenAI is available
        if openai is None:
            return jsonify({'error': 'OpenAI Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ù†ØµØ¨ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª. Ù„Ø·ÙØ§Ù‹ Ø¨Ø§ Ø¯Ø³ØªÙˆØ± pip install openai Ø¢Ù† Ø±Ø§ Ù†ØµØ¨ Ú©Ù†ÛŒØ¯.'}), 500
        
        # Call OpenAI API
        try:
            # Set the API key for this request
            openai.api_key = OPENAI_API_KEY
            
            response = openai.chat.completions.create(
                model=gpt_model,
                messages=[
                    {"role": "system", "content": "Ø´Ù…Ø§ ÛŒÚ© Ù…ØªØ®ØµØµ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª Ù…ØªÙ† Ø¯Ø± Ø­ÙˆØ²Ù‡ Ø²ÛŒØ³Øªâ€ŒÙ¾Ø²Ø´Ú©ÛŒØŒ Ú˜Ù†ØªÛŒÚ© Ùˆ Ù¾Ø²Ø´Ú©ÛŒ Ø´Ø®ØµÛŒ Ù‡Ø³ØªÛŒØ¯. ÙˆØ¸ÛŒÙÙ‡ Ø´Ù…Ø§ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¯Ùˆ Ù…ØªÙ† Ø§Ø² Ù†Ø¸Ø± Ø³Ø·Ø­ Ø¹Ù„Ù…ÛŒØŒ Ø³Ø§Ø®ØªØ§Ø± ØªØ­Ù„ÛŒÙ„ÛŒØŒ Ø¹Ù…Ù‚ Ù…ÙÙ‡ÙˆÙ…ÛŒ Ùˆ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù¾Ø°ÛŒØ±ÛŒ Ø¨Ø§Ù„ÛŒÙ†ÛŒ Ø§Ø³Øª.\n\nÙ¾Ø§Ø³Ø®ÛŒ Ú©Ù‡ Ø¯Ø§Ø±Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø¨Ø§Ø´Ø¯ØŒ Ø¨Ø§ÛŒØ¯ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø§Ù„Ø§ØªØ±ÛŒ Ø¨Ú¯ÛŒØ±Ø¯:\n- ØªØ­Ù„ÛŒÙ„ Ø¯Ù‚ÛŒÙ‚ Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø²ÛŒØ³ØªÛŒ Ùˆ Ø³ÛŒÚ¯Ù†Ø§Ù„ÛŒÙ†Ú¯ \n- Ø§Ø´Ø§Ø±Ù‡ Ø¨Ù‡ Ú˜Ù†â€ŒÙ‡Ø§ÛŒ Ø®Ø§Øµ Ø¨Ø§ Ù†Ù‚Ø´ Ø¨Ø§Ù„ÛŒÙ†ÛŒ \n- Ù¾ÛŒÙˆÙ†Ø¯ ÙˆØ§Ø¶Ø­ Ø¨ÛŒÙ† Ø¹Ù…Ù„Ú©Ø±Ø¯ Ú˜Ù† Ùˆ Ø¨ÛŒÙ…Ø§Ø±ÛŒ\n- ØªÙˆØ¶ÛŒØ­ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Ø¯Ø±Ù…Ø§Ù†ÛŒ ÛŒØ§ ØªØ´Ø®ÛŒØµÛŒ (Ù…Ø§Ù†Ù†Ø¯ Ø¯Ø§Ø±ÙˆÙ‡Ø§ÛŒ Ù‡Ø¯ÙÙ…Ù†Ø¯ØŒ Ø¨ÛŒÙˆÙ…Ø§Ø±Ú©Ø±Ù‡Ø§ØŒ Ù…Ù‡Ø§Ø±Ú©Ù†Ù†Ø¯Ù‡â€ŒÙ‡Ø§)\n- Ø³Ø§Ø®ØªØ§Ø± ØªØ­Ù„ÛŒÙ„ÛŒ Ù…Ù†Ø¸Ù… Ø´Ø§Ù…Ù„ Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ (Ù…Ø«Ù„Ø§Ù‹: Ø§Ù‡Ù…ÛŒØª Ø²ÛŒØ³ØªÛŒØŒ Ø§Ù‡Ù…ÛŒØª Ø¨Ø§Ù„ÛŒÙ†ÛŒØŒ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ØŒ Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ)\n\nØ¯Ø± Ù…Ù‚Ø§Ø¨Ù„ØŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ ÙÙ‚Ø· Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„ÛŒØŒ Ø¹Ù…ÙˆÙ…ÛŒ ÛŒØ§ ØºÛŒØ±ØªØ­Ù„ÛŒÙ„ÛŒ Ù…ÛŒâ€ŒØ¯Ù‡Ù†Ø¯ ÛŒØ§ ØµØ±ÙØ§Ù‹ ÙÙ‡Ø±Ø³ØªÛŒ Ø§Ø² Ø§Ø³Ø§Ù…ÛŒ Ù‡Ø³ØªÙ†Ø¯ØŒ Ø¨Ø§ÛŒØ¯ Ø§Ù…ØªÛŒØ§Ø² Ú©Ù…ØªØ±ÛŒ Ø¨Ú¯ÛŒØ±Ù†Ø¯.\n\nØ§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø§ÛŒØ¯ Ø¯Ù‚ÛŒÙ‚ØŒ ØªØ­Ù„ÛŒÙ„ÛŒ Ùˆ Ø¨Ø§ ØªÙ…Ø±Ú©Ø² Ø¨Ø± Ú©ÛŒÙÛŒØª Ø¹Ù„Ù…ÛŒØŒ Ø¹Ù…Ù‚ Ù…Ø­ØªÙˆØ§ Ùˆ Ø§Ø±Ø²Ø´ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯. Ù‡Ø¯Ù Ø§Ù†ØªØ®Ø§Ø¨ Ù¾Ø§Ø³Ø®ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø§Ø±Ø²Ø´ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù¾Ú˜ÙˆÙ‡Ø´Ú¯Ø± ÛŒØ§ Ù…ØªØ®ØµØµ Ø­ÙˆØ²Ù‡ Ø²ÛŒØ³Øªâ€ŒÙ¾Ø²Ø´Ú©ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=2000
            )
            
            gpt_response = response.choices[0].message.content
            
            # Parse the GPT response
            parsed_result = parse_gpt_comparison_response(gpt_response, label1, label2, comparison_type)
            parsed_result['gpt_model'] = gpt_model
            
            return jsonify(parsed_result)
            
        except Exception as e:
            return jsonify({'error': f'Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ {gpt_model}: {str(e)}'}), 500
        
    except Exception as e:
        return jsonify({'error': f'Ø®Ø·Ø§ Ø¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡: {str(e)}'}), 500

def create_gpt_comparison_prompt(text1, text2, label1, label2, comparison_type):
    """Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø±Ø§Ù…Ù¾ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ GPT-4o"""
    
    comparison_focus = {
        'comprehensive': 'Ú©ÛŒÙÛŒØª Ú©Ù„ÛŒØŒ Ø¯Ù‚ØªØŒ Ø¬Ø§Ù…Ø¹ÛŒØªØŒ ÙˆØ¶ÙˆØ­ØŒ Ùˆ Ù…Ø±ØªØ¨Ø· Ø¨ÙˆØ¯Ù†',
        'practical_specialized': 'Ø¹Ù…Ù„ÛŒ Ø¨ÙˆØ¯Ù†ØŒ ØªØ®ØµØµÛŒ Ø¨ÙˆØ¯Ù†ØŒ Ùˆ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ Ø¨ÙˆØ¯Ù† Ù…Ø­ØªÙˆØ§',
        'accuracy': 'Ø¯Ù‚Øª Ùˆ ØµØ­Øª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡',
        'completeness': 'Ø¬Ø§Ù…Ø¹ÛŒØª Ùˆ Ú©Ø§Ù…Ù„ Ø¨ÙˆØ¯Ù† Ù¾Ø§Ø³Ø®',
        'clarity': 'ÙˆØ¶ÙˆØ­ Ùˆ Ù‚Ø§Ø¨Ù„ ÙÙ‡Ù… Ø¨ÙˆØ¯Ù† Ù…ØªÙ†',
        'relevance': 'Ù…Ø±ØªØ¨Ø· Ø¨ÙˆØ¯Ù† Ø¨Ø§ Ø³ÙˆØ§Ù„ Ø§ØµÙ„ÛŒ'
    }
    
    focus = comparison_focus.get(comparison_type, comparison_focus['comprehensive'])
    
    prompt = f"""
    Ù„Ø·ÙØ§Ù‹ Ø¯Ùˆ Ù…ØªÙ† Ø²ÛŒØ± Ø±Ø§ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø¯Ù‚ÛŒÙ‚ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯. ØªÙˆØ¬Ù‡ ÙˆÛŒÚ˜Ù‡ Ø¨Ù‡ Ø¹Ù…Ù„ÛŒ Ø¨ÙˆØ¯Ù† Ùˆ ØªØ®ØµØµÛŒ Ø¨ÙˆØ¯Ù† Ù…Ø­ØªÙˆØ§ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯. Ø§Ø² Ø§Ø±Ø§Ø¦Ù‡ Ù‡Ø±Ú¯ÙˆÙ†Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø¹Ø¯Ø¯ÛŒ Ø®ÙˆØ¯Ø¯Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯ Ùˆ ÙÙ‚Ø· ØªØ­Ù„ÛŒÙ„ Ù…ØªÙ†ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯.
    
    **Ù…ØªÙ† Ø§ÙˆÙ„ ({label1}):**
    {text1}
    
    **Ù…ØªÙ† Ø¯ÙˆÙ… ({label2}):**
    {text2}
    
    **Ù†ÙˆØ¹ Ù…Ù‚Ø§ÛŒØ³Ù‡:** {focus}
    
    **Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ù‡Ù…:**
    1. **Ø¹Ù…Ù„ÛŒ Ø¨ÙˆØ¯Ù†**: Ù…ØªÙ† Ø¨Ø§ÛŒØ¯ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ Ùˆ Ù‚Ø§Ø¨Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡Ø¯
    2. **ØªØ®ØµØµÛŒ Ø¨ÙˆØ¯Ù†**: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§ØµØ·Ù„Ø§Ø­Ø§Øª ØªØ®ØµØµÛŒ Ùˆ Ù…ÙØ§Ù‡ÛŒÙ… Ø¯Ù‚ÛŒÙ‚ Ø¹Ù„Ù…ÛŒ
    3. **Ø¯Ù‚Øª Ø¹Ù„Ù…ÛŒ**: ØµØ­Øª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ùˆ Ø§Ø³ØªÙ†Ø§Ø¯ Ø¨Ù‡ Ù…ÙØ§Ù‡ÛŒÙ… Ø¹Ù„Ù…ÛŒ
    4. **Ø¬Ø§Ù…Ø¹ÛŒØª**: Ù¾ÙˆØ´Ø´ Ú©Ø§Ù…Ù„ Ø¬Ù†Ø¨Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ù…ÙˆØ¶ÙˆØ¹
    5. **ÙˆØ¶ÙˆØ­**: Ù‚Ø§Ø¨Ù„ ÙÙ‡Ù… Ø¨ÙˆØ¯Ù† Ø¨Ø±Ø§ÛŒ Ù…Ø®Ø§Ø·Ø¨ ØªØ®ØµØµÛŒ
    
    Ù„Ø·ÙØ§Ù‹ ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ¯ Ø±Ø§ Ø¯Ø± Ù‚Ø§Ù„Ø¨ Ø²ÛŒØ± Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯ Ùˆ Ø§Ø² Ø§Ù…ØªÛŒØ§Ø² Ø¹Ø¯Ø¯ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†ÛŒØ¯:
    
    **Ø®Ù„Ø§ØµÙ‡ Ù…Ù‚Ø§ÛŒØ³Ù‡:**
    [ÛŒÚ© Ø®Ù„Ø§ØµÙ‡ Ú©ÙˆØªØ§Ù‡ Ø§Ø² ØªÙØ§ÙˆØªâ€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø§ ØªÙ…Ø±Ú©Ø² Ø¨Ø± Ø¹Ù…Ù„ÛŒ Ø¨ÙˆØ¯Ù† Ùˆ ØªØ®ØµØµÛŒ Ø¨ÙˆØ¯Ù†]
    
    **ØªØ­Ù„ÛŒÙ„ Ú©Ù„ÛŒ:**
    [ØªØ­Ù„ÛŒÙ„ Ù…ØªÙ†ÛŒ Ø§Ø² Ù†Ù‚Ø§Ø· Ù‚ÙˆØª Ùˆ Ø¶Ø¹Ù Ú©Ù„ÛŒ Ù‡Ø± Ø¯Ùˆ Ù…ØªÙ† Ùˆ Ø¯Ù„ÛŒÙ„ Ø¨Ø±ØªØ±ÛŒ Ù†Ø³Ø¨ÛŒ]
    
    **Ù†Ù‚Ø§Ø· Ù‚ÙˆØª {label1}:**
    [Ù„ÛŒØ³Øª Ù†Ù‚Ø§Ø· Ù‚ÙˆØª Ø¨Ø§ ØªÙ…Ø±Ú©Ø² Ø¨Ø± Ø¬Ù†Ø¨Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ Ùˆ ØªØ®ØµØµÛŒ]
    
    **Ù†Ù‚Ø§Ø· Ù‚ÙˆØª {label2}:**
    [Ù„ÛŒØ³Øª Ù†Ù‚Ø§Ø· Ù‚ÙˆØª Ø¨Ø§ ØªÙ…Ø±Ú©Ø² Ø¨Ø± Ø¬Ù†Ø¨Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ Ùˆ ØªØ®ØµØµÛŒ]
    
    **Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù {label1}:**
    [Ù„ÛŒØ³Øª Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù Ø§Ø² Ù†Ø¸Ø± Ø¹Ù…Ù„ÛŒ Ø¨ÙˆØ¯Ù† Ùˆ ØªØ®ØµØµÛŒ Ø¨ÙˆØ¯Ù†]
    
    **Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù {label2}:**
    [Ù„ÛŒØ³Øª Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù Ø§Ø² Ù†Ø¸Ø± Ø¹Ù…Ù„ÛŒ Ø¨ÙˆØ¯Ù† Ùˆ ØªØ®ØµØµÛŒ Ø¨ÙˆØ¯Ù†]
    
    **ØªÙˆØµÛŒÙ‡ Ù†Ù‡Ø§ÛŒÛŒ:**
    [ØªÙˆØµÛŒÙ‡ Ú©Ø¯Ø§Ù… Ø±ÙˆØ´ Ø¨Ù‡ØªØ± Ø§Ø³Øª Ø¨Ø§ ØªØ£Ú©ÛŒØ¯ Ø¨Ø± Ø¹Ù…Ù„ÛŒ Ø¨ÙˆØ¯Ù† Ùˆ ØªØ®ØµØµÛŒ Ø¨ÙˆØ¯Ù†]
    """
    
    return prompt

def parse_gpt_comparison_response(response, label1, label2, comparison_type):
    """ØªØ¬Ø²ÛŒÙ‡ Ùˆ ØªØ­Ù„ÛŒÙ„ Ù¾Ø§Ø³Ø® GPT-4o - ÙÙ‚Ø· ØªØ­Ù„ÛŒÙ„ Ù…ØªÙ†ÛŒ Ø¨Ø¯ÙˆÙ† Ø§Ù…ØªÛŒØ§Ø² Ø¹Ø¯Ø¯ÛŒ"""
    import re
    
    # Split response into sections
    sections = response.split('\n\n')
    
    summary = ""
    analysis = ""
    strengths1 = ""
    strengths2 = ""
    weaknesses1 = ""
    weaknesses2 = ""
    recommendation = ""
    
    for section in sections:
        if "Ø®Ù„Ø§ØµÙ‡ Ù…Ù‚Ø§ÛŒØ³Ù‡" in section:
            summary = section.replace("**Ø®Ù„Ø§ØµÙ‡ Ù…Ù‚Ø§ÛŒØ³Ù‡:**", "").strip()
        elif "ØªØ­Ù„ÛŒÙ„ Ú©Ù„ÛŒ" in section:
            analysis = section.replace("**ØªØ­Ù„ÛŒÙ„ Ú©Ù„ÛŒ:**", "").strip()
        elif "ØªØ­Ù„ÛŒÙ„" in section and not analysis:
            # Ø¯Ø± ØµÙˆØ±Øª Ù†Ø¨ÙˆØ¯ Ø¹Ù†ÙˆØ§Ù† Ø¯Ù‚ÛŒÙ‚ Â«ØªØ­Ù„ÛŒÙ„ Ú©Ù„ÛŒÂ»ØŒ Ù‡Ø± Ø¨Ø®Ø´ Ø­Ø§ÙˆÛŒ ÙˆØ§Ú˜Ù‡ ØªØ­Ù„ÛŒÙ„ Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ØªØ­Ù„ÛŒÙ„ Ú©Ù„ÛŒ Ø¯Ø± Ù†Ø¸Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ…
            analysis = re.sub(r"^\*\*[^:]+:\*\*", "", section).strip()
        elif f"Ù†Ù‚Ø§Ø· Ù‚ÙˆØª {label1}" in section:
            strengths1 = section.replace(f"**Ù†Ù‚Ø§Ø· Ù‚ÙˆØª {label1}:**", "").strip()
        elif f"Ù†Ù‚Ø§Ø· Ù‚ÙˆØª {label2}" in section:
            strengths2 = section.replace(f"**Ù†Ù‚Ø§Ø· Ù‚ÙˆØª {label2}:**", "").strip()
        elif f"Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù {label1}" in section:
            weaknesses1 = section.replace(f"**Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù {label1}:**", "").strip()
        elif f"Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù {label2}" in section:
            weaknesses2 = section.replace(f"**Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù {label2}:**", "").strip()
        elif "ØªÙˆØµÛŒÙ‡ Ù†Ù‡Ø§ÛŒÛŒ" in section:
            recommendation = section.replace("**ØªÙˆØµÛŒÙ‡ Ù†Ù‡Ø§ÛŒÛŒ:**", "").strip()
    
    # If sections are empty, use the full response
    if not summary:
        summary = response[:200] + "..." if len(response) > 200 else response
    if not analysis:
        # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÛŒÚ© ØªØ­Ù„ÛŒÙ„ Ú©Ù„ÛŒ Ø§Ø² Ø±ÙˆÛŒ Ú©Ù„ Ù¾Ø§Ø³Ø® Ø¯Ø± ØµÙˆØ±Øª Ù†Ø¨ÙˆØ¯ Ø¨Ø®Ø´ Ù…Ø´Ø®Øµ
        analysis = ""
    
    return {
        'summary': summary,
        'analysis': analysis,
        'strengths1': strengths1,
        'strengths2': strengths2,
        'weaknesses1': weaknesses1,
        'weaknesses2': weaknesses2,
        'recommendation': recommendation,
        'label1': label1,
        'label2': label2,
        'comparison_type': comparison_type
    }

def preprocess_text(text):
    """Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡"""
    # Convert to lowercase
    text = text.lower()
    
    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)
    
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def cosine_similarity_tfidf(text1, text2):
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ú©Ø³ÛŒÙ†ÙˆØ³ÛŒ Ø¨Ø§ TF-IDF"""
    try:
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform([text1, text2])
        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
        return float(similarity)
    except:
        return 0.0

def cosine_similarity_sbert(text1, text2):
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ú©Ø³ÛŒÙ†ÙˆØ³ÛŒ Ø¨Ø§ SBERT"""
    try:
        if sentence_transformer is None:
            return 0.0
        
        embeddings = sentence_transformer.encode([text1, text2])
        similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
        return float(similarity)
    except:
        return 0.0

def jaccard_similarity(text1, text2):
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø¬Ø§Ú©Ø§Ø±Ø¯"""
    try:
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        if union == 0:
            return 0.0
        
        return intersection / union
    except:
        return 0.0

def levenshtein_similarity(text1, text2):
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ù„ÙˆÙ†Ø´ØªØ§ÛŒÙ†"""
    try:
        def levenshtein_distance(s1, s2):
            if len(s1) < len(s2):
                return levenshtein_distance(s2, s1)
            
            if len(s2) == 0:
                return len(s1)
            
            previous_row = list(range(len(s2) + 1))
            for i, c1 in enumerate(s1):
                current_row = [i + 1]
                for j, c2 in enumerate(s2):
                    insertions = previous_row[j + 1] + 1
                    deletions = current_row[j] + 1
                    substitutions = previous_row[j] + (c1 != c2)
                    current_row.append(min(insertions, deletions, substitutions))
                previous_row = current_row
            
            return previous_row[-1]
        
        distance = levenshtein_distance(text1, text2)
        max_len = max(len(text1), len(text2))
        
        if max_len == 0:
            return 1.0
        
        similarity = 1 - (distance / max_len)
        return similarity
    except:
        return 0.0

def sequence_matcher_similarity(text1, text2):
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø¨Ø§ Sequence Matcher"""
    try:
        similarity = SequenceMatcher(None, text1, text2).ratio()
        return similarity
    except:
        return 0.0

def word_overlap_similarity(text1, text2):
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‡Ù…Ù¾ÙˆØ´Ø§Ù†ÛŒ Ú©Ù„Ù…Ø§Øª"""
    try:
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        if len(words1) == 0 and len(words2) == 0:
            return 1.0
        
        intersection = len(words1.intersection(words2))
        min_length = min(len(words1), len(words2))
        
        if min_length == 0:
            return 0.0
        
        return intersection / min_length
    except:
        return 0.0

def get_quality_level(similarity_score):
    """ØªØ¹ÛŒÛŒÙ† Ø³Ø·Ø­ Ú©ÛŒÙÛŒØª Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ù…Ø±Ù‡ Ø´Ø¨Ø§Ù‡Øª"""
    if similarity_score >= 0.9:
        return "Ø¹Ø§Ù„ÛŒ"
    elif similarity_score >= 0.8:
        return "Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ¨"
    elif similarity_score >= 0.7:
        return "Ø®ÙˆØ¨"
    elif similarity_score >= 0.6:
        return "Ù…ØªÙˆØ³Ø·"
    elif similarity_score >= 0.5:
        return "Ø¶Ø¹ÛŒÙ"
    else:
        return "Ø®ÛŒÙ„ÛŒ Ø¶Ø¹ÛŒÙ"

if __name__ == '__main__':
    # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ templates Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯
    os.makedirs('templates', exist_ok=True)
    os.makedirs('static', exist_ok=True)
    os.makedirs('static/css', exist_ok=True)
    os.makedirs('static/js', exist_ok=True)
    
    app.run(debug=True, host='0.0.0.0', port=5000) 
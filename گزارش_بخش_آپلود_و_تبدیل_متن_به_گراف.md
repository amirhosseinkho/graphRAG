# گزارش کامل بخش آپلود گراف و تبدیل متن به گراف

## فهرست مطالب
1. [مقدمه](#مقدمه)
2. [بخش آپلود گراف](#بخش-آپلود-گراف)
3. [تبدیل متن به گراف](#تبدیل-متن-به-گراف)
4. [روش‌های مختلف استخراج](#روش‌های-مختلف-استخراج)
5. [تبدیل URL به متن و سپس به گراف](#تبدیل-url-به-متن-و-سپس-به-گراف)
6. [معماری سیستم](#معماری-سیستم)
7. [جریان کار (Workflow)](#جریان-کار-workflow)
8. [ویژگی‌های پیشرفته](#ویژگی‌های-پیشرفته)
9. [نتیجه‌گیری](#نتیجه‌گیری)

---

## مقدمه

این سیستم یک پلتفرم جامع برای مدیریت و ساخت گراف‌های دانش است که قابلیت‌های زیر را ارائه می‌دهد:

- **آپلود گراف‌های از پیش ساخته شده** در فرمت‌های مختلف
- **تبدیل متن به گراف دانش** با استفاده از تکنیک‌های پیشرفته NLP
- **استخراج خودکار از URL** و تبدیل به گراف
- **چندین روش استخراج** برای نیازهای مختلف

---

## بخش آپلود گراف

### 1.1. فرمت‌های پشتیبانی شده

سیستم از فرمت‌های زیر پشتیبانی می‌کند:

| فرمت | توضیحات | کاربرد |
|------|---------|--------|
| **PKL** | فایل‌های Python Pickle | گراف‌های از پیش پردازش شده با NetworkX |
| **SIF** | Simple Interaction Format | فرمت استاندارد تعاملات زیستی |
| **TSV** | Tab-Separated Values | داده‌های جدا شده با Tab |
| **CSV** | Comma-Separated Values | داده‌های جدا شده با کاما |
| **TXT** | فایل‌های متنی ساده | متن خام |
| **GZ** | فایل‌های فشرده Gzip | فایل‌های فشرده شده |

### 1.2. ساختار فایل SIF

برای فایل‌های SIF، هر خط باید شامل سه ستون باشد:

```
NodeA    interaction_type    NodeB
```

**مثال:**
```
TP53    regulates    CDKN1A
BRCA1    interacts_with    TP53
```

### 1.3. ساختار فایل TSV/CSV

ستون‌های مورد نیاز:
- **ستون 1:** نود منبع (Source Node)
- **ستون 2:** نوع رابطه (Relationship Type)
- **ستون 3:** نود هدف (Target Node)

### 1.4. API آپلود

**Endpoint:** `/api/upload_graph`  
**Method:** `POST`  
**Content-Type:** `multipart/form-data`

**پارامترها:**
- `graph_file`: فایل گراف برای آپلود

**پاسخ موفق:**
```json
{
  "success": true,
  "message": "فایل {filename} با موفقیت آپلود شد",
  "filename": "20240217_143022_example.pkl",
  "filepath": "uploaded_graphs/20240217_143022_example.pkl"
}
```

**پاسخ خطا:**
```json
{
  "success": false,
  "error": "نوع فایل مجاز نیست"
}
```

### 1.5. پردازش فایل‌های فشرده

سیستم به صورت خودکار فایل‌های `.gz` را:
1. دریافت می‌کند
2. از حالت فشرده خارج می‌کند
3. فایل اصلی را حذف می‌کند
4. فایل باز شده را ذخیره می‌کند

---

## تبدیل متن به گراف

### 2.1. مراحل کلی

فرآیند تبدیل متن به گراف شامل مراحل زیر است:

```
متن ورودی
    ↓
پیش‌پردازش (اختیاری)
    ↓
استخراج موجودیت‌ها و روابط
    ↓
ساخت گراف NetworkX
    ↓
پس‌پردازش (Entity Resolution, Weighting)
    ↓
ذخیره گراف
```

### 2.2. API تبدیل متن به گراف

**Endpoint:** `/api/text_to_graph`  
**Method:** `POST`  
**Content-Type:** `application/json`

**پارامترهای اصلی:**
```json
{
  "text": "متن ورودی...",
  "method": "spacy",
  "max_entities": 100,
  "max_relationships": 200,
  "llm_model": "mistralai/Mistral-7B-Instruct-v0.2",
  "confidence_threshold": 0.5,
  "enable_entity_resolution": true,
  "enable_relationship_weighting": true,
  "enable_preprocessing": false,
  "language": "auto"
}
```

**پاسخ موفق:**
```json
{
  "success": true,
  "message": "گراف با موفقیت ساخته شد",
  "filename": "20240217_143022_text_graph.pkl",
  "filepath": "uploaded_graphs/20240217_143022_text_graph.pkl",
  "stats": {
    "num_nodes": 45,
    "num_edges": 78,
    "num_entities": 45,
    "num_relationships": 78
  },
  "extraction_method": "spacy",
  "graph_data": {
    "nodes": [...],
    "edges": [...]
  }
}
```

---

## روش‌های مختلف استخراج

سیستم از **14 روش مختلف** برای استخراج موجودیت‌ها و روابط پشتیبانی می‌کند:

### 3.1. روش‌های پایه

#### 3.1.1. Simple (Rule-based)
- **توضیحات:** استخراج بر اساس الگوهای از پیش تعریف شده
- **سرعت:** بسیار سریع
- **دقت:** متوسط
- **کاربرد:** متن‌های ساختاریافته و ساده
- **الگوهای پشتیبانی شده:**
  - الگوهای ژن (Gene patterns): `[A-Z][A-Z0-9]+`
  - الگوهای بیماری (Disease keywords)
  - الگوهای دارو (Compound patterns)
  - الگوهای رابطه: `X regulates Y`, `X interacts with Y`, `X treats Y`

#### 3.1.2. spaCy
- **توضیحات:** استفاده از کتابخانه spaCy برای پردازش زبان طبیعی
- **سرعت:** سریع
- **دقت:** خوب
- **کاربرد:** متن‌های انگلیسی و ساختاریافته
- **ویژگی‌ها:**
  - استخراج موجودیت‌های نام‌گذاری شده (NER)
  - تحلیل وابستگی (Dependency Parsing)
  - استخراج روابط Subject-Verb-Object

#### 3.1.3. spaCy SVO Enhanced
- **توضیحات:** نسخه پیشرفته spaCy با تمرکز بر استخراج روابط SVO
- **سرعت:** سریع
- **دقت:** خوب تا عالی
- **کاربرد:** متن‌هایی که نیاز به استخراج دقیق روابط دارند

### 3.2. روش‌های مبتنی بر LLM

#### 3.2.1. LLM (Single-pass)
- **توضیحات:** استفاده از مدل‌های زبانی بزرگ برای استخراج
- **سرعت:** متوسط تا کند
- **دقت:** عالی
- **مدل‌های پشتیبانی شده:**
  - `mistralai/Mistral-7B-Instruct-v0.2` (پیشنهادی)
  - `HuggingFaceH4/zephyr-7b-beta`
  - `microsoft/Phi-3-mini-4k-instruct`
  - `Qwen/Qwen2.5-7B-Instruct`
  - `TinyLlama/TinyLlama-1.1B-Chat-v1.0`
  - `google/gemma-2b-it`
  - `meta-llama/Llama-3.2-3B-Instruct`

#### 3.2.2. LLM Multi-pass
- **توضیحات:** استخراج با چندین پاس برای افزایش recall
- **سرعت:** کند
- **دقت:** بسیار عالی
- **پارامترها:**
  - `max_gleanings`: تعداد پاس‌های تکرار (پیش‌فرض: 2)

**مزایا:**
- استخراج موجودیت‌های از دست رفته در پاس اول
- بهبود recall
- مناسب برای متن‌های پیچیده

### 3.3. روش‌های ترکیبی

#### 3.3.1. Hybrid
- **توضیحات:** ترکیب چندین روش برای بهترین نتیجه
- **روش‌های قابل ترکیب:**
  - spaCy
  - LLM
  - spaCy SVO Enhanced
  - LLM Multi-pass

**استراتژی ترکیب:**
1. استخراج با هر روش
2. ادغام نتایج
3. حذف تکراری‌ها
4. رتبه‌بندی بر اساس confidence

### 3.4. روش‌های تخصصی

#### 3.4.1. Persian
- **توضیحات:** استخراج از متن فارسی با مدل‌های ParsBERT و mT5
- **سرعت:** متوسط
- **دقت:** خوب برای فارسی
- **ویژگی‌ها:**
  - پشتیبانی از زبان فارسی
  - تشخیص خودکار زبان
  - Coreference Resolution (اختیاری)

**مدل‌های استفاده شده:**
- ParsBERT برای NER
- mT5 برای استخراج روابط

#### 3.4.2. Span-based
- **توضیحات:** استخراج مبتنی بر Span با BioBERT/SciBERT
- **سرعت:** متوسط
- **دقت:** عالی برای متون علمی
- **مدل‌های پشتیبانی شده:**
  - BioBERT: برای موجودیت‌های زیست‌پزشکی
  - SciBERT: برای متون علمی
  - Auto: انتخاب خودکار

#### 3.4.3. With Coreference
- **توضیحات:** استخراج با حل ارجاعات (Coreference Resolution)
- **سرعت:** متوسط
- **دقت:** عالی
- **کاربرد:** متن‌هایی با ارجاعات زیاد

**مثال:**
```
"TP53 is a tumor suppressor. It regulates cell cycle."
→ "TP53 regulates cell cycle"
```

#### 3.4.4. Long Text
- **توضیحات:** پردازش متن‌های طولانی با chunking
- **سرعت:** متوسط
- **دقت:** خوب
- **استراتژی‌های Chunking:**
  - Smart: ترکیب پاراگراف و جمله
  - Sliding Window: با overlap
  - Sentence: بر اساس جملات
  - Paragraph: بر اساس پاراگراف

**پارامترها:**
- `chunking_strategy`: استراتژی chunking
- `chunk_overlap`: نسبت overlap (0.0 تا 1.0)
- `max_tokens`: حداکثر توکن در هر chunk

### 3.5. روش‌های پیشرفته

#### 3.5.1. Joint ER (Entity-Relation Extraction)
- **توضیحات:** استخراج همزمان موجودیت و رابطه با Graph Structure Learning
- **سرعت:** کند
- **دقت:** بسیار عالی
- **ویژگی‌ها:**
  - بهینه‌سازی ساختار گراف در حین استخراج
  - یادگیری ساختار گراف
  - تکرار برای بهبود ساختار

**پارامترها:**
- `structure_iterations`: تعداد تکرار برای بهینه‌سازی (پیش‌فرض: 3)

#### 3.5.2. Autoregressive
- **توضیحات:** تولید گراف به صورت sequential با Transformer
- **سرعت:** کند
- **دقت:** عالی
- **ویژگی‌ها:**
  - تولید تدریجی گراف
  - استفاده از Transformer
  - مناسب برای متن‌های پیچیده

**پارامترها:**
- `max_generation_length`: حداکثر طول تولید (پیش‌فرض: 2048)

#### 3.5.3. EDC Framework (Extract-Define-Canonicalize)
- **توضیحات:** روش سه مرحله‌ای برای استخراج، تعریف و استانداردسازی
- **سرعت:** متوسط
- **دقت:** عالی
- **مراحل:**
  1. Extract: استخراج موجودیت‌ها و روابط
  2. Define: تعریف و استانداردسازی
  3. Canonicalize: یکسان‌سازی

**ویژگی‌ها:**
- استفاده از RAG برای بهبود استخراج (اختیاری)
- استانداردسازی خودکار

#### 3.5.4. Incremental
- **توضیحات:** ساخت تدریجی گراف برای متن‌های طولانی
- **سرعت:** متوسط
- **دقت:** خوب
- **ویژگی‌ها:**
  - پردازش chunk به chunk
  - ادغام تدریجی نتایج
  - مناسب برای متن‌های بسیار طولانی

**پارامترها:**
- `chunk_size`: اندازه هر chunk (پیش‌فرض: 500)
- `overlap`: overlap بین chunkها (پیش‌فرض: 100)
- `base_method`: روش پایه (spacy/simple/llm)

---

## تبدیل URL به متن و سپس به گراف

### 4.1. مراحل کلی

```
URL ورودی
    ↓
تشخیص نوع URL (ویکی‌پدیا یا عادی)
    ↓
استخراج متن از URL
    ↓
پیش‌پردازش متن
    ↓
تبدیل متن به گراف (با روش انتخابی)
    ↓
ذخیره گراف
```

### 4.2. استخراج از URL عادی

**ماژول:** `url_extractor.py`

**ویژگی‌ها:**
- استخراج متن از HTML با BeautifulSoup
- حذف محتوای غیرضروری (اسکریپت‌ها، استایل‌ها، تبلیغات)
- پاکسازی متن
- محدودیت طول (max_length)

**پارامترها:**
- `url`: URL صفحه وب
- `timeout`: زمان انتظار (پیش‌فرض: 30 ثانیه)
- `clean_content`: پاکسازی محتوا (پیش‌فرض: True)
- `max_length`: حداکثر طول متن (پیش‌فرض: 10000)

### 4.3. استخراج تخصصی از ویکی‌پدیا

**ماژول:** `wikipedia_extractor.py`

**ویژگی‌های خاص:**
- استخراج از API ویکی‌پدیا
- استخراج Infobox
- استخراج Categories
- استخراج لینک‌ها
- استخراج موجودیت‌ها و روابط از ساختار ویکی‌پدیا
- پشتیبانی از زبان فارسی و انگلیسی

**متدهای استخراج:**

1. **استخراج از API:**
   - استفاده از Wikipedia API
   - دریافت متن کامل صفحه
   - دریافت metadata

2. **استخراج از HTML:**
   - Parsing مستقیم HTML
   - استخراج محتوای اصلی
   - حذف navigation و sidebar

3. **استخراج تخصصی:**
   - استخراج Infobox به عنوان موجودیت‌ها
   - استخراج Categories به عنوان روابط
   - استخراج لینک‌ها به عنوان روابط

**پارامترها:**
- `language`: زبان (fa/en)
- `use_wikipedia_extraction`: استفاده از استخراج تخصصی (پیش‌فرض: True)

### 4.4. جریان کار در `process_url_to_graph`

```python
def process_url_to_graph(url, method, use_wikipedia_extraction=True, ...):
    # 1. بررسی معتبر بودن URL
    if not is_valid_url(url):
        raise ValueError("URL نامعتبر است")
    
    # 2. تشخیص ویکی‌پدیا
    is_wikipedia = 'wikipedia.org' in url.lower()
    
    # 3. استخراج متن
    if is_wikipedia and use_wikipedia_extraction:
        # استخراج تخصصی ویکی‌پدیا
        wiki_extractor = WikipediaExtractor(language='fa' or 'en')
        wiki_data = wiki_extractor.extract_from_url(url)
        text = wiki_data.get("text", "")
        
        # Fallback به استخراج عادی در صورت خطا
        if not text:
            text = extract_text_from_url(url, clean_content=True)
    else:
        # استخراج عادی
        text = extract_text_from_url(url, clean_content=True)
    
    # 4. تبدیل متن به گراف
    return self.process_text_to_graph(text, method, ...)
```

### 4.5. ادغام داده‌های ویکی‌پدیا

اگر از استخراج تخصصی ویکی‌پدیا استفاده شود:

1. **موجودیت‌های ویکی‌پدیا:**
   - از Infobox استخراج می‌شوند
   - با موجودیت‌های استخراج شده از متن ادغام می‌شوند
   - تکراری‌ها حذف می‌شوند

2. **روابط ویکی‌پدیا:**
   - از Categories و لینک‌ها استخراج می‌شوند
   - با روابط استخراج شده از متن ادغام می‌شوند

3. **Metadata:**
   - عنوان صفحه
   - زبان
   - تاریخ آخرین ویرایش

---

## معماری سیستم

### 5.1. ساختار فایل‌ها

```
tir/
├── web_app.py                    # اپلیکیشن Flask اصلی
├── text_to_graph_service.py      # سرویس تبدیل متن به گراف
├── url_extractor.py              # استخراج از URL عادی
├── wikipedia_extractor.py         # استخراج از ویکی‌پدیا
├── templates/
│   └── upload_graph.html         # رابط کاربری آپلود
└── static/
    └── js/
        └── upload.js             # JavaScript برای آپلود
```

### 5.2. کلاس‌های اصلی

#### 5.2.1. TextToGraphService

**مسئولیت‌ها:**
- مدیریت روش‌های مختلف استخراج
- ساخت گراف NetworkX
- پس‌پردازش گراف
- ذخیره گراف

**متدهای کلیدی:**
- `extract()`: انتخاب و اجرای روش استخراج
- `process_text_to_graph()`: پردازش کامل متن به گراف
- `process_url_to_graph()`: پردازش URL به گراف
- `build_graph()`: ساخت گراف از نتایج استخراج
- `save_graph()`: ذخیره گراف

#### 5.2.2. WikipediaExtractor

**مسئولیت‌ها:**
- استخراج از ویکی‌پدیا
- استخراج Infobox
- استخراج Categories
- استخراج لینک‌ها

#### 5.2.3. URL Extractor Functions

**توابع:**
- `extract_text_from_url()`: استخراج متن از URL
- `is_valid_url()`: بررسی معتبر بودن URL
- `_extract_clean_content()`: استخراج محتوای پاک شده

### 5.3. جریان داده

```
User Input (Text/URL)
    ↓
web_app.py (/api/text_to_graph)
    ↓
TextToGraphService.process_url_to_graph() یا process_text_to_graph()
    ↓
URL Extractor (اگر URL)
    ↓
TextToGraphService.extract() (با method انتخابی)
    ↓
Extraction Method (simple/spacy/llm/...)
    ↓
Extraction Result (entities + relationships)
    ↓
TextToGraphService.build_graph()
    ↓
Post-processing (Entity Resolution, Weighting)
    ↓
Save Graph (PKL file)
    ↓
Response to User
```

---

## جریان کار (Workflow)

### 6.1. آپلود گراف

```
1. کاربر فایل را انتخاب می‌کند
   ↓
2. بررسی نوع فایل (allowed_file)
   ↓
3. آپلود فایل به uploaded_graphs/
   ↓
4. اگر فایل .gz است، از حالت فشرده خارج می‌شود
   ↓
5. بازگشت اطلاعات فایل به کاربر
   ↓
6. فایل در لیست گراف‌های موجود نمایش داده می‌شود
```

### 6.2. تبدیل متن به گراف

```
1. کاربر متن یا URL را وارد می‌کند
   ↓
2. اگر URL است:
   - استخراج متن از URL
   - تشخیص ویکی‌پدیا
   - استخراج تخصصی (اگر ویکی‌پدیا)
   ↓
3. انتخاب روش استخراج
   ↓
4. پیش‌پردازش متن (اختیاری)
   ↓
5. استخراج موجودیت‌ها و روابط
   ↓
6. ساخت گراف NetworkX
   ↓
7. پس‌پردازش:
   - Entity Resolution (اختیاری)
   - Relationship Weighting (اختیاری)
   - حذف نودهای ایزوله (اختیاری)
   ↓
8. ذخیره گراف
   ↓
9. بازگشت اطلاعات گراف به کاربر
```

### 6.3. تبدیل URL به گراف

```
1. کاربر URL را وارد می‌کند
   ↓
2. بررسی معتبر بودن URL
   ↓
3. تشخیص نوع URL:
   - ویکی‌پدیا → استخراج تخصصی
   - عادی → استخراج عادی
   ↓
4. استخراج متن:
   - ویکی‌پدیا: WikipediaExtractor.extract_from_url()
   - عادی: extract_text_from_url()
   ↓
5. ادغام داده‌های ویکی‌پدیا (اگر ویکی‌پدیا):
   - موجودیت‌های Infobox
   - روابط Categories
   ↓
6. تبدیل متن به گراف (مثل جریان کار تبدیل متن)
   ↓
7. ذخیره و بازگشت
```

---

## ویژگی‌های پیشرفته

### 7.1. Entity Resolution

**هدف:** ادغام موجودیت‌های مشابه

**الگوریتم:**
- محاسبه شباهت بین موجودیت‌ها
- استفاده از threshold برای تشخیص مشابه بودن
- ادغام موجودیت‌های مشابه

**پارامترها:**
- `enable_entity_resolution`: فعال/غیرفعال (پیش‌فرض: True)
- `similarity_threshold`: آستانه شباهت (پیش‌فرض: 0.8)

### 7.2. Relationship Weighting

**هدف:** وزن‌دهی روابط بر اساس تکرار و confidence

**فرمول:**
```
weight = frequency × (0.5 + 0.5 × confidence)
```

**پارامترها:**
- `enable_relationship_weighting`: فعال/غیرفعال (پیش‌فرض: True)
- `min_relationship_weight`: حداقل وزن (پیش‌فرض: 0.0)

### 7.3. پیش‌پردازش متن

**هدف:** حذف کلمات غیر مهم از گراف (نه از متن اصلی)

**ویژگی‌ها:**
- حذف stop words از گراف
- حفظ متن اصلی برای مدل‌های زبانی
- پشتیبانی از فارسی و انگلیسی

**پارامترها:**
- `enable_preprocessing`: فعال/غیرفعال (پیش‌فرض: False)
- `language`: زبان (auto/fa/en)

### 7.4. حذف نودهای ایزوله

**هدف:** حذف نودهایی که هیچ رابطه‌ای ندارند

**پارامترها:**
- `remove_isolated_nodes`: فعال/غیرفعال (پیش‌فرض: False)

### 7.5. تشخیص خودکار زبان

**الگوریتم:**
- شمارش کاراکترهای فارسی و انگلیسی
- محاسبه نسبت
- تشخیص زبان بر اساس نسبت

**نتیجه:**
- `fa`: فارسی
- `en`: انگلیسی
- `mixed`: ترکیبی

### 7.6. مدیریت خطا

**سطح‌های خطا:**
1. **خطاهای اعتبارسنجی:** بررسی ورودی‌ها
2. **خطاهای استخراج:** خطا در استخراج متن
3. **خطاهای ساخت گراف:** خطا در ساخت گراف
4. **خطاهای ذخیره:** خطا در ذخیره فایل

**مدیریت:**
- Logging کامل خطاها
- پیام‌های خطای واضح برای کاربر
- Fallback به روش‌های جایگزین

---

## نتیجه‌گیری

### 8.1. خلاصه قابلیت‌ها

این سیستم یک پلتفرم جامع و قدرتمند برای:

1. **آپلود گراف‌های از پیش ساخته شده** در 6 فرمت مختلف
2. **تبدیل متن به گراف** با 14 روش مختلف استخراج
3. **استخراج خودکار از URL** با پشتیبانی تخصصی از ویکی‌پدیا
4. **پس‌پردازش پیشرفته** شامل Entity Resolution و Weighting

### 8.2. نقاط قوت

- **انعطاف‌پذیری:** پشتیبانی از روش‌های مختلف استخراج
- **دقت:** روش‌های پیشرفته برای دقت بالا
- **سرعت:** روش‌های سریع برای پردازش سریع
- **پشتیبانی از زبان فارسی:** روش‌های تخصصی برای فارسی
- **یکپارچگی:** ادغام کامل با سیستم GraphRAG

### 8.3. کاربردها

- **تحقیقات علمی:** ساخت گراف دانش از مقالات
- **تحلیل محتوا:** استخراج دانش از صفحات وب
- **پردازش ویکی‌پدیا:** ساخت گراف از صفحات ویکی‌پدیا
- **تحلیل متون فارسی:** پردازش متون فارسی

### 8.4. آینده

**پیشنهادات برای بهبود:**
- افزودن روش‌های استخراج بیشتر
- بهبود دقت استخراج از ویکی‌پدیا
- پشتیبانی از زبان‌های بیشتر
- بهینه‌سازی سرعت پردازش
- افزودن رابط کاربری پیشرفته‌تر

---

## ضمیمه

### A. لیست کامل روش‌های استخراج

1. `simple` - Rule-based
2. `spacy` - spaCy NLP
3. `spacy_svo_enhanced` - spaCy SVO Enhanced
4. `llm` - LLM Single-pass
5. `llm_multipass` - LLM Multi-pass
6. `hybrid` - Hybrid
7. `persian` - Persian
8. `span_based` - Span-based
9. `with_coreference` - With Coreference
10. `long_text` - Long Text
11. `joint_er` - Joint ER
12. `autoregressive` - Autoregressive
13. `edc` - EDC Framework
14. `incremental` - Incremental

### B. فرمت فایل گراف ذخیره شده

گراف‌ها به صورت فایل PKL (Python Pickle) ذخیره می‌شوند که شامل:
- گراف NetworkX (MultiDiGraph)
- Metadata (تاریخ ساخت، روش استخراج، ...)
- آمار گراف

### C. محدودیت‌ها

- حداکثر طول متن از URL: 10000 کاراکتر
- حداکثر موجودیت‌ها: 10000 (قابل تنظیم)
- حداکثر روابط: 20000 (قابل تنظیم)
- Timeout برای URL: 30 ثانیه

---

**تاریخ تهیه گزارش:** 17 فوریه 2026  
**نسخه:** 1.0
